{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Human-Like TTS Generator for Google Colab\n",
    "\n",
    "**Enhanced Multilingual TTS with Emotional Narration Support (Hindi + English)**\n",
    "\n",
    "This notebook allows you to:\n",
    "1. Upload your transcription file\n",
    "2. Select AI provider and model\n",
    "3. Generate TTS audio and download it\n",
    "\n",
    "**Supported Models:**\n",
    "- üå≥ `suno/bark` - Best for emotions (recommended)\n",
    "- üì¢ `facebook/mms-tts-hin` - Fast Hindi TTS\n",
    "- üé§ `microsoft/speecht5_tts` - English TTS\n",
    "- üáÆüá≥ AI4Bharat models (requires authentication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies\n",
    "Run this cell to install all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers soundfile pydub numpy huggingface-hub accelerate scipy\n",
    "!pip install -q datasets  # For speaker embeddings\n",
    "!apt-get -qq install -y ffmpeg  # For audio processing\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Step 2: Upload Your Transcription File\n",
    "Upload your text/JSON file containing the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì§ Please upload your transcription file (.txt or .json):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded file name\n",
    "UPLOADED_FILE = list(uploaded.keys())[0]\n",
    "print(f\"\\n‚úÖ Uploaded: {UPLOADED_FILE}\")\n",
    "print(f\"üìÑ File size: {len(uploaded[UPLOADED_FILE])} bytes\")\n",
    "\n",
    "# Display preview\n",
    "with open(UPLOADED_FILE, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "print(f\"\\nüìù Preview (first 500 chars):\\n{content[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 3: Select AI Provider and Model\n",
    "Choose your preferred TTS model and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Model options with descriptions\n",
    "MODEL_OPTIONS = {\n",
    "    \"suno/bark (Best for emotions, supports Hindi)\": \"suno/bark\",\n",
    "    \"facebook/mms-tts-hin (Fast Hindi TTS)\": \"facebook/mms-tts-hin\",\n",
    "    \"microsoft/speecht5_tts (English TTS)\": \"microsoft/speecht5_tts\",\n",
    "    \"ai4bharat/indic-parler-tts (Hindi - requires auth)\": \"ai4bharat/indic-parler-tts\",\n",
    "    \"Custom Model (enter below)\": \"custom\"\n",
    "}\n",
    "\n",
    "# AI Provider dropdown\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=list(MODEL_OPTIONS.keys()),\n",
    "    value=\"suno/bark (Best for emotions, supports Hindi)\",\n",
    "    description='Model:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# Custom model input\n",
    "custom_model_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter HuggingFace model name (e.g., suno/bark-small)',\n",
    "    description='Custom Model:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# HuggingFace token (for gated models)\n",
    "hf_token_input = widgets.Password(\n",
    "    value='',\n",
    "    placeholder='Optional: Enter HF token for gated models',\n",
    "    description='HF Token:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "# Language selection\n",
    "language_dropdown = widgets.Dropdown(\n",
    "    options=['auto (detect)', 'hi (Hindi)', 'en (English)'],\n",
    "    value='auto (detect)',\n",
    "    description='Language:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "display(HTML(\"<h3>üéõÔ∏è Select Your TTS Configuration</h3>\"))\n",
    "display(model_dropdown)\n",
    "display(custom_model_input)\n",
    "display(hf_token_input)\n",
    "display(language_dropdown)\n",
    "\n",
    "print(\"\\nüí° Tip: suno/bark is recommended for emotional Hindi narration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the selected configuration\n",
    "selected_model_key = model_dropdown.value\n",
    "SELECTED_MODEL = MODEL_OPTIONS[selected_model_key]\n",
    "\n",
    "if SELECTED_MODEL == \"custom\":\n",
    "    SELECTED_MODEL = custom_model_input.value\n",
    "    if not SELECTED_MODEL:\n",
    "        raise ValueError(\"Please enter a custom model name!\")\n",
    "\n",
    "HF_TOKEN = hf_token_input.value if hf_token_input.value else None\n",
    "\n",
    "LANGUAGE = language_dropdown.value.split()[0]  # Extract 'auto', 'hi', or 'en'\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration saved:\")\n",
    "print(f\"   üì¶ Model: {SELECTED_MODEL}\")\n",
    "print(f\"   üåê Language: {LANGUAGE}\")\n",
    "print(f\"   üîë HF Token: {'Provided' if HF_TOKEN else 'Not provided'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: TTS Engine Setup\n",
    "This cell contains the complete TTS engine code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Enhanced Multilingual TTS Engine for Google Colab\n",
    "Supports: Bark, VITS, SpeechT5, AI4Bharat models\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from transformers import (\n",
    "    AutoProcessor, AutoModel, AutoTokenizer,\n",
    "    VitsModel, VitsTokenizer,\n",
    "    BarkModel, BarkProcessor,\n",
    "    SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    ")\n",
    "from huggingface_hub import login, HfFolder\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize, compress_dynamic_range\n",
    "\n",
    "# Check GPU availability\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è Using device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "class TranscriptionParser:\n",
    "    \"\"\"Parse human-like transcription with language-agnostic markers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tone_markers = re.compile(r'\\[TONE:\\s*(\\w+)\\]')\n",
    "        self.pause_markers = re.compile(r'\\[PAUSE-(SHORT|MEDIUM|LONG)\\]')\n",
    "        self.pronounce_markers = re.compile(r'(\\S+)\\s*\\[PRONOUNCE:\\s*([^\\]]+)\\]')\n",
    "        self.emphasis_markers = re.compile(r'\\[EMPHASIS:\\s*([^\\]]+)\\]')\n",
    "    \n",
    "    def parse(self, text):\n",
    "        \"\"\"Parse transcription and extract emotional/contextual information.\"\"\"\n",
    "        segments = []\n",
    "        current_pos = 0\n",
    "        current_tone = \"neutral\"\n",
    "        \n",
    "        markers = []\n",
    "        \n",
    "        for match in self.tone_markers.finditer(text):\n",
    "            markers.append(('tone', match.start(), match.end(), match.group(1)))\n",
    "        \n",
    "        for match in self.pause_markers.finditer(text):\n",
    "            markers.append(('pause', match.start(), match.end(), match.group(1)))\n",
    "        \n",
    "        markers.sort(key=lambda x: x[1])\n",
    "        \n",
    "        for marker_type, start, end, value in markers:\n",
    "            if start > current_pos:\n",
    "                segment_text = text[current_pos:start].strip()\n",
    "                if segment_text:\n",
    "                    segment_text = self._clean_markers(segment_text)\n",
    "                    segments.append({\n",
    "                        'text': segment_text,\n",
    "                        'tone': current_tone,\n",
    "                        'type': 'speech'\n",
    "                    })\n",
    "            \n",
    "            if marker_type == 'tone':\n",
    "                current_tone = value\n",
    "            elif marker_type == 'pause':\n",
    "                pause_duration = {\n",
    "                    'SHORT': 0.3,\n",
    "                    'MEDIUM': 0.6,\n",
    "                    'LONG': 1.0\n",
    "                }.get(value, 0.5)\n",
    "                \n",
    "                segments.append({\n",
    "                    'text': '',\n",
    "                    'duration': pause_duration,\n",
    "                    'type': 'pause'\n",
    "                })\n",
    "            \n",
    "            current_pos = end\n",
    "        \n",
    "        if current_pos < len(text):\n",
    "            segment_text = text[current_pos:].strip()\n",
    "            if segment_text:\n",
    "                segment_text = self._clean_markers(segment_text)\n",
    "                segments.append({\n",
    "                    'text': segment_text,\n",
    "                    'tone': current_tone,\n",
    "                    'type': 'speech'\n",
    "                })\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def _clean_markers(self, text):\n",
    "        \"\"\"Remove all markers from text.\"\"\"\n",
    "        text = self.tone_markers.sub('', text)\n",
    "        text = self.pause_markers.sub('', text)\n",
    "        text = self.pronounce_markers.sub(r'\\1', text)\n",
    "        text = self.emphasis_markers.sub(r'\\1', text)\n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "class MultilingualTTSEngine:\n",
    "    \"\"\"TTS engine with multilingual support.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, model_type=\"auto\", device=\"cuda\", language=\"auto\", hf_token=None):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.device = device\n",
    "        self.language = language\n",
    "        self.hf_token = hf_token\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.vocoder = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        if self.hf_token:\n",
    "            os.environ['HF_TOKEN'] = self.hf_token\n",
    "        \n",
    "        if self.model_type == \"auto\":\n",
    "            self.model_type = self._detect_model_type(model_name)\n",
    "            print(f\"üîç Auto-detected model type: {self.model_type}\")\n",
    "        \n",
    "        self.bark_voice_presets = {\n",
    "            'neutral': 'v2/en_speaker_6',\n",
    "            'happy': 'v2/en_speaker_9',\n",
    "            'sad': 'v2/en_speaker_3',\n",
    "            'excited': 'v2/en_speaker_9',\n",
    "            'serious': 'v2/en_speaker_1',\n",
    "            'thoughtful': 'v2/en_speaker_6',\n",
    "            'angry': 'v2/en_speaker_1',\n",
    "            'calm': 'v2/en_speaker_6',\n",
    "            'worried': 'v2/en_speaker_3',\n",
    "            'determined': 'v2/en_speaker_1',\n",
    "            'curious': 'v2/en_speaker_9',\n",
    "        }\n",
    "        \n",
    "        self.load_model()\n",
    "    \n",
    "    def _detect_model_type(self, model_name):\n",
    "        \"\"\"Auto-detect model type from model name.\"\"\"\n",
    "        model_lower = model_name.lower()\n",
    "        \n",
    "        if 'bark' in model_lower:\n",
    "            return 'bark'\n",
    "        elif 'ai4bharat' in model_lower or 'indic' in model_lower:\n",
    "            return 'ai4bharat'\n",
    "        elif 'speecht5' in model_lower:\n",
    "            return 'speecht5'\n",
    "        elif 'mms-tts' in model_lower or 'vits' in model_lower:\n",
    "            return 'vits'\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Could not auto-detect model type, defaulting to 'vits'\")\n",
    "            return 'vits'\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load TTS model.\"\"\"\n",
    "        print(f\"üì• Loading {self.model_type} model: {self.model_name}\")\n",
    "        \n",
    "        if self.model_type == \"bark\":\n",
    "            self._load_bark()\n",
    "        elif self.model_type == \"vits\":\n",
    "            self._load_vits()\n",
    "        elif self.model_type == \"speecht5\":\n",
    "            self._load_speecht5()\n",
    "        elif self.model_type == \"ai4bharat\":\n",
    "            self._load_ai4bharat()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "        \n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    def _load_bark(self):\n",
    "        \"\"\"Load Bark model.\"\"\"\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            self.processor = BarkProcessor.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token\n",
    "            )\n",
    "            self.model = BarkModel.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                token=self.hf_token\n",
    "            ).to(self.device)\n",
    "            \n",
    "            if hasattr(self.model, 'generation_config'):\n",
    "                if self.model.generation_config.pad_token_id is None:\n",
    "                    self.model.generation_config.pad_token_id = self.model.generation_config.eos_token_id\n",
    "            \n",
    "            if self.device == \"cuda\":\n",
    "                try:\n",
    "                    self.model = self.model.to_bettertransformer()\n",
    "                    print(\"   ‚úÖ Optimized for GPU with BetterTransformer\")\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    def _load_vits(self):\n",
    "        \"\"\"Load VITS model.\"\"\"\n",
    "        print(f\"   Language: {self.language}\")\n",
    "        try:\n",
    "            self.tokenizer = VitsTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token\n",
    "            )\n",
    "            self.model = VitsModel.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token\n",
    "            ).to(self.device)\n",
    "        except Exception as e:\n",
    "            self.processor = AutoProcessor.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token\n",
    "            )\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token\n",
    "            ).to(self.device)\n",
    "    \n",
    "    def _load_speecht5(self):\n",
    "        \"\"\"Load SpeechT5 model.\"\"\"\n",
    "        self.processor = SpeechT5Processor.from_pretrained(\n",
    "            self.model_name,\n",
    "            token=self.hf_token\n",
    "        )\n",
    "        self.model = SpeechT5ForTextToSpeech.from_pretrained(\n",
    "            self.model_name,\n",
    "            token=self.hf_token\n",
    "        ).to(self.device)\n",
    "        self.vocoder = SpeechT5HifiGan.from_pretrained(\n",
    "            \"microsoft/speecht5_hifigan\"\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def _load_ai4bharat(self):\n",
    "        \"\"\"Load AI4Bharat models.\"\"\"\n",
    "        print(f\"   Loading AI4Bharat model...\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token\n",
    "            )\n",
    "            self.processor = AutoProcessor.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token\n",
    "            )\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                token=self.hf_token\n",
    "            ).to(self.device)\n",
    "            \n",
    "            if self.tokenizer.pad_token_id is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Trying as VITS model...\")\n",
    "            self.tokenizer = VitsTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token\n",
    "            )\n",
    "            self.model = VitsModel.from_pretrained(\n",
    "                self.model_name,\n",
    "                token=self.hf_token\n",
    "            ).to(self.device)\n",
    "    \n",
    "    def detect_language(self, text):\n",
    "        \"\"\"Detect text language.\"\"\"\n",
    "        hindi_chars = len(re.findall(r'[\\u0900-\\u097F]', text))\n",
    "        english_chars = len(re.findall(r'[a-zA-Z]', text))\n",
    "        \n",
    "        total_chars = hindi_chars + english_chars\n",
    "        if total_chars == 0:\n",
    "            return \"en\"\n",
    "        \n",
    "        hindi_ratio = hindi_chars / total_chars\n",
    "        return \"hi\" if hindi_ratio > 0.3 else \"en\"\n",
    "    \n",
    "    def generate_with_emotion(self, text, tone=\"neutral\", sample_rate=24000):\n",
    "        \"\"\"Generate audio with emotional context.\"\"\"\n",
    "        if self.language == \"auto\":\n",
    "            detected_lang = self.detect_language(text)\n",
    "        else:\n",
    "            detected_lang = self.language\n",
    "        \n",
    "        if self.model_type == \"bark\":\n",
    "            return self._generate_bark(text, tone, detected_lang)\n",
    "        elif self.model_type == \"vits\":\n",
    "            return self._generate_vits(text, detected_lang)\n",
    "        elif self.model_type == \"speecht5\":\n",
    "            return self._generate_speecht5(text)\n",
    "        elif self.model_type == \"ai4bharat\":\n",
    "            return self._generate_ai4bharat(text, tone, detected_lang)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "    \n",
    "    def _generate_bark(self, text, tone, language):\n",
    "        \"\"\"Generate audio using Bark.\"\"\"\n",
    "        voice_preset = self.bark_voice_presets.get(tone, 'v2/en_speaker_6')\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                text,\n",
    "                voice_preset=voice_preset,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            if 'input_ids' in inputs:\n",
    "                attention_mask = torch.ones_like(inputs['input_ids'])\n",
    "                inputs['attention_mask'] = attention_mask\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                speech_output = self.model.generate(\n",
    "                    **inputs,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.model.generation_config.pad_token_id\n",
    "                )\n",
    "            \n",
    "            audio_array = speech_output.cpu().numpy().squeeze()\n",
    "        \n",
    "        return audio_array, self.model.generation_config.sample_rate\n",
    "    \n",
    "    def _generate_vits(self, text, language):\n",
    "        \"\"\"Generate audio using VITS.\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "            input_ids = inputs['input_ids'].to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_ids)\n",
    "            \n",
    "            audio_array = output.waveform.cpu().numpy().squeeze()\n",
    "            \n",
    "        except Exception as e:\n",
    "            inputs = self.processor(text, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(**inputs)\n",
    "            \n",
    "            audio_array = output.cpu().numpy().squeeze()\n",
    "        \n",
    "        sample_rate = getattr(self.model.config, 'sampling_rate', 22050)\n",
    "        return audio_array, sample_rate\n",
    "    \n",
    "    def _generate_speecht5(self, text):\n",
    "        \"\"\"Generate audio using SpeechT5.\"\"\"\n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        from datasets import load_dataset\n",
    "        embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "        speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            speech = self.model.generate_speech(\n",
    "                inputs[\"input_ids\"],\n",
    "                speaker_embeddings,\n",
    "                vocoder=self.vocoder\n",
    "            )\n",
    "        \n",
    "        audio_array = speech.cpu().numpy()\n",
    "        return audio_array, 16000\n",
    "    \n",
    "    def _generate_ai4bharat(self, text, tone, language):\n",
    "        \"\"\"Generate audio using AI4Bharat models.\"\"\"\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            if 'attention_mask' not in inputs and 'input_ids' in inputs:\n",
    "                inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if hasattr(self.model, 'generate'):\n",
    "                    output = self.model.generate(**inputs, max_length=2048)\n",
    "                    audio_array = output.cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    output = self.model(**inputs)\n",
    "                    if hasattr(output, 'waveform'):\n",
    "                        audio_array = output.waveform.cpu().numpy().squeeze()\n",
    "                    elif hasattr(output, 'audio'):\n",
    "                        audio_array = output.audio.cpu().numpy().squeeze()\n",
    "                    else:\n",
    "                        audio_array = output[0].cpu().numpy().squeeze()\n",
    "            \n",
    "            sample_rate = getattr(self.model.config, 'sampling_rate', 24000)\n",
    "            return audio_array, sample_rate\n",
    "\n",
    "\n",
    "class HumanLikeTTSGenerator:\n",
    "    \"\"\"Main generator class.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, model_type=\"auto\", device=\"cuda\", output_dir=\".\", language=\"auto\", hf_token=None):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.device = device\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.language = language\n",
    "        \n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.parser = TranscriptionParser()\n",
    "        self.engine = MultilingualTTSEngine(model_name, model_type, device, language, hf_token)\n",
    "    \n",
    "    def generate_from_transcription(self, transcription_file):\n",
    "        \"\"\"Generate audio from transcription file.\"\"\"\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"üé¨ HUMAN-LIKE TTS GENERATION\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        print(f\"üìÑ Input: {transcription_file}\")\n",
    "        print(f\"ü§ñ Model: {self.model_name} ({self.model_type})\")\n",
    "        print(f\"üñ•Ô∏è Device: {self.device}\")\n",
    "        print(f\"üåê Language: {self.language}\")\n",
    "        print(f\"{'=' * 70}\\n\")\n",
    "        \n",
    "        transcription_path = Path(transcription_file)\n",
    "        \n",
    "        if transcription_path.suffix == '.json':\n",
    "            with open(transcription_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                text = self._extract_text_from_json(data)\n",
    "        else:\n",
    "            with open(transcription_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        \n",
    "        primary_lang = self.engine.detect_language(text)\n",
    "        print(f\"üîç Detected primary language: {'Hindi' if primary_lang == 'hi' else 'English'}\")\n",
    "        \n",
    "        print(f\"\\nüìù Parsing transcription...\")\n",
    "        segments = self.parser.parse(text)\n",
    "        print(f\"   Found {len(segments)} segments\")\n",
    "        \n",
    "        print(f\"\\nüéµ Generating audio segments...\")\n",
    "        audio_segments = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, segment in enumerate(segments, 1):\n",
    "            if segment['type'] == 'pause':\n",
    "                duration_ms = int(segment['duration'] * 1000)\n",
    "                silence = AudioSegment.silent(duration=duration_ms)\n",
    "                audio_segments.append(silence)\n",
    "                print(f\"   [{i}/{len(segments)}] üîá Pause ({segment['duration']}s)\")\n",
    "            \n",
    "            else:\n",
    "                text = segment['text']\n",
    "                tone = segment.get('tone', 'neutral')\n",
    "                \n",
    "                if not text.strip():\n",
    "                    continue\n",
    "                \n",
    "                seg_lang = self.engine.detect_language(text)\n",
    "                lang_label = \"HI\" if seg_lang == \"hi\" else \"EN\"\n",
    "                \n",
    "                display_text = text[:50] + \"...\" if len(text) > 50 else text\n",
    "                print(f\"   [{i}/{len(segments)}] üéôÔ∏è [{lang_label}] ({tone}): {display_text}\")\n",
    "                \n",
    "                try:\n",
    "                    seg_start = time.time()\n",
    "                    audio_array, sample_rate = self.engine.generate_with_emotion(text, tone)\n",
    "                    seg_time = time.time() - seg_start\n",
    "                    \n",
    "                    audio_array = (audio_array * 32767).astype(np.int16)\n",
    "                    audio_seg = AudioSegment(\n",
    "                        audio_array.tobytes(),\n",
    "                        frame_rate=sample_rate,\n",
    "                        sample_width=2,\n",
    "                        channels=1\n",
    "                    )\n",
    "                    \n",
    "                    audio_segments.append(audio_seg)\n",
    "                    print(f\"       ‚úÖ Generated in {seg_time:.1f}s\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"       ‚ö†Ô∏è Failed: {e}\")\n",
    "                    audio_segments.append(AudioSegment.silent(duration=500))\n",
    "                    continue\n",
    "        \n",
    "        print(f\"\\nüîó Combining {len(audio_segments)} audio segments...\")\n",
    "        final_audio = AudioSegment.empty()\n",
    "        for seg in audio_segments:\n",
    "            final_audio += seg\n",
    "        \n",
    "        print(f\"üéõÔ∏è Post-processing audio...\")\n",
    "        final_audio = self._post_process(final_audio)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = self.output_dir / f\"narration_{primary_lang}_{timestamp}.mp3\"\n",
    "        \n",
    "        print(f\"üíæ Exporting to: {output_file}\")\n",
    "        final_audio.export(\n",
    "            str(output_file),\n",
    "            format=\"mp3\",\n",
    "            bitrate=\"192k\",\n",
    "            parameters=[\"-ar\", \"44100\"]\n",
    "        )\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        duration_sec = len(final_audio) / 1000\n",
    "        \n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"üéâ AUDIO GENERATION COMPLETE!\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        print(f\"üåê Language: {primary_lang.upper()}\")\n",
    "        print(f\"‚è±Ô∏è Generation time: {total_time/60:.2f} minutes\")\n",
    "        print(f\"üéµ Audio duration: {duration_sec/60:.2f} minutes\")\n",
    "        print(f\"‚ö° Speed: {duration_sec/total_time:.2f}x realtime\")\n",
    "        print(f\"üìä File size: {output_file.stat().st_size / 1e6:.2f} MB\")\n",
    "        print(f\"üíæ Output: {output_file}\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        \n",
    "        return str(output_file)\n",
    "    \n",
    "    def _extract_text_from_json(self, data):\n",
    "        \"\"\"Extract narration text from JSON transcription.\"\"\"\n",
    "        text_parts = []\n",
    "        \n",
    "        for chapter in data.get('chapters', []):\n",
    "            if chapter.get('title'):\n",
    "                text_parts.append(f\"[PAUSE-SHORT] {chapter['title']} [PAUSE-MEDIUM]\")\n",
    "            \n",
    "            for chunk in chapter.get('chunks', []):\n",
    "                if chunk.get('narration'):\n",
    "                    text_parts.append(chunk['narration'])\n",
    "                    text_parts.append('[PAUSE-SHORT]')\n",
    "        \n",
    "        return '\\n\\n'.join(text_parts)\n",
    "    \n",
    "    def _post_process(self, audio):\n",
    "        \"\"\"Post-process audio for quality.\"\"\"\n",
    "        target_dBFS = -20.0\n",
    "        change_in_dBFS = target_dBFS - audio.dBFS\n",
    "        audio = audio.apply_gain(change_in_dBFS)\n",
    "        \n",
    "        audio = compress_dynamic_range(\n",
    "            audio,\n",
    "            threshold=-20.0,\n",
    "            ratio=4.0,\n",
    "            attack=5.0,\n",
    "            release=50.0\n",
    "        )\n",
    "        \n",
    "        audio = normalize(audio)\n",
    "        return audio\n",
    "\n",
    "\n",
    "print(\"‚úÖ TTS Engine loaded and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéµ Step 5: Generate TTS Audio\n",
    "Run this cell to generate the audio from your transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR = \"./tts_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize the generator\n",
    "print(\"üöÄ Initializing TTS Generator...\")\n",
    "generator = HumanLikeTTSGenerator(\n",
    "    model_name=SELECTED_MODEL,\n",
    "    model_type=\"auto\",\n",
    "    device=DEVICE,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    language=LANGUAGE,\n",
    "    hf_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Generate audio\n",
    "print(f\"\\nüéôÔ∏è Starting TTS generation...\")\n",
    "OUTPUT_FILE = generator.generate_from_transcription(UPLOADED_FILE)\n",
    "\n",
    "print(f\"\\n‚úÖ Audio file generated: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéß Step 6: Preview & Download Audio\n",
    "Listen to your generated audio and download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display, HTML\n",
    "import os\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    # Display audio player\n",
    "    print(\"üéß Preview your generated audio:\")\n",
    "    display(Audio(OUTPUT_FILE))\n",
    "    \n",
    "    # Get file info\n",
    "    file_size = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n",
    "    print(f\"\\nüìä File size: {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå Output file not found. Please run the generation step again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the generated audio file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading your TTS audio file...\")\n",
    "files.download(OUTPUT_FILE)\n",
    "print(\"‚úÖ Download started! Check your browser's download folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ (Optional) Save to Google Drive\n",
    "If you want to save the audio to your Google Drive instead of downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "print(\"üìÇ Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create TTS output folder in Drive\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/TTS_Output\"\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Copy file to Drive\n",
    "drive_output_path = os.path.join(DRIVE_OUTPUT_DIR, os.path.basename(OUTPUT_FILE))\n",
    "shutil.copy(OUTPUT_FILE, drive_output_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Audio saved to Google Drive:\")\n",
    "print(f\"   üìÅ {drive_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Quick Reference\n",
    "\n",
    "### Supported Markers in Transcription Files:\n",
    "- `[TONE: happy/sad/excited/serious/thoughtful/angry/calm/worried/determined/curious]` - Set emotional tone\n",
    "- `[PAUSE-SHORT]` - 0.3 second pause\n",
    "- `[PAUSE-MEDIUM]` - 0.6 second pause  \n",
    "- `[PAUSE-LONG]` - 1.0 second pause\n",
    "- `word [PRONOUNCE: pronunciation]` - Pronunciation guide\n",
    "- `[EMPHASIS: word]` - Emphasize a word\n",
    "\n",
    "### Recommended Models:\n",
    "| Model | Best For | Speed | Quality |\n",
    "|-------|----------|-------|--------|\n",
    "| `suno/bark` | Emotional Hindi/English | Slow | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| `facebook/mms-tts-hin` | Fast Hindi | Fast | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| `microsoft/speecht5_tts` | English | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "### Tips:\n",
    "- Use Colab GPU runtime for faster generation (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "- For long texts, generation may take 10-30 minutes\n",
    "- Bark produces the most natural-sounding speech but is slower"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

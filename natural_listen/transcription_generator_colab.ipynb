{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéôÔ∏è Multilingual Transcription Generator for TTS\n",
                "\n",
                "**Create emotional, human-level transcriptions for Text-to-Speech**\n",
                "\n",
                "This notebook converts text into narrator-ready transcriptions with:\n",
                "- üé≠ Pause markers `[PAUSE-SHORT]`, `[PAUSE-MEDIUM]`, `[PAUSE-LONG]`\n",
                "- üé® Tone indicators `[TONE: serious/thoughtful/curious/calm]`\n",
                "- üåç Multi-language support (Hindi & English)\n",
                "- üßπ Automatic repetition removal and validation\n",
                "\n",
                "**Steps:**\n",
                "1. üì§ Upload your text file\n",
                "2. ü§ñ Select AI provider (Ollama / HuggingFace) and model\n",
                "3. ‚öôÔ∏è Configure settings\n",
                "4. üìù Generate transcription\n",
                "5. üíæ Download the result\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q transformers torch accelerate colorama ollama\n",
                "\n",
                "print(\"‚úÖ Dependencies installed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Import Libraries and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import time\n",
                "import re\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "from collections import OrderedDict\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "# Color support\n",
                "try:\n",
                "    from colorama import init, Fore, Style\n",
                "    init(autoreset=True)\n",
                "except ImportError:\n",
                "    class Fore: RED = GREEN = YELLOW = CYAN = MAGENTA = RESET = \"\"\n",
                "    class Style: BRIGHT = RESET_ALL = \"\"\n",
                "\n",
                "# Ollama support\n",
                "try:\n",
                "    import ollama\n",
                "    OLLAMA_AVAILABLE = True\n",
                "    print(f\"{Fore.GREEN}‚úÖ Ollama package available{Style.RESET_ALL}\")\n",
                "except ImportError:\n",
                "    OLLAMA_AVAILABLE = False\n",
                "    print(f\"{Fore.YELLOW}‚ö†Ô∏è Ollama package not available{Style.RESET_ALL}\")\n",
                "\n",
                "# HuggingFace support\n",
                "try:\n",
                "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "    import torch\n",
                "    HF_AVAILABLE = True\n",
                "    print(f\"{Fore.GREEN}‚úÖ HuggingFace Transformers available{Style.RESET_ALL}\")\n",
                "except ImportError:\n",
                "    HF_AVAILABLE = False\n",
                "    print(f\"{Fore.YELLOW}‚ö†Ô∏è HuggingFace Transformers not available{Style.RESET_ALL}\")\n",
                "\n",
                "# Check GPU availability\n",
                "if HF_AVAILABLE and torch.cuda.is_available():\n",
                "    print(f\"{Fore.GREEN}üöÄ GPU Available: {torch.cuda.get_device_name(0)}{Style.RESET_ALL}\")\n",
                "else:\n",
                "    print(f\"{Fore.YELLOW}üíª Running on CPU{Style.RESET_ALL}\")\n",
                "\n",
                "print(\"\\n‚úÖ Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Define Narrator Prompts & Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ImprovedNarratorPrompts:\n",
                "    \"\"\"Strictly controlled prompts with better constraints.\"\"\"\n",
                "    \n",
                "    SYSTEM_PROMPT_HINDI = \"\"\"‡§Ü‡§™ ‡§è‡§ï ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§ë‡§°‡§ø‡§Ø‡•ã‡§¨‡•Å‡§ï ‡§ï‡§•‡§æ‡§µ‡§æ‡§ö‡§ï ‡§π‡•à‡§Ç‡•§ ‡§Ü‡§™‡§ï‡§æ ‡§ï‡§æ‡§Æ ‡§ï‡•á‡§µ‡§≤ ‡§¶‡§ø‡§è ‡§ó‡§è ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§®‡§æ‡§®‡§æ ‡§π‡•à‡•§\n",
                "\n",
                "‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§®‡§ø‡§Ø‡§Æ:\n",
                "1. ‡§Æ‡•Ç‡§≤ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§Æ‡•á‡§Ç ‡§ú‡•ã ‡§≤‡§ø‡§ñ‡§æ ‡§π‡•à ‡§µ‡§π‡•Ä ‡§¨‡•ã‡§≤‡•á‡§Ç - ‡§ï‡•Å‡§õ ‡§≠‡•Ä ‡§®‡§Ø‡§æ ‡§® ‡§ú‡•ã‡§°‡§º‡•á‡§Ç\n",
                "2. ‡§ï‡•Å‡§õ ‡§≠‡•Ä ‡§® ‡§õ‡•ã‡§°‡§º‡•á‡§Ç - ‡§π‡§∞ ‡§∂‡§¨‡•ç‡§¶ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à\n",
                "3. ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ ‡§® ‡§ï‡§∞‡•á‡§Ç, ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§® ‡§¶‡•á‡§Ç - ‡§¨‡§∏ ‡§µ‡§π‡•Ä ‡§™‡§¢‡§º‡•á‡§Ç ‡§ú‡•ã ‡§≤‡§ø‡§ñ‡§æ ‡§π‡•à\n",
                "4. ‡§ï‡•á‡§µ‡§≤ ‡§Ø‡•á ‡§Æ‡§æ‡§∞‡•ç‡§ï‡§∞ ‡§ú‡•ã‡§°‡§º‡•á‡§Ç: [PAUSE-SHORT], [PAUSE-MEDIUM], [PAUSE-LONG]\n",
                "5. ‡§ü‡•ã‡§® ‡§Æ‡§æ‡§∞‡•ç‡§ï‡§∞ (‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§Æ‡•á‡§Ç): [TONE: serious/thoughtful/curious/calm]\n",
                "6. ‡§≤‡§Ç‡§¨‡•á ‡§µ‡§æ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï ‡§†‡§π‡§∞‡§æ‡§µ ‡§∏‡•á ‡§§‡•ã‡§°‡§º‡•á‡§Ç\n",
                "7. ‡§ï‡•ã‡§à ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§µ‡§ø‡§µ‡§∞‡§£, ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Ø‡§æ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü‡•Ä‡§ï‡§∞‡§£ ‡§® ‡§ú‡•ã‡§°‡§º‡•á‡§Ç\n",
                "\n",
                "‡§Ü‡§™ ‡§ï‡•á‡§µ‡§≤ ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§π‡•à‡§Ç‡•§ ‡§Æ‡•Ç‡§≤ ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡§¶‡§≤‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç‡•§\"\"\"\n",
                "\n",
                "    SYSTEM_PROMPT_ENGLISH = \"\"\"You are a professional audiobook narrator. Your job is ONLY to read the text aloud naturally.\n",
                "\n",
                "CRITICAL RULES:\n",
                "1. Speak EXACTLY what's written - add NOTHING new\n",
                "2. Skip NOTHING - every word matters\n",
                "3. DO NOT interpret, summarize, or explain - just read what's written\n",
                "4. ONLY add these markers: [PAUSE-SHORT], [PAUSE-MEDIUM], [PAUSE-LONG]\n",
                "5. Tone markers (in English): [TONE: serious/thoughtful/curious/calm]\n",
                "6. Break long sentences with natural pauses\n",
                "7. NO additional details, context, or clarifications\n",
                "\n",
                "You are a VOICE only. Do not change the original words.\"\"\"\n",
                "\n",
                "    NARRATION_TEMPLATE_HINDI = \"\"\"‡§®‡•Ä‡§ö‡•á ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§¨‡§ø‡§≤‡§ï‡•Å‡§≤ ‡§µ‡•à‡§∏‡•á ‡§π‡•Ä ‡§∏‡•Å‡§®‡§æ‡§è‡§Ç ‡§ú‡•à‡§∏‡•á ‡§≤‡§ø‡§ñ‡§æ ‡§π‡•à‡•§ ‡§ï‡•Å‡§õ ‡§≠‡•Ä ‡§®‡§Ø‡§æ ‡§® ‡§ú‡•ã‡§°‡§º‡•á‡§Ç‡•§\n",
                "\n",
                "‡§Æ‡•Ç‡§≤ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü:\n",
                "\\\"\\\"\\\"\n",
                "{text}\n",
                "\\\"\\\"\\\"\n",
                "\n",
                "‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂:\n",
                "- ‡§ä‡§™‡§∞ ‡§ï‡•á ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡§ø‡§≤‡§ï‡•Å‡§≤ ‡§µ‡•à‡§∏‡•á ‡§π‡•Ä ‡§¨‡•ã‡§≤‡•á‡§Ç\n",
                "- ‡§ï‡•á‡§µ‡§≤ [PAUSE-SHORT], [PAUSE-MEDIUM], [PAUSE-LONG] ‡§ú‡•ã‡§°‡§º‡•á‡§Ç\n",
                "- ‡§ï‡•ã‡§à ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ, ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§Ø‡§æ ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§µ‡§ø‡§µ‡§∞‡§£ ‡§® ‡§¶‡•á‡§Ç\n",
                "- ‡§Æ‡•Ç‡§≤ ‡§µ‡§æ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡§¶‡§≤‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç\n",
                "\n",
                "‡§ï‡§•‡§® (‡§Æ‡•Ç‡§≤ ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§Æ‡•á‡§Ç):\"\"\"\n",
                "\n",
                "    NARRATION_TEMPLATE_ENGLISH = \"\"\"Read the text below EXACTLY as written. Add NOTHING new.\n",
                "\n",
                "ORIGINAL TEXT:\n",
                "\\\"\\\"\\\"\n",
                "{text}\n",
                "\\\"\\\"\\\"\n",
                "\n",
                "INSTRUCTIONS:\n",
                "- Speak the exact words above\n",
                "- ONLY add [PAUSE-SHORT], [PAUSE-MEDIUM], [PAUSE-LONG]\n",
                "- NO interpretation, summary, or additional details\n",
                "- DO NOT change the original sentences\n",
                "\n",
                "NARRATION (using original words):\"\"\"\n",
                "\n",
                "    @staticmethod\n",
                "    def detect_language(text):\n",
                "        \"\"\"Detect if text is primarily Hindi or English.\"\"\"\n",
                "        hindi_chars = len(re.findall(r'[\\u0900-\\u097F]', text))\n",
                "        english_chars = len(re.findall(r'[a-zA-Z]', text))\n",
                "        total_chars = hindi_chars + english_chars\n",
                "        if total_chars == 0:\n",
                "            return \"english\"\n",
                "        hindi_ratio = hindi_chars / total_chars\n",
                "        return \"hindi\" if hindi_ratio > 0.3 else \"english\"\n",
                "\n",
                "\n",
                "class RepetitionRemover:\n",
                "    \"\"\"Remove repetitive content from narration.\"\"\"\n",
                "    \n",
                "    @staticmethod\n",
                "    def remove_repetitions(text):\n",
                "        \"\"\"Remove repeated sentences and phrases.\"\"\"\n",
                "        sentences = re.split(r'(?<=[.!?‡•§])\\s+', text)\n",
                "        seen = OrderedDict()\n",
                "        \n",
                "        for sent in sentences:\n",
                "            sent = sent.strip()\n",
                "            if not sent:\n",
                "                continue\n",
                "            key = ' '.join(sent.split()[:10]).lower()\n",
                "            if key not in seen:\n",
                "                seen[key] = sent\n",
                "        \n",
                "        return ' '.join(seen.values())\n",
                "    \n",
                "    @staticmethod\n",
                "    def remove_meta_commentary(text, original):\n",
                "        \"\"\"Remove sentences that aren't in the original.\"\"\"\n",
                "        meta_patterns = [\n",
                "            r'‡§Ø‡§π.*?(‡§¶‡§∞‡•ç‡§∂‡§æ‡§§‡§æ|‡§∞‡•á‡§ñ‡§æ‡§Ç‡§ï‡§ø‡§§|‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§|‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞‡§ø‡§§).*?‡§π‡•à',\n",
                "            r'‡§Ø‡§π ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø.*?(‡§â‡§ú‡§æ‡§ó‡§∞|‡§¨‡§®‡§æ‡§§‡§æ|‡§∏‡•ç‡§™‡§∑‡•ç‡§ü).*?‡§π‡•à',\n",
                "            r'This.*?(shows|demonstrates|establishes|highlights)',\n",
                "            r'This chapter.*?(reveals|creates|clarifies)'\n",
                "        ]\n",
                "        \n",
                "        sentences = re.split(r'(?<=[.!?‡•§])\\s+', text)\n",
                "        filtered = []\n",
                "        \n",
                "        for sent in sentences:\n",
                "            is_meta = False\n",
                "            for pattern in meta_patterns:\n",
                "                if re.search(pattern, sent, re.IGNORECASE):\n",
                "                    is_meta = True\n",
                "                    break\n",
                "            if not is_meta:\n",
                "                filtered.append(sent)\n",
                "        \n",
                "        return ' '.join(filtered)\n",
                "\n",
                "print(\"‚úÖ Narrator prompts and utilities defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Define LLM Narrator Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ImprovedLLMNarrator:\n",
                "    \"\"\"Improved LLM narrator with better validation.\"\"\"\n",
                "    \n",
                "    def __init__(self, provider=\"ollama\", model_name=None, device=\"auto\", language=\"auto\"):\n",
                "        self.provider = provider\n",
                "        self.model_name = model_name or self._get_default_model()\n",
                "        self.device = self._detect_device() if device == \"auto\" else device\n",
                "        self.language = language\n",
                "        self.model = None\n",
                "        self.tokenizer = None\n",
                "        self.prompts = ImprovedNarratorPrompts()\n",
                "        self.repetition_remover = RepetitionRemover()\n",
                "    \n",
                "    def _detect_device(self):\n",
                "        \"\"\"Auto-detect available device.\"\"\"\n",
                "        try:\n",
                "            if HF_AVAILABLE and torch.cuda.is_available():\n",
                "                if hasattr(torch.version, 'hip') and torch.version.hip:\n",
                "                    print(\"üîç ROCm (AMD GPU) detected\")\n",
                "                else:\n",
                "                    print(\"üîç CUDA (NVIDIA GPU) detected\")\n",
                "                return \"cuda\"\n",
                "        except:\n",
                "            pass\n",
                "        print(\"üîç No GPU detected, using CPU\")\n",
                "        return \"cpu\"\n",
                "    \n",
                "    def _get_default_model(self):\n",
                "        \"\"\"Get best default model based on provider.\"\"\"\n",
                "        if self.provider == \"ollama\":\n",
                "            return \"gemma2:9b\"\n",
                "        else:\n",
                "            return \"ai4bharat/Airavata\"\n",
                "    \n",
                "    def load(self):\n",
                "        \"\"\"Load the LLM model.\"\"\"\n",
                "        print(f\"üé≠ Initializing {self.provider} narrator...\")\n",
                "        print(f\"   Model: {self.model_name}\")\n",
                "        print(f\"   Device: {self.device}\")\n",
                "        print(f\"   Language: {self.language}\")\n",
                "        \n",
                "        if self.provider == \"ollama\":\n",
                "            if not OLLAMA_AVAILABLE:\n",
                "                raise ImportError(\"Ollama not installed. Install: pip install ollama\")\n",
                "            try:\n",
                "                ollama.list()\n",
                "                print(f\"{Fore.GREEN}‚úÖ Ollama connection successful{Style.RESET_ALL}\")\n",
                "            except Exception as e:\n",
                "                raise RuntimeError(f\"Cannot connect to Ollama: {e}\")\n",
                "        \n",
                "        elif self.provider == \"huggingface\":\n",
                "            if not HF_AVAILABLE:\n",
                "                raise ImportError(\"Transformers not installed.\")\n",
                "            \n",
                "            print(f\"{Fore.CYAN}üì• Loading HuggingFace model: {self.model_name}...{Style.RESET_ALL}\")\n",
                "            print(\"(This may take a few minutes for large models)\")\n",
                "            \n",
                "            if self.device == \"cuda\":\n",
                "                torch_dtype = torch.float16\n",
                "                device_map = \"auto\"\n",
                "            else:\n",
                "                torch_dtype = torch.float32\n",
                "                device_map = None\n",
                "            \n",
                "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
                "            self.model = AutoModelForCausalLM.from_pretrained(\n",
                "                self.model_name,\n",
                "                torch_dtype=torch_dtype,\n",
                "                device_map=device_map\n",
                "            )\n",
                "            \n",
                "            if self.device == \"cpu\":\n",
                "                self.model = self.model.to(\"cpu\")\n",
                "            \n",
                "            device_type = \"GPU\" if self.device == \"cuda\" else \"CPU\"\n",
                "            print(f\"{Fore.GREEN}‚úÖ HuggingFace model loaded on {device_type}{Style.RESET_ALL}\")\n",
                "        \n",
                "        return True\n",
                "    \n",
                "    def generate(self, prompt, system_prompt, max_tokens=2048, temperature=0.2):\n",
                "        \"\"\"Generate with lower temperature for faithful reproduction.\"\"\"\n",
                "        if self.provider == \"ollama\":\n",
                "            return self._generate_ollama(prompt, system_prompt, max_tokens, temperature)\n",
                "        else:\n",
                "            return self._generate_huggingface(prompt, system_prompt, max_tokens, temperature)\n",
                "    \n",
                "    def _generate_ollama(self, prompt, system_prompt, max_tokens, temperature):\n",
                "        \"\"\"Generate using Ollama.\"\"\"\n",
                "        try:\n",
                "            response = ollama.chat(\n",
                "                model=self.model_name,\n",
                "                messages=[\n",
                "                    {\"role\": \"system\", \"content\": system_prompt},\n",
                "                    {\"role\": \"user\", \"content\": prompt}\n",
                "                ],\n",
                "                options={\n",
                "                    \"temperature\": temperature,\n",
                "                    \"num_predict\": max_tokens,\n",
                "                    \"top_p\": 0.85,\n",
                "                    \"repeat_penalty\": 1.3,\n",
                "                    \"top_k\": 40,\n",
                "                }\n",
                "            )\n",
                "            return response[\"message\"][\"content\"].strip()\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è Ollama generation error: {e}\")\n",
                "            return None\n",
                "    \n",
                "    def _generate_huggingface(self, prompt, system_prompt, max_tokens, temperature):\n",
                "        \"\"\"Generate using HuggingFace.\"\"\"\n",
                "        try:\n",
                "            if \"Airavata\" in self.model_name or \"sarvam\" in self.model_name:\n",
                "                formatted_prompt = f\"### System:\\n{system_prompt}\\n\\n### User:\\n{prompt}\\n\\n### Assistant:\\n\"\n",
                "            else:\n",
                "                formatted_prompt = f\"<s>[INST] {system_prompt}\\n\\n{prompt} [/INST]\"\n",
                "            \n",
                "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
                "            \n",
                "            with torch.no_grad():\n",
                "                outputs = self.model.generate(\n",
                "                    **inputs,\n",
                "                    max_new_tokens=max_tokens,\n",
                "                    temperature=temperature,\n",
                "                    do_sample=True,\n",
                "                    top_p=0.85,\n",
                "                    top_k=40,\n",
                "                    repetition_penalty=1.3\n",
                "                )\n",
                "            \n",
                "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "            \n",
                "            if \"[/INST]\" in response:\n",
                "                response = response.split(\"[/INST]\")[-1].strip()\n",
                "            elif \"### Assistant:\" in response:\n",
                "                response = response.split(\"### Assistant:\")[-1].strip()\n",
                "            \n",
                "            return response\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è HuggingFace generation error: {e}\")\n",
                "            return None\n",
                "    \n",
                "    def validate_and_clean(self, original, narration):\n",
                "        \"\"\"Validate and clean the narration.\"\"\"\n",
                "        if not narration:\n",
                "            return None, \"Empty narration\"\n",
                "        \n",
                "        cleaned = self.repetition_remover.remove_meta_commentary(narration, original)\n",
                "        cleaned = self.repetition_remover.remove_repetitions(cleaned)\n",
                "        \n",
                "        original_words = set(original.lower().split())\n",
                "        clean_narration = re.sub(r'\\[(?:TONE|PAUSE|PRONOUNCE|EMPHASIS):[^\\]]*\\]', '', cleaned)\n",
                "        clean_narration = re.sub(r'\\[PAUSE-(?:SHORT|MEDIUM|LONG)\\]', '', clean_narration)\n",
                "        narration_words = set(clean_narration.lower().split())\n",
                "        \n",
                "        new_words = narration_words - original_words\n",
                "        lang = self.prompts.detect_language(original)\n",
                "        threshold = 0.6 if lang == \"hindi\" else 0.4\n",
                "        \n",
                "        if len(new_words) > len(original_words) * threshold:\n",
                "            return None, f\"Too many new words added ({len(new_words)} new vs {len(original_words)} original)\"\n",
                "        \n",
                "        return cleaned, \"Valid\"\n",
                "    \n",
                "    def narrate_text(self, text, max_retries=3):\n",
                "        \"\"\"Convert text to narration with strict validation.\"\"\"\n",
                "        detected_lang = self.prompts.detect_language(text) if self.language == \"auto\" else self.language\n",
                "        \n",
                "        system_prompt = (self.prompts.SYSTEM_PROMPT_HINDI if detected_lang == \"hindi\" \n",
                "                        else self.prompts.SYSTEM_PROMPT_ENGLISH)\n",
                "        template = (self.prompts.NARRATION_TEMPLATE_HINDI if detected_lang == \"hindi\" \n",
                "                   else self.prompts.NARRATION_TEMPLATE_ENGLISH)\n",
                "        \n",
                "        prompt = template.format(text=text)\n",
                "        \n",
                "        for attempt in range(max_retries):\n",
                "            temp = 0.2 - (attempt * 0.05)\n",
                "            narration = self.generate(prompt, system_prompt, max_tokens=3072, temperature=temp)\n",
                "            \n",
                "            if not narration:\n",
                "                continue\n",
                "            \n",
                "            cleaned, reason = self.validate_and_clean(text, narration)\n",
                "            \n",
                "            if cleaned:\n",
                "                return cleaned, True, detected_lang\n",
                "            else:\n",
                "                print(f\"      ‚ö†Ô∏è Attempt {attempt + 1} failed: {reason}\")\n",
                "                if attempt < max_retries - 1:\n",
                "                    print(f\"      üîÑ Retrying with temperature {temp - 0.05:.2f}...\")\n",
                "        \n",
                "        print(f\"      ‚ö†Ô∏è All attempts failed, using minimal narration\")\n",
                "        return self._minimal_narration(text), False, detected_lang\n",
                "    \n",
                "    def _minimal_narration(self, text):\n",
                "        \"\"\"Minimal fallback - just add pauses.\"\"\"\n",
                "        sentences = re.split(r'([.!?‡•§]+\\s+)', text)\n",
                "        result = []\n",
                "        for i, sent in enumerate(sentences):\n",
                "            if not sent.strip():\n",
                "                continue\n",
                "            result.append(sent)\n",
                "            if sent.strip() in '.!?‡•§':\n",
                "                if i < len(sentences) - 1:\n",
                "                    result.append(\" [PAUSE-SHORT] \")\n",
                "        return ''.join(result)\n",
                "\n",
                "print(\"‚úÖ LLM Narrator class defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Define Text Preprocessor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextPreprocessor:\n",
                "    \"\"\"Preprocess text with better chapter detection.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.chapter_pattern = re.compile(\n",
                "            r'^(={3,}\\s*)?(Chapter|CHAPTER|‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø|CHAPTER)\\s+(\\d+|[IVXivx]+|[‡•¶-‡•Ø]+):?\\s*(.*)(\\s*={3,})?$',\n",
                "            re.MULTILINE\n",
                "        )\n",
                "    \n",
                "    def split_into_chapters(self, text):\n",
                "        \"\"\"Split text into chapters.\"\"\"\n",
                "        chapters = []\n",
                "        matches = list(self.chapter_pattern.finditer(text))\n",
                "        \n",
                "        if not matches:\n",
                "            return [{\n",
                "                \"number\": 1,\n",
                "                \"title\": \"Complete Text\",\n",
                "                \"content\": text\n",
                "            }]\n",
                "        \n",
                "        for i, match in enumerate(matches):\n",
                "            chapter_num = match.group(3)\n",
                "            chapter_title = match.group(4).strip() or f\"Chapter {chapter_num}\"\n",
                "            start_pos = match.end()\n",
                "            end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
                "            content = text[start_pos:end_pos].strip()\n",
                "            \n",
                "            chapters.append({\n",
                "                \"number\": i + 1,\n",
                "                \"title\": chapter_title,\n",
                "                \"content\": content\n",
                "            })\n",
                "        \n",
                "        return chapters\n",
                "    \n",
                "    def split_into_sentences(self, text):\n",
                "        \"\"\"Split into sentences (Hindi + English).\"\"\"\n",
                "        sentences = re.split(r'(?<=[.!?‡•§])\\s+(?=[A-Z–ê-–Ø\"\\u0900-\\u097F])', text)\n",
                "        return [s.strip() for s in sentences if s.strip()]\n",
                "    \n",
                "    def create_chunks(self, sentences, chunk_size=8, overlap=1):\n",
                "        \"\"\"Create smaller overlapping chunks.\"\"\"\n",
                "        chunks = []\n",
                "        i = 0\n",
                "        while i < len(sentences):\n",
                "            chunk_sentences = sentences[i:i + chunk_size]\n",
                "            chunk_text = ' '.join(chunk_sentences)\n",
                "            chunks.append({\n",
                "                'text': chunk_text,\n",
                "                'start_idx': i,\n",
                "                'end_idx': i + len(chunk_sentences)\n",
                "            })\n",
                "            i += max(1, chunk_size - overlap)\n",
                "        return chunks\n",
                "\n",
                "print(\"‚úÖ Text preprocessor defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Upload Your Text File üì§"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "print(\"üì§ Please upload your text file to transcribe:\")\n",
                "print(\"(This will be converted to narrator-ready format for TTS)\")\n",
                "print()\n",
                "\n",
                "uploaded = files.upload()\n",
                "\n",
                "if uploaded:\n",
                "    uploaded_filename = list(uploaded.keys())[0]\n",
                "    input_text = uploaded[uploaded_filename].decode('utf-8')\n",
                "    \n",
                "    # Detect language\n",
                "    detected_lang = ImprovedNarratorPrompts.detect_language(input_text)\n",
                "    word_count = len(input_text.split())\n",
                "    \n",
                "    print()\n",
                "    print(f\"{Fore.GREEN}‚úÖ File uploaded successfully!{Style.RESET_ALL}\")\n",
                "    print(f\"üìÑ Filename: {uploaded_filename}\")\n",
                "    print(f\"üåç Detected language: {detected_lang.upper()}\")\n",
                "    print(f\"üìä Word count: {word_count:,} words\")\n",
                "    print(f\"üìù Preview (first 500 chars):\")\n",
                "    print(\"-\" * 50)\n",
                "    print(input_text[:500] + \"...\" if len(input_text) > 500 else input_text)\n",
                "else:\n",
                "    print(f\"{Fore.RED}‚ùå No file uploaded. Please run this cell again.{Style.RESET_ALL}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Configure Options üéõÔ∏è"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ipywidgets as widgets\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "# Create configuration widgets\n",
                "provider_dropdown = widgets.Dropdown(\n",
                "    options=['huggingface', 'ollama'],\n",
                "    value='huggingface',\n",
                "    description='AI Provider:',\n",
                "    style={'description_width': '120px'},\n",
                "    layout=widgets.Layout(width='400px')\n",
                ")\n",
                "\n",
                "model_text = widgets.Text(\n",
                "    value='Qwen/Qwen2.5-1.5B-Instruct',\n",
                "    placeholder='Enter model name',\n",
                "    description='Model:',\n",
                "    style={'description_width': '120px'},\n",
                "    layout=widgets.Layout(width='500px')\n",
                ")\n",
                "\n",
                "language_dropdown = widgets.Dropdown(\n",
                "    options=[\n",
                "        ('üîÑ Auto-detect', 'auto'),\n",
                "        ('üáÆüá≥ Hindi', 'hindi'),\n",
                "        ('üá¨üáß English', 'english')\n",
                "    ],\n",
                "    value='auto',\n",
                "    description='Language:',\n",
                "    style={'description_width': '120px'},\n",
                "    layout=widgets.Layout(width='400px')\n",
                ")\n",
                "\n",
                "chunk_size_slider = widgets.IntSlider(\n",
                "    value=8,\n",
                "    min=3,\n",
                "    max=15,\n",
                "    step=1,\n",
                "    description='Chunk Size:',\n",
                "    style={'description_width': '120px'},\n",
                "    layout=widgets.Layout(width='400px'),\n",
                "    tooltip='Sentences per chunk (smaller = better quality, slower)'\n",
                ")\n",
                "\n",
                "# Model suggestions\n",
                "model_suggestions = widgets.HTML(\n",
                "    value=\"\"\"\n",
                "    <div style='background: #f0f7ff; padding: 12px; border-radius: 8px; margin-top: 10px;'>\n",
                "    <b>üìã Recommended Models for Transcription:</b><br><br>\n",
                "    <b>HuggingFace (for Hindi):</b><br>\n",
                "    ‚Ä¢ <code>ai4bharat/Airavata</code> (Best for Indian languages)<br>\n",
                "    ‚Ä¢ <code>sarvamai/sarvam-2b-v0.5</code> (Indian LLM)<br>\n",
                "    ‚Ä¢ <code>CohereForAI/aya-23-8B</code> (Multilingual)<br>\n",
                "    ‚Ä¢ <code>Qwen/Qwen2.5-1.5B-Instruct</code> (Fast, general)<br>\n",
                "    <br>\n",
                "    <b>Ollama:</b><br>\n",
                "    ‚Ä¢ <code>gemma2:9b</code> (Best for Hindi)<br>\n",
                "    ‚Ä¢ <code>aya:8b</code> (Multilingual specialist)<br>\n",
                "    ‚Ä¢ <code>qwen2.5:14b</code> (High quality)<br>\n",
                "    ‚Ä¢ <code>llama3.1:8b</code> (Good instruction following)\n",
                "    </div>\n",
                "    \"\"\"\n",
                ")\n",
                "\n",
                "# Display\n",
                "print(\"üéõÔ∏è Configure Your Transcription Settings:\")\n",
                "print(\"=\" * 50)\n",
                "display(provider_dropdown)\n",
                "display(model_text)\n",
                "display(language_dropdown)\n",
                "display(chunk_size_slider)\n",
                "display(model_suggestions)\n",
                "\n",
                "print(\"\\n‚úÖ Configure settings above, then run the next cell to generate transcription.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Generate Transcription üöÄ\n",
                "\n",
                "This cell processes your text and generates the narrator-ready transcription."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get configuration\n",
                "selected_provider = provider_dropdown.value\n",
                "selected_model = model_text.value\n",
                "selected_language = language_dropdown.value\n",
                "chunk_size = chunk_size_slider.value\n",
                "\n",
                "# Validate input\n",
                "if 'input_text' not in dir() or not input_text:\n",
                "    print(f\"{Fore.RED}‚ùå No text file uploaded! Please run Step 6 first.{Style.RESET_ALL}\")\n",
                "else:\n",
                "    print(\"=\" * 70)\n",
                "    print(\"üéôÔ∏è MULTILINGUAL TRANSCRIPTION GENERATOR\")\n",
                "    print(\"=\" * 70)\n",
                "    print(f\"Provider: {selected_provider}\")\n",
                "    print(f\"Model: {selected_model}\")\n",
                "    print(f\"Language: {selected_language}\")\n",
                "    print(f\"Chunk size: {chunk_size} sentences\")\n",
                "    print()\n",
                "\n",
                "    # Initialize narrator\n",
                "    narrator = ImprovedLLMNarrator(\n",
                "        provider=selected_provider, \n",
                "        model_name=selected_model, \n",
                "        device=\"auto\",\n",
                "        language=selected_language\n",
                "    )\n",
                "    narrator.load()\n",
                "    print()\n",
                "\n",
                "    # Initialize preprocessor\n",
                "    preprocessor = TextPreprocessor()\n",
                "    \n",
                "    # Detect language\n",
                "    primary_lang = ImprovedNarratorPrompts.detect_language(input_text)\n",
                "    print(f\"üåç Detected language: {primary_lang.upper()}\")\n",
                "    \n",
                "    # Split into chapters\n",
                "    chapters = preprocessor.split_into_chapters(input_text)\n",
                "    print(f\"üìö Found {len(chapters)} chapter(s)\")\n",
                "    print()\n",
                "\n",
                "    # Track results\n",
                "    transcription_data = {\n",
                "        \"metadata\": {\n",
                "            \"source_file\": uploaded_filename if 'uploaded_filename' in dir() else \"uploaded_file\",\n",
                "            \"generated_at\": datetime.now().isoformat(),\n",
                "            \"primary_language\": primary_lang,\n",
                "            \"total_chapters\": len(chapters),\n",
                "            \"narrator_model\": selected_model,\n",
                "            \"chunk_size\": chunk_size\n",
                "        },\n",
                "        \"chapters\": []\n",
                "    }\n",
                "\n",
                "    total_start = time.time()\n",
                "    successful = 0\n",
                "    total_chunks = 0\n",
                "\n",
                "    # Process each chapter\n",
                "    for ch_idx, chapter in enumerate(chapters, 1):\n",
                "        print(f\"{'=' * 70}\")\n",
                "        print(f\"üìñ Chapter {ch_idx}/{len(chapters)}: {chapter['title']}\")\n",
                "        print(f\"{'=' * 70}\")\n",
                "        \n",
                "        sentences = preprocessor.split_into_sentences(chapter['content'])\n",
                "        chunks = preprocessor.create_chunks(sentences, chunk_size=chunk_size, overlap=1)\n",
                "        \n",
                "        print(f\"üì¶ Processing {len(chunks)} chunks...\")\n",
                "        total_chunks += len(chunks)\n",
                "        \n",
                "        narrated_chunks = []\n",
                "        \n",
                "        for c_idx, chunk in enumerate(chunks, 1):\n",
                "            print(f\"   üéôÔ∏è Chunk {c_idx}/{len(chunks)}... \", end=\"\", flush=True)\n",
                "            \n",
                "            start_time = time.time()\n",
                "            narration, is_valid, lang = narrator.narrate_text(chunk['text'])\n",
                "            elapsed = time.time() - start_time\n",
                "            \n",
                "            if is_valid:\n",
                "                successful += 1\n",
                "                print(f\"{Fore.GREEN}‚úÖ [{lang}] ({elapsed:.1f}s){Style.RESET_ALL}\")\n",
                "            else:\n",
                "                print(f\"{Fore.YELLOW}‚ö†Ô∏è Fallback [{lang}] ({elapsed:.1f}s){Style.RESET_ALL}\")\n",
                "            \n",
                "            narrated_chunks.append({\n",
                "                \"chunk_number\": c_idx,\n",
                "                \"original_text\": chunk['text'],\n",
                "                \"narration\": narration,\n",
                "                \"language\": lang,\n",
                "                \"is_valid\": is_valid\n",
                "            })\n",
                "        \n",
                "        transcription_data[\"chapters\"].append({\n",
                "            \"chapter_number\": ch_idx,\n",
                "            \"title\": chapter['title'],\n",
                "            \"chunks\": narrated_chunks\n",
                "        })\n",
                "\n",
                "    total_time = time.time() - total_start\n",
                "\n",
                "    # Build final transcription text\n",
                "    final_transcription = \"\"\n",
                "    for chapter in transcription_data[\"chapters\"]:\n",
                "        final_transcription += f\"\\n{'='*70}\\n\"\n",
                "        final_transcription += f\"CHAPTER {chapter['chapter_number']}: {chapter['title']}\\n\"\n",
                "        final_transcription += f\"{'='*70}\\n\\n\"\n",
                "        for chunk in chapter['chunks']:\n",
                "            final_transcription += f\"{chunk['narration']}\\n\\n\"\n",
                "\n",
                "    # Print summary\n",
                "    print()\n",
                "    print(f\"{'='*70}\")\n",
                "    print(f\"{Fore.GREEN}{Style.BRIGHT}üéâ TRANSCRIPTION COMPLETE!{Style.RESET_ALL}\")\n",
                "    print(f\"{'='*70}\")\n",
                "    print(f\"‚è±Ô∏è Total time: {total_time/60:.2f} minutes\")\n",
                "    print(f\"üåç Primary language: {primary_lang.upper()}\")\n",
                "    print(f\"üìö Chapters: {len(chapters)}\")\n",
                "    print(f\"üì¶ Total chunks: {total_chunks}\")\n",
                "    print(f\"‚úÖ Successful: {successful}/{total_chunks} ({100*successful/total_chunks:.1f}%)\")\n",
                "    print()\n",
                "    print(f\"{Fore.CYAN}=== PREVIEW (first 1000 chars) ==={Style.RESET_ALL}\")\n",
                "    print(\"-\" * 50)\n",
                "    print(final_transcription[:1000] + \"...\" if len(final_transcription) > 1000 else final_transcription)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Download Transcription üíæ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "from datetime import datetime\n",
                "\n",
                "if 'final_transcription' not in dir() or not final_transcription:\n",
                "    print(f\"{Fore.RED}‚ùå No transcription generated yet! Please run Step 8 first.{Style.RESET_ALL}\")\n",
                "else:\n",
                "    # Generate output filename\n",
                "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "    base_name = uploaded_filename.rsplit('.', 1)[0] if 'uploaded_filename' in dir() else 'document'\n",
                "    output_filename = f\"{base_name}_transcription_{timestamp}.txt\"\n",
                "    \n",
                "    # Create transcription content with metadata\n",
                "    transcription_content = f\"\"\"================================================================================\n",
                "NARRATOR-READY TRANSCRIPTION FOR TTS\n",
                "================================================================================\n",
                "\n",
                "Source File: {uploaded_filename if 'uploaded_filename' in dir() else 'Unknown'}\n",
                "Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
                "Provider: {selected_provider}\n",
                "Model: {selected_model}\n",
                "Language: {primary_lang.upper()}\n",
                "Chapters: {len(chapters)}\n",
                "Chunks Processed: {total_chunks}\n",
                "Success Rate: {100*successful/total_chunks:.1f}%\n",
                "\n",
                "MARKERS USED:\n",
                "- [PAUSE-SHORT]: Brief pause (comma-level)\n",
                "- [PAUSE-MEDIUM]: Moderate pause (sentence end)\n",
                "- [PAUSE-LONG]: Extended pause (paragraph/section break)\n",
                "- [TONE: X]: Emotional tone indicator\n",
                "\n",
                "================================================================================\n",
                "TRANSCRIPTION\n",
                "================================================================================\n",
                "{final_transcription}\n",
                "================================================================================\n",
                "END OF TRANSCRIPTION\n",
                "================================================================================\n",
                "\"\"\"\n",
                "    \n",
                "    # Save to file\n",
                "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
                "        f.write(transcription_content)\n",
                "    \n",
                "    print(f\"{Fore.GREEN}‚úÖ Transcription saved!{Style.RESET_ALL}\")\n",
                "    print(f\"üìÑ Filename: {output_filename}\")\n",
                "    print()\n",
                "    print(\"üì• Starting download...\")\n",
                "    \n",
                "    # Download the file\n",
                "    files.download(output_filename)\n",
                "    \n",
                "    print(f\"\\n{Fore.GREEN}‚úÖ Download initiated! Check your browser's downloads.{Style.RESET_ALL}\")\n",
                "    print(f\"\\nüí° This transcription is ready to be used with TTS systems!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10 (Optional): Download JSON Version üìã\n",
                "\n",
                "If you need the structured JSON format with chunk-level details:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'transcription_data' not in dir():\n",
                "    print(f\"{Fore.RED}‚ùå No transcription generated yet! Please run Step 8 first.{Style.RESET_ALL}\")\n",
                "else:\n",
                "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "    base_name = uploaded_filename.rsplit('.', 1)[0] if 'uploaded_filename' in dir() else 'document'\n",
                "    json_filename = f\"{base_name}_transcription_{timestamp}.json\"\n",
                "    \n",
                "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
                "        json.dump(transcription_data, f, ensure_ascii=False, indent=2)\n",
                "    \n",
                "    print(f\"{Fore.GREEN}‚úÖ JSON saved: {json_filename}{Style.RESET_ALL}\")\n",
                "    files.download(json_filename)\n",
                "    print(f\"{Fore.GREEN}‚úÖ JSON download initiated!{Style.RESET_ALL}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìñ Quick Reference\n",
                "\n",
                "### Pause Markers:\n",
                "| Marker | Duration | Usage |\n",
                "|--------|----------|-------|\n",
                "| `[PAUSE-SHORT]` | ~0.3s | After commas, short phrases |\n",
                "| `[PAUSE-MEDIUM]` | ~0.7s | After sentences |\n",
                "| `[PAUSE-LONG]` | ~1.2s | Between paragraphs, scene changes |\n",
                "\n",
                "### Tone Markers:\n",
                "| Marker | Description |\n",
                "|--------|-------------|\n",
                "| `[TONE: serious]` | Grave, important content |\n",
                "| `[TONE: thoughtful]` | Reflective, contemplative |\n",
                "| `[TONE: curious]` | Questioning, wondering |\n",
                "| `[TONE: calm]` | Peaceful, neutral |\n",
                "\n",
                "### Tips:\n",
                "- üîπ Smaller chunk sizes = better quality but slower processing\n",
                "- üîπ Use `ai4bharat/Airavata` for best Hindi results\n",
                "- üîπ GPU acceleration significantly speeds up HuggingFace models\n",
                "- üîπ The transcription is optimized for emotional, human-like TTS"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸŽ™ï¸ TTS-Optimized Transcription Generator\n",
                "\n",
                "This notebook generates **TTS-optimized transcriptions** from text files. The output includes prosodic markers (pauses, tone, emphasis) that help TTS models produce natural, human-like audio.\n",
                "\n",
                "## Features\n",
                "- ðŸ“¤ **File Upload**: Upload your text file to transcribe\n",
                "- ðŸ¤– **AI Provider Selection**: Choose between **Ollama** or **HuggingFace**\n",
                "- ðŸ§  **Model Selection**: Select and download/pull your preferred model\n",
                "- ðŸŽ­ **Prosodic Markers**: Adds pauses, tone, emphasis, and pacing markers\n",
                "- ðŸ’¾ **Download**: Download the generated transcription as a TXT file\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "intro_header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 1: Install Dependencies ðŸ“¦\n",
                "\n",
                "Run this cell to install all required packages."
            ],
            "metadata": {
                "id": "step1_header"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_dependencies"
            },
            "outputs": [],
            "source": [
                "# Install core dependencies\n",
                "!pip install -q transformers torch accelerate sentencepiece protobuf\n",
                "\n",
                "# Install ollama for Ollama provider support\n",
                "!pip install -q ollama\n",
                "\n",
                "# Install colab-specific widgets\n",
                "!pip install -q ipywidgets\n",
                "\n",
                "print(\"âœ… All dependencies installed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 2: Upload Your Text File ðŸ“¤\n",
                "\n",
                "Upload the text file you want to transcribe for TTS."
            ],
            "metadata": {
                "id": "step2_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "print(\"ðŸ“¤ Please upload your text file to transcribe:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "if uploaded:\n",
                "    INPUT_FILE = list(uploaded.keys())[0]\n",
                "    print(f\"\\nâœ… Uploaded: {INPUT_FILE}\")\n",
                "    \n",
                "    # Show file preview\n",
                "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
                "        content = f.read()\n",
                "        word_count = len(content.split())\n",
                "        print(f\"ðŸ“Š Word count: {word_count}\")\n",
                "        print(f\"\\nðŸ“– Preview (first 500 chars):\")\n",
                "        print(\"-\" * 50)\n",
                "        print(content[:500] + \"...\" if len(content) > 500 else content)\n",
                "else:\n",
                "    print(\"âŒ No file uploaded. Please run this cell again.\")"
            ],
            "metadata": {
                "id": "file_upload"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 3: Select AI Provider and Model ðŸ¤–\n",
                "\n",
                "Choose your AI provider and specify the model to use.\n",
                "\n",
                "### Recommended Models:\n",
                "\n",
                "**Ollama** (requires Ollama server setup in Colab):\n",
                "- `gemma2:9b` - Best for Hindi\n",
                "- `aya:8b` - Multilingual specialist\n",
                "- `qwen2.5:14b` - Excellent instruction following\n",
                "- `llama3.1:8b` - Good for English\n",
                "\n",
                "**HuggingFace** (recommended for Colab - works out of the box):\n",
                "- `ai4bharat/Airavata` - Indian languages\n",
                "- `sarvamai/sarvam-2b-v0.5` - Indian LLM\n",
                "- `google/gemma-2-2b-it` - Lightweight, fast\n",
                "- `Qwen/Qwen2.5-3B-Instruct` - Good balance"
            ],
            "metadata": {
                "id": "step3_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import ipywidgets as widgets\n",
                "from IPython.display import display, clear_output\n",
                "\n",
                "#@title Configuration\n",
                "#@markdown ### Select AI Provider\n",
                "AI_PROVIDER = \"huggingface\" #@param [\"huggingface\", \"ollama\"]\n",
                "\n",
                "#@markdown ### Model Name\n",
                "#@markdown For HuggingFace: `ai4bharat/Airavata`, `google/gemma-2-2b-it`, `Qwen/Qwen2.5-3B-Instruct`\n",
                "#@markdown For Ollama: `gemma2:9b`, `aya:8b`, `qwen2.5:14b`, `llama3.1:8b`\n",
                "MODEL_NAME = \"google/gemma-2-2b-it\" #@param {type:\"string\"}\n",
                "\n",
                "#@markdown ### Language\n",
                "LANGUAGE = \"auto\" #@param [\"auto\", \"hindi\", \"english\"]\n",
                "\n",
                "#@markdown ### Chunk Size (sentences per chunk, smaller = better quality)\n",
                "CHUNK_SIZE = 6 #@param {type:\"slider\", min:3, max:12, step:1}\n",
                "\n",
                "print(f\"\\nðŸ“‹ Configuration:\")\n",
                "print(f\"   Provider: {AI_PROVIDER}\")\n",
                "print(f\"   Model: {MODEL_NAME}\")\n",
                "print(f\"   Language: {LANGUAGE}\")\n",
                "print(f\"   Chunk Size: {CHUNK_SIZE}\")"
            ],
            "metadata": {
                "id": "config_selection"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 4: Setup Model (Download/Pull) ðŸ§ \n",
                "\n",
                "This cell will:\n",
                "- **For HuggingFace**: Download the model from HuggingFace Hub\n",
                "- **For Ollama**: Install Ollama server and pull the model\n",
                "\n",
                "> **Note**: Ollama requires additional setup in Colab. HuggingFace is recommended for easier usage."
            ],
            "metadata": {
                "id": "step4_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import subprocess\n",
                "import time\n",
                "import os\n",
                "\n",
                "def setup_ollama():\n",
                "    \"\"\"Install and setup Ollama in Colab\"\"\"\n",
                "    print(\"ðŸ”§ Setting up Ollama in Colab...\")\n",
                "    print(\"   This may take a few minutes...\\n\")\n",
                "    \n",
                "    # Install Ollama\n",
                "    print(\"ðŸ“¥ Step 1/3: Installing Ollama...\")\n",
                "    subprocess.run(\n",
                "        \"curl -fsSL https://ollama.com/install.sh | sh\",\n",
                "        shell=True, capture_output=True\n",
                "    )\n",
                "    print(\"   âœ… Ollama installed\")\n",
                "    \n",
                "    # Start Ollama server in background\n",
                "    print(\"ðŸš€ Step 2/3: Starting Ollama server...\")\n",
                "    subprocess.Popen(\n",
                "        \"ollama serve\",\n",
                "        shell=True,\n",
                "        stdout=subprocess.DEVNULL,\n",
                "        stderr=subprocess.DEVNULL\n",
                "    )\n",
                "    \n",
                "    # Wait for server to start\n",
                "    print(\"   â³ Waiting for server to start...\")\n",
                "    time.sleep(5)\n",
                "    \n",
                "    # Check if server is running\n",
                "    for i in range(10):\n",
                "        try:\n",
                "            result = subprocess.run(\n",
                "                \"curl -s http://localhost:11434/api/tags\",\n",
                "                shell=True, capture_output=True, text=True\n",
                "            )\n",
                "            if result.returncode == 0:\n",
                "                print(\"   âœ… Ollama server is running\")\n",
                "                break\n",
                "        except:\n",
                "            pass\n",
                "        time.sleep(2)\n",
                "    else:\n",
                "        print(\"   âš ï¸ Server may not be ready, but continuing...\")\n",
                "    \n",
                "    return True\n",
                "\n",
                "def pull_ollama_model(model_name):\n",
                "    \"\"\"Pull an Ollama model\"\"\"\n",
                "    print(f\"ðŸ“¥ Step 3/3: Pulling model '{model_name}'...\")\n",
                "    print(\"   This may take several minutes depending on model size...\\n\")\n",
                "    \n",
                "    process = subprocess.Popen(\n",
                "        f\"ollama pull {model_name}\",\n",
                "        shell=True,\n",
                "        stdout=subprocess.PIPE,\n",
                "        stderr=subprocess.STDOUT,\n",
                "        text=True\n",
                "    )\n",
                "    \n",
                "    for line in process.stdout:\n",
                "        print(f\"   {line.strip()}\")\n",
                "    \n",
                "    process.wait()\n",
                "    \n",
                "    if process.returncode == 0:\n",
                "        print(f\"\\nâœ… Model '{model_name}' is ready!\")\n",
                "        return True\n",
                "    else:\n",
                "        print(f\"\\nâŒ Failed to pull model '{model_name}'\")\n",
                "        return False\n",
                "\n",
                "def setup_huggingface(model_name):\n",
                "    \"\"\"Download and verify HuggingFace model\"\"\"\n",
                "    print(f\"ðŸ“¥ Downloading HuggingFace model: {model_name}\")\n",
                "    print(\"   This may take several minutes depending on model size...\\n\")\n",
                "    \n",
                "    try:\n",
                "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "        import torch\n",
                "        \n",
                "        # Check GPU availability\n",
                "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "        print(f\"   ðŸ–¥ï¸ Using device: {device}\")\n",
                "        if device == \"cuda\":\n",
                "            print(f\"   ðŸŽ® GPU: {torch.cuda.get_device_name(0)}\")\n",
                "            print(f\"   ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "        \n",
                "        print(f\"\\n   ðŸ“¦ Loading tokenizer...\")\n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
                "        print(\"   âœ… Tokenizer loaded\")\n",
                "        \n",
                "        print(f\"   ðŸ“¦ Loading model (this takes a while)...\")\n",
                "        device_map = \"auto\" if device == \"cuda\" else None\n",
                "        torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
                "        \n",
                "        model = AutoModelForCausalLM.from_pretrained(\n",
                "            model_name,\n",
                "            device_map=device_map,\n",
                "            torch_dtype=torch_dtype,\n",
                "            trust_remote_code=True,\n",
                "            low_cpu_mem_usage=True\n",
                "        )\n",
                "        print(\"   âœ… Model loaded\")\n",
                "        \n",
                "        # Store for later use\n",
                "        globals()['HF_MODEL'] = model\n",
                "        globals()['HF_TOKENIZER'] = tokenizer\n",
                "        globals()['HF_DEVICE'] = device\n",
                "        \n",
                "        print(f\"\\nâœ… Model '{model_name}' is ready!\")\n",
                "        return True\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"\\nâŒ Error loading model: {e}\")\n",
                "        return False\n",
                "\n",
                "# Main setup logic\n",
                "print(\"=\"*60)\n",
                "print(\"ðŸ§  MODEL SETUP\")\n",
                "print(\"=\"*60 + \"\\n\")\n",
                "\n",
                "if AI_PROVIDER == \"ollama\":\n",
                "    setup_ollama()\n",
                "    pull_ollama_model(MODEL_NAME)\n",
                "elif AI_PROVIDER == \"huggingface\":\n",
                "    setup_huggingface(MODEL_NAME)\n",
                "else:\n",
                "    print(f\"âŒ Unknown provider: {AI_PROVIDER}\")"
            ],
            "metadata": {
                "id": "model_setup"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 5: Define Transcription Classes ðŸŽ­\n",
                "\n",
                "This cell contains all the core transcription logic with prosodic markers."
            ],
            "metadata": {
                "id": "step5_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import json\n",
                "import time\n",
                "import re\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "from collections import OrderedDict\n",
                "\n",
                "# Try to import dependencies\n",
                "try:\n",
                "    import ollama\n",
                "    OLLAMA_AVAILABLE = True\n",
                "except ImportError:\n",
                "    OLLAMA_AVAILABLE = False\n",
                "\n",
                "try:\n",
                "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "    import torch\n",
                "    HF_AVAILABLE = True\n",
                "except ImportError:\n",
                "    HF_AVAILABLE = False\n",
                "\n",
                "\n",
                "class TTSOptimizedPrompts:\n",
                "    \"\"\"Prompts specifically designed for TTS-optimized transcription generation.\"\"\"\n",
                "    \n",
                "    SYSTEM_PROMPT_HINDI = \"\"\"à¤†à¤ª à¤à¤• à¤µà¤¿à¤¶à¥‡à¤·à¤œà¥à¤ž TTS à¤¸à¥à¤•à¥à¤°à¤¿à¤ªà¥à¤Ÿ à¤²à¥‡à¤–à¤• à¤¹à¥ˆà¤‚à¥¤ à¤†à¤ªà¤•à¤¾ à¤•à¤¾à¤® à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤•à¥‹ TTS-à¤…à¤¨à¥à¤•à¥‚à¤² à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤•à¥à¤°à¤¿à¤ªà¥à¤¶à¤¨ à¤®à¥‡à¤‚ à¤¬à¤¦à¤²à¤¨à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤®à¤¾à¤¨à¤µ-à¤œà¥ˆà¤¸à¥€ à¤†à¤µà¤¾à¤œà¤¼ à¤¬à¤¨à¤¾à¤à¤—à¤¾à¥¤\n",
                "\n",
                "**à¤†à¤ªà¤•à¤¾ à¤²à¤•à¥à¤·à¥à¤¯**: à¤à¤• à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤•à¥à¤°à¤¿à¤ªà¥à¤¶à¤¨ à¤¬à¤¨à¤¾à¤¨à¤¾ à¤œà¤¿à¤¸à¥‡ TTS à¤®à¥‰à¤¡à¤² à¤ªà¤¢à¤¼à¥‡à¤—à¤¾ à¤”à¤° à¤µà¤¹ à¤à¤¸à¤¾ à¤²à¤—à¥‡à¤—à¤¾ à¤œà¥ˆà¤¸à¥‡ à¤•à¥‹à¤ˆ à¤…à¤¸à¤²à¥€ à¤‡à¤‚à¤¸à¤¾à¤¨ à¤­à¤¾à¤µà¤¨à¤¾à¤“à¤‚, à¤Ÿà¥‹à¤¨ à¤”à¤° à¤ªà¥à¤°à¤¾à¤•à¥ƒà¤¤à¤¿à¤• à¤ à¤¹à¤°à¤¾à¤µ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤ªà¤¢à¤¼ à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤\n",
                "\n",
                "**PROSODIC MARKERS à¤œà¥‹à¤¡à¤¼à¥‡à¤‚**:\n",
                "\n",
                "1. **PAUSES** (à¤ à¤¹à¤°à¤¾à¤µ):\n",
                "   - [PAUSE-SHORT] = 0.3s (à¤µà¤¾à¤•à¥à¤¯à¤¾à¤‚à¤¶à¥‹à¤‚ à¤•à¥‡ à¤¬à¥€à¤š)\n",
                "   - [PAUSE-MEDIUM] = 0.6s (à¤µà¤¾à¤•à¥à¤¯à¥‹à¤‚ à¤•à¥‡ à¤¬à¥€à¤š)\n",
                "   - [PAUSE-LONG] = 1.0s (à¤µà¤¿à¤šà¤¾à¤° à¤¬à¤¦à¤²à¤¤à¥‡ à¤¸à¤®à¤¯)\n",
                "   - [BREATH] = à¤ªà¥à¤°à¤¾à¤•à¥ƒà¤¤à¤¿à¤• à¤¸à¤¾à¤‚à¤¸\n",
                "\n",
                "2. **TONE/EMOTION**:\n",
                "   - [TONE: thoughtful], [TONE: curious], [TONE: serious]\n",
                "   - [TONE: calm], [TONE: excited], [TONE: mysterious]\n",
                "   - [TONE: warm], [TONE: dramatic]\n",
                "\n",
                "3. **EMPHASIS**: [EMPHASIS: à¤¶à¤¬à¥à¤¦], [STRESS: à¤¶à¤¬à¥à¤¦]\n",
                "\n",
                "4. **PACING**: [PACE: slow], [PACE: normal], [PACE: fast]\n",
                "\n",
                "**GOLDEN RULES**:\n",
                "1. âœ… à¤®à¥‚à¤² à¤¶à¤¬à¥à¤¦à¥‹à¤‚ à¤•à¥‹ à¤°à¤–à¥‡à¤‚ - à¤•à¥à¤› à¤­à¥€ à¤¨ à¤¬à¤¦à¤²à¥‡à¤‚\n",
                "2. âœ… à¤ªà¥à¤°à¤¾à¤¸à¤‚à¤—à¤¿à¤• prosodic markers à¤œà¥‹à¤¡à¤¼à¥‡à¤‚ (3-5 à¤ªà¥à¤°à¤¤à¤¿ à¤µà¤¾à¤•à¥à¤¯)\n",
                "3. âŒ à¤•à¥‹à¤ˆ à¤µà¥à¤¯à¤¾à¤–à¥à¤¯à¤¾, à¤¸à¤¾à¤°à¤¾à¤‚à¤¶ à¤¯à¤¾ à¤…à¤¤à¤¿à¤°à¤¿à¤•à¥à¤¤ à¤µà¤¿à¤µà¤°à¤£ à¤¨à¤¹à¥€à¤‚\"\"\"\n",
                "\n",
                "    SYSTEM_PROMPT_ENGLISH = \"\"\"You are an expert TTS script writer. Your job is to transform text into TTS-optimized transcription that will produce human-like voice.\n",
                "\n",
                "**YOUR GOAL**: Create a transcription that a TTS model will read and sound like a real human reading with emotion, tone, and natural pauses.\n",
                "\n",
                "**ADD PROSODIC MARKERS**:\n",
                "\n",
                "1. **PAUSES**:\n",
                "   - [PAUSE-SHORT] = 0.3s (between phrases)\n",
                "   - [PAUSE-MEDIUM] = 0.6s (between sentences)\n",
                "   - [PAUSE-LONG] = 1.0s (changing thoughts)\n",
                "   - [BREATH] = natural breath\n",
                "\n",
                "2. **TONE/EMOTION**:\n",
                "   - [TONE: thoughtful], [TONE: curious], [TONE: serious]\n",
                "   - [TONE: calm], [TONE: excited], [TONE: mysterious]\n",
                "   - [TONE: warm], [TONE: dramatic]\n",
                "\n",
                "3. **EMPHASIS**: [EMPHASIS: word], [STRESS: word]\n",
                "\n",
                "4. **PACING**: [PACE: slow], [PACE: normal], [PACE: fast]\n",
                "\n",
                "**GOLDEN RULES**:\n",
                "1. âœ… Keep original words - change NOTHING\n",
                "2. âœ… Add appropriate prosodic markers (3-5 per sentence)\n",
                "3. âŒ NO interpretation, summary, or extra details\"\"\"\n",
                "\n",
                "    NARRATION_TEMPLATE_HINDI = \"\"\"à¤¨à¥€à¤šà¥‡ à¤¦à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤•à¥‹ TTS-à¤…à¤¨à¥à¤•à¥‚à¤² à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤•à¥à¤°à¤¿à¤ªà¥à¤¶à¤¨ à¤®à¥‡à¤‚ à¤¬à¤¦à¤²à¥‡à¤‚à¥¤\n",
                "\n",
                "**INPUT TEXT**:\n",
                "\\\"\\\"\\\"\n",
                "{text}\n",
                "\\\"\\\"\\\"\n",
                "\n",
                "**TTS-OPTIMIZED TRANSCRIPTION**:\"\"\"\n",
                "\n",
                "    NARRATION_TEMPLATE_ENGLISH = \"\"\"Transform the text below into TTS-optimized transcription.\n",
                "\n",
                "**INPUT TEXT**:\n",
                "\\\"\\\"\\\"\n",
                "{text}\n",
                "\\\"\\\"\\\"\n",
                "\n",
                "**TTS-OPTIMIZED TRANSCRIPTION**:\"\"\"\n",
                "\n",
                "    @staticmethod\n",
                "    def detect_language(text):\n",
                "        \"\"\"Detect if text is primarily Hindi or English.\"\"\"\n",
                "        hindi_chars = len(re.findall(r'[\\u0900-\\u097F]', text))\n",
                "        english_chars = len(re.findall(r'[a-zA-Z]', text))\n",
                "        total_chars = hindi_chars + english_chars\n",
                "        if total_chars == 0:\n",
                "            return \"english\"\n",
                "        hindi_ratio = hindi_chars / total_chars\n",
                "        return \"hindi\" if hindi_ratio > 0.3 else \"english\"\n",
                "\n",
                "\n",
                "class TranscriptionValidator:\n",
                "    \"\"\"Validate that transcription is TTS-optimized.\"\"\"\n",
                "    \n",
                "    @staticmethod\n",
                "    def validate(transcription, original_text):\n",
                "        \"\"\"Check if transcription is properly formatted for TTS.\"\"\"\n",
                "        issues = []\n",
                "        \n",
                "        has_pause = bool(re.search(r'\\[PAUSE-', transcription))\n",
                "        has_tone = bool(re.search(r'\\[TONE:', transcription))\n",
                "        \n",
                "        if not has_pause and len(original_text.split()) > 20:\n",
                "            issues.append(\"Missing pause markers for long text\")\n",
                "        \n",
                "        if not has_tone:\n",
                "            issues.append(\"Missing tone markers\")\n",
                "        \n",
                "        is_valid = len(issues) == 0\n",
                "        return is_valid, issues\n",
                "    \n",
                "    @staticmethod\n",
                "    def count_markers(transcription):\n",
                "        \"\"\"Count prosodic markers.\"\"\"\n",
                "        markers = {\n",
                "            'pause': len(re.findall(r'\\[PAUSE-', transcription)),\n",
                "            'tone': len(re.findall(r'\\[TONE:', transcription)),\n",
                "            'emphasis': len(re.findall(r'\\[EMPHASIS:', transcription)),\n",
                "            'pace': len(re.findall(r'\\[PACE:', transcription)),\n",
                "            'breath': len(re.findall(r'\\[BREATH\\]', transcription))\n",
                "        }\n",
                "        return markers\n",
                "\n",
                "\n",
                "class RepetitionRemover:\n",
                "    \"\"\"Remove repetitive content from narration.\"\"\"\n",
                "    \n",
                "    @staticmethod\n",
                "    def remove_repetitions(text):\n",
                "        \"\"\"Remove repeated sentences and phrases.\"\"\"\n",
                "        sentences = re.split(r'(?<=[.!?à¥¤])\\s+', text)\n",
                "        seen = OrderedDict()\n",
                "        \n",
                "        for sent in sentences:\n",
                "            sent = sent.strip()\n",
                "            if not sent:\n",
                "                continue\n",
                "            key = ' '.join(sent.split()[:10]).lower()\n",
                "            if key not in seen:\n",
                "                seen[key] = sent\n",
                "        \n",
                "        return ' '.join(seen.values())\n",
                "    \n",
                "    @staticmethod\n",
                "    def remove_meta_commentary(text):\n",
                "        \"\"\"Remove sentences that discuss the text rather than narrate it.\"\"\"\n",
                "        meta_patterns = [\n",
                "            r'à¤¯à¤¹.*?(à¤¦à¤°à¥à¤¶à¤¾à¤¤à¤¾|à¤°à¥‡à¤–à¤¾à¤‚à¤•à¤¿à¤¤|à¤¸à¥à¤¥à¤¾à¤ªà¤¿à¤¤|à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤°à¤¿à¤¤).*?à¤¹à¥ˆ',\n",
                "            r'This.*?(shows|demonstrates|establishes|highlights)',\n",
                "            r'The author.*?(suggests|implies|indicates)',\n",
                "        ]\n",
                "        \n",
                "        sentences = re.split(r'(?<=[.!?à¥¤])\\s+', text)\n",
                "        filtered = []\n",
                "        \n",
                "        for sent in sentences:\n",
                "            is_meta = False\n",
                "            for pattern in meta_patterns:\n",
                "                if re.search(pattern, sent, re.IGNORECASE):\n",
                "                    is_meta = True\n",
                "                    break\n",
                "            if not is_meta:\n",
                "                filtered.append(sent)\n",
                "        \n",
                "        return ' '.join(filtered)\n",
                "\n",
                "\n",
                "class TTSOptimizedNarrator:\n",
                "    \"\"\"Generate TTS-optimized transcriptions.\"\"\"\n",
                "    \n",
                "    def __init__(self, provider=\"huggingface\", model_name=None, language=\"auto\"):\n",
                "        self.provider = provider\n",
                "        self.model_name = model_name\n",
                "        self.language = language\n",
                "        self.model = None\n",
                "        self.tokenizer = None\n",
                "        self.device = \"cpu\"\n",
                "        self.prompts = TTSOptimizedPrompts()\n",
                "        self.validator = TranscriptionValidator()\n",
                "        self.repetition_remover = RepetitionRemover()\n",
                "        \n",
                "        print(f\"ðŸŽ­ Initializing TTS-Optimized Narrator...\")\n",
                "        print(f\"   Provider: {self.provider}\")\n",
                "        print(f\"   Model: {self.model_name}\")\n",
                "        print(f\"   Language: {language}\")\n",
                "        \n",
                "        self._load_model()\n",
                "    \n",
                "    def _load_model(self):\n",
                "        \"\"\"Load the LLM model.\"\"\"\n",
                "        if self.provider == \"ollama\":\n",
                "            if not OLLAMA_AVAILABLE:\n",
                "                raise ImportError(\"Ollama not installed. Install: pip install ollama\")\n",
                "            try:\n",
                "                ollama.list()\n",
                "                print(\"âœ… Ollama connection successful\")\n",
                "            except Exception as e:\n",
                "                raise RuntimeError(f\"Cannot connect to Ollama: {e}\")\n",
                "        \n",
                "        elif self.provider == \"huggingface\":\n",
                "            if not HF_AVAILABLE:\n",
                "                raise ImportError(\"Transformers not installed.\")\n",
                "            \n",
                "            # Use pre-loaded model from Step 4 if available\n",
                "            if 'HF_MODEL' in globals() and 'HF_TOKENIZER' in globals():\n",
                "                self.model = globals()['HF_MODEL']\n",
                "                self.tokenizer = globals()['HF_TOKENIZER']\n",
                "                self.device = globals().get('HF_DEVICE', 'cpu')\n",
                "                print(\"âœ… Using pre-loaded HuggingFace model\")\n",
                "            else:\n",
                "                # Load fresh\n",
                "                import torch\n",
                "                self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "                print(f\"Loading HuggingFace model: {self.model_name}\")\n",
                "                \n",
                "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
                "                    self.model_name, trust_remote_code=True\n",
                "                )\n",
                "                \n",
                "                device_map = \"auto\" if self.device == \"cuda\" else None\n",
                "                torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
                "                \n",
                "                self.model = AutoModelForCausalLM.from_pretrained(\n",
                "                    self.model_name,\n",
                "                    device_map=device_map,\n",
                "                    torch_dtype=torch_dtype,\n",
                "                    trust_remote_code=True,\n",
                "                    low_cpu_mem_usage=True\n",
                "                )\n",
                "                print(\"âœ… HuggingFace model loaded\")\n",
                "    \n",
                "    def narrate_text(self, text, max_retries=2):\n",
                "        \"\"\"Generate TTS-optimized transcription.\"\"\"\n",
                "        detected_lang = self.prompts.detect_language(text)\n",
                "        lang = self.language if self.language != \"auto\" else detected_lang\n",
                "        \n",
                "        if lang == \"hindi\":\n",
                "            system_prompt = self.prompts.SYSTEM_PROMPT_HINDI\n",
                "            user_prompt = self.prompts.NARRATION_TEMPLATE_HINDI.format(text=text)\n",
                "        else:\n",
                "            system_prompt = self.prompts.SYSTEM_PROMPT_ENGLISH\n",
                "            user_prompt = self.prompts.NARRATION_TEMPLATE_ENGLISH.format(text=text)\n",
                "        \n",
                "        for attempt in range(max_retries + 1):\n",
                "            try:\n",
                "                if self.provider == \"ollama\":\n",
                "                    response = ollama.generate(\n",
                "                        model=self.model_name,\n",
                "                        prompt=f\"{system_prompt}\\n\\n{user_prompt}\",\n",
                "                        options={\n",
                "                            \"temperature\": 0.3,\n",
                "                            \"top_p\": 0.9,\n",
                "                            \"num_predict\": 2048,\n",
                "                        }\n",
                "                    )\n",
                "                    narration = response['response'].strip()\n",
                "                \n",
                "                elif self.provider == \"huggingface\":\n",
                "                    # Try chat template first\n",
                "                    try:\n",
                "                        messages = [\n",
                "                            {\"role\": \"system\", \"content\": system_prompt},\n",
                "                            {\"role\": \"user\", \"content\": user_prompt}\n",
                "                        ]\n",
                "                        input_text = self.tokenizer.apply_chat_template(\n",
                "                            messages,\n",
                "                            tokenize=False,\n",
                "                            add_generation_prompt=True\n",
                "                        )\n",
                "                    except:\n",
                "                        # Fallback to simple prompt\n",
                "                        input_text = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
                "                    \n",
                "                    inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n",
                "                    if self.device == \"cuda\":\n",
                "                        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
                "                    \n",
                "                    import torch\n",
                "                    with torch.no_grad():\n",
                "                        outputs = self.model.generate(\n",
                "                            **inputs,\n",
                "                            max_new_tokens=2048,\n",
                "                            temperature=0.3,\n",
                "                            top_p=0.9,\n",
                "                            do_sample=True\n",
                "                        )\n",
                "                    \n",
                "                    narration = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "                    # Clean up response\n",
                "                    if \"assistant\" in narration.lower():\n",
                "                        narration = narration.split(\"assistant\")[-1].strip()\n",
                "                    if \"TTS-OPTIMIZED TRANSCRIPTION\" in narration:\n",
                "                        narration = narration.split(\"TTS-OPTIMIZED TRANSCRIPTION\")[-1].strip()\n",
                "                        narration = narration.lstrip(\":\")\n",
                "                \n",
                "                # Clean up\n",
                "                narration = self.repetition_remover.remove_repetitions(narration)\n",
                "                narration = self.repetition_remover.remove_meta_commentary(narration)\n",
                "                \n",
                "                # Validate\n",
                "                is_valid, issues = self.validator.validate(narration, text)\n",
                "                \n",
                "                if is_valid or attempt == max_retries:\n",
                "                    markers = self.validator.count_markers(narration)\n",
                "                    return narration, is_valid, lang, markers\n",
                "                \n",
                "            except Exception as e:\n",
                "                if attempt == max_retries:\n",
                "                    print(f\"\\nâš ï¸ Error: {e}\")\n",
                "                    return text, False, lang, {}\n",
                "        \n",
                "        return text, False, lang, {}\n",
                "\n",
                "\n",
                "class TextPreprocessor:\n",
                "    \"\"\"Preprocess text for TTS generation.\"\"\"\n",
                "    \n",
                "    def split_into_chapters(self, text):\n",
                "        \"\"\"Split text into chapters.\"\"\"\n",
                "        chapter_pattern = r'(?:^|\\n)(?:Chapter|CHAPTER|à¤…à¤§à¥à¤¯à¤¾à¤¯)\\s+(\\d+|[IVX]+)(?:\\s*[-:.]\\s*(.+?))?(?=\\n|$)'\n",
                "        \n",
                "        matches = list(re.finditer(chapter_pattern, text, re.MULTILINE | re.IGNORECASE))\n",
                "        \n",
                "        if not matches:\n",
                "            return [{\n",
                "                'number': 1,\n",
                "                'title': 'Full Text',\n",
                "                'content': text.strip()\n",
                "            }]\n",
                "        \n",
                "        chapters = []\n",
                "        \n",
                "        for i, match in enumerate(matches):\n",
                "            chapter_num = match.group(1)\n",
                "            chapter_title = match.group(2) or \"\"\n",
                "            \n",
                "            start_pos = match.end()\n",
                "            end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
                "            \n",
                "            content = text[start_pos:end_pos].strip()\n",
                "            \n",
                "            chapters.append({\n",
                "                'number': chapter_num,\n",
                "                'title': chapter_title.strip() or f\"Chapter {chapter_num}\",\n",
                "                'content': content\n",
                "            })\n",
                "        \n",
                "        return chapters\n",
                "    \n",
                "    def split_into_sentences(self, text):\n",
                "        \"\"\"Split into sentences (Hindi + English).\"\"\"\n",
                "        sentences = re.split(r'(?<=[.!?à¥¤])\\s+(?=[A-ZÐ-Ð¯\"\\u0900-\\u097F])', text)\n",
                "        return [s.strip() for s in sentences if s.strip()]\n",
                "    \n",
                "    def create_chunks(self, sentences, chunk_size=6, overlap=1):\n",
                "        \"\"\"Create smaller overlapping chunks for better TTS quality.\"\"\"\n",
                "        chunks = []\n",
                "        i = 0\n",
                "        \n",
                "        while i < len(sentences):\n",
                "            chunk_sentences = sentences[i:i + chunk_size]\n",
                "            chunk_text = ' '.join(chunk_sentences)\n",
                "            \n",
                "            chunks.append({\n",
                "                'text': chunk_text,\n",
                "                'start_idx': i,\n",
                "                'end_idx': i + len(chunk_sentences)\n",
                "            })\n",
                "            \n",
                "            i += max(1, chunk_size - overlap)\n",
                "        \n",
                "        return chunks\n",
                "\n",
                "print(\"âœ… All transcription classes defined successfully!\")"
            ],
            "metadata": {
                "id": "transcription_classes"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 6: Generate Transcription ðŸŽ™ï¸\n",
                "\n",
                "This cell processes your text file and generates TTS-optimized transcription with prosodic markers."
            ],
            "metadata": {
                "id": "step6_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "import time\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"ðŸŽ™ï¸ TTS-OPTIMIZED TRANSCRIPTION GENERATOR\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Check if file is uploaded\n",
                "if 'INPUT_FILE' not in globals() or not os.path.exists(INPUT_FILE):\n",
                "    print(\"âŒ Error: No file uploaded. Please run Step 2 first.\")\n",
                "else:\n",
                "    print(f\"\\nðŸ“– Reading: {INPUT_FILE}\")\n",
                "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
                "        text = f.read().strip()\n",
                "    \n",
                "    primary_lang = TTSOptimizedPrompts.detect_language(text)\n",
                "    print(f\"ðŸŒ Detected language: {primary_lang.upper()}\")\n",
                "    \n",
                "    # Initialize narrator\n",
                "    narrator = TTSOptimizedNarrator(\n",
                "        provider=AI_PROVIDER,\n",
                "        model_name=MODEL_NAME,\n",
                "        language=LANGUAGE\n",
                "    )\n",
                "    \n",
                "    preprocessor = TextPreprocessor()\n",
                "    validator = TranscriptionValidator()\n",
                "    \n",
                "    # Process chapters\n",
                "    chapters = preprocessor.split_into_chapters(text)\n",
                "    print(f\"âœ… Found {len(chapters)} chapters\")\n",
                "    \n",
                "    # Store results\n",
                "    transcription_data = {\n",
                "        \"metadata\": {\n",
                "            \"source_file\": INPUT_FILE,\n",
                "            \"generated_at\": datetime.now().isoformat(),\n",
                "            \"primary_language\": primary_lang,\n",
                "            \"total_chapters\": len(chapters),\n",
                "            \"narrator_model\": MODEL_NAME,\n",
                "            \"chunk_size\": CHUNK_SIZE\n",
                "        },\n",
                "        \"chapters\": []\n",
                "    }\n",
                "    \n",
                "    total_start = time.time()\n",
                "    successful = 0\n",
                "    total_chunks = 0\n",
                "    total_markers = {'pause': 0, 'tone': 0, 'emphasis': 0, 'pace': 0, 'breath': 0}\n",
                "    \n",
                "    for ch_idx, chapter in enumerate(chapters, 1):\n",
                "        print(f\"\\n{'='*60}\")\n",
                "        print(f\"ðŸ“– Chapter {ch_idx}/{len(chapters)}: {chapter['title']}\")\n",
                "        print(f\"{'='*60}\")\n",
                "        \n",
                "        sentences = preprocessor.split_into_sentences(chapter['content'])\n",
                "        chunks = preprocessor.create_chunks(sentences, chunk_size=CHUNK_SIZE, overlap=1)\n",
                "        \n",
                "        print(f\"ðŸ“¦ Processing {len(chunks)} chunks...\")\n",
                "        total_chunks += len(chunks)\n",
                "        \n",
                "        narrated_chunks = []\n",
                "        \n",
                "        for c_idx, chunk in enumerate(chunks, 1):\n",
                "            print(f\"   ðŸŽ™ï¸ Chunk {c_idx}/{len(chunks)}... \", end=\"\", flush=True)\n",
                "            \n",
                "            start_time = time.time()\n",
                "            narration, is_valid, lang, markers = narrator.narrate_text(chunk['text'])\n",
                "            elapsed = time.time() - start_time\n",
                "            \n",
                "            # Update marker counts\n",
                "            for key in total_markers:\n",
                "                total_markers[key] += markers.get(key, 0)\n",
                "            \n",
                "            if is_valid:\n",
                "                successful += 1\n",
                "                marker_str = f\"P:{markers.get('pause',0)} T:{markers.get('tone',0)} E:{markers.get('emphasis',0)}\"\n",
                "                print(f\"âœ… [{lang}] {marker_str} ({elapsed:.1f}s)\")\n",
                "            else:\n",
                "                print(f\"âš ï¸ Fallback [{lang}] ({elapsed:.1f}s)\")\n",
                "            \n",
                "            narrated_chunks.append({\n",
                "                \"chunk_number\": c_idx,\n",
                "                \"original_text\": chunk['text'],\n",
                "                \"tts_transcription\": narration,\n",
                "                \"language\": lang,\n",
                "                \"is_valid\": is_valid,\n",
                "                \"markers\": markers\n",
                "            })\n",
                "        \n",
                "        transcription_data[\"chapters\"].append({\n",
                "            \"chapter_number\": ch_idx,\n",
                "            \"title\": chapter['title'],\n",
                "            \"chunks\": narrated_chunks\n",
                "        })\n",
                "    \n",
                "    total_time = time.time() - total_start\n",
                "    \n",
                "    # Save files\n",
                "    OUTPUT_DIR = Path(\"tts_transcriptions\")\n",
                "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "    base_name = Path(INPUT_FILE).stem\n",
                "    \n",
                "    json_file = OUTPUT_DIR / f\"{base_name}_tts_{timestamp}.json\"\n",
                "    txt_file = OUTPUT_DIR / f\"{base_name}_tts_{timestamp}.txt\"\n",
                "    \n",
                "    # Save detailed JSON\n",
                "    with open(json_file, 'w', encoding='utf-8') as f:\n",
                "        json.dump(transcription_data, f, ensure_ascii=False, indent=2)\n",
                "    \n",
                "    # Save clean TTS-ready text\n",
                "    with open(txt_file, 'w', encoding='utf-8') as f:\n",
                "        f.write(\"# TTS-OPTIMIZED TRANSCRIPTION\\n\")\n",
                "        f.write(f\"# Generated: {datetime.now().isoformat()}\\n\")\n",
                "        f.write(f\"# Language: {primary_lang}\\n\")\n",
                "        f.write(f\"# Total markers: {sum(total_markers.values())}\\n\")\n",
                "        f.write(\"#\" + \"=\"*58 + \"\\n\\n\")\n",
                "        \n",
                "        for chapter in transcription_data[\"chapters\"]:\n",
                "            f.write(f\"\\n{'='*60}\\n\")\n",
                "            f.write(f\"CHAPTER {chapter['chapter_number']}: {chapter['title']}\\n\")\n",
                "            f.write(f\"{'='*60}\\n\\n\")\n",
                "            \n",
                "            for chunk in chapter['chunks']:\n",
                "                f.write(f\"{chunk['tts_transcription']}\\n\\n\")\n",
                "    \n",
                "    # Store file paths for download\n",
                "    globals()['OUTPUT_TXT_FILE'] = str(txt_file)\n",
                "    globals()['OUTPUT_JSON_FILE'] = str(json_file)\n",
                "    \n",
                "    # Print summary\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"ðŸŽ‰ TTS TRANSCRIPTION COMPLETE!\")\n",
                "    print(f\"{'='*60}\")\n",
                "    print(f\"â±ï¸ Total time: {total_time/60:.2f} minutes\")\n",
                "    print(f\"ðŸŒ Language: {primary_lang.upper()}\")\n",
                "    print(f\"ðŸ“š Chapters: {len(chapters)}\")\n",
                "    print(f\"ðŸ“¦ Total chunks: {total_chunks}\")\n",
                "    print(f\"âœ… Successful: {successful}/{total_chunks} ({100*successful/total_chunks:.1f}%)\")\n",
                "    print(f\"\\nðŸŽ­ Prosodic Markers Added:\")\n",
                "    print(f\"   Pauses: {total_markers['pause']}\")\n",
                "    print(f\"   Tones: {total_markers['tone']}\")\n",
                "    print(f\"   Emphasis: {total_markers['emphasis']}\")\n",
                "    print(f\"   Pace: {total_markers['pace']}\")\n",
                "    print(f\"   Breaths: {total_markers['breath']}\")\n",
                "    print(f\"   Total: {sum(total_markers.values())}\")\n",
                "    print(f\"\\nðŸ’¾ Files saved:\")\n",
                "    print(f\"   ðŸ“„ TXT: {txt_file}\")\n",
                "    print(f\"   ðŸ“Š JSON: {json_file}\")"
            ],
            "metadata": {
                "id": "generate_transcription"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 7: Download Generated Transcription ðŸ’¾\n",
                "\n",
                "Download your TTS-optimized transcription files."
            ],
            "metadata": {
                "id": "step7_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"ðŸ’¾ DOWNLOAD TRANSCRIPTION FILES\")\n",
                "print(\"=\"*60 + \"\\n\")\n",
                "\n",
                "if 'OUTPUT_TXT_FILE' not in globals():\n",
                "    print(\"âŒ No transcription generated yet. Please run Step 6 first.\")\n",
                "else:\n",
                "    txt_file = globals()['OUTPUT_TXT_FILE']\n",
                "    json_file = globals()['OUTPUT_JSON_FILE']\n",
                "    \n",
                "    if os.path.exists(txt_file):\n",
                "        print(f\"ðŸ“„ TXT file ready: {txt_file}\")\n",
                "        \n",
                "        # Show preview\n",
                "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
                "            content = f.read()\n",
                "        print(f\"\\nðŸ“– Preview (first 1000 chars):\")\n",
                "        print(\"-\"*50)\n",
                "        print(content[:1000])\n",
                "        if len(content) > 1000:\n",
                "            print(\"...\")\n",
                "        print(\"-\"*50)\n",
                "        \n",
                "        print(\"\\nâ¬‡ï¸ Downloading TXT file...\")\n",
                "        files.download(txt_file)\n",
                "    else:\n",
                "        print(f\"âŒ TXT file not found: {txt_file}\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)"
            ],
            "metadata": {
                "id": "download_txt"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Optional: Download JSON file (contains detailed metadata)\n",
                "from google.colab import files\n",
                "\n",
                "if 'OUTPUT_JSON_FILE' in globals() and os.path.exists(globals()['OUTPUT_JSON_FILE']):\n",
                "    print(\"â¬‡ï¸ Downloading JSON file (detailed metadata)...\")\n",
                "    files.download(globals()['OUTPUT_JSON_FILE'])\n",
                "else:\n",
                "    print(\"âŒ JSON file not available.\")"
            ],
            "metadata": {
                "id": "download_json"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "\n",
                "## ðŸ“‹ Notes\n",
                "\n",
                "### Prosodic Markers Explained\n",
                "\n",
                "| Marker | Description | Duration |\n",
                "|--------|-------------|----------|\n",
                "| `[PAUSE-SHORT]` | Brief pause between phrases | 0.3s |\n",
                "| `[PAUSE-MEDIUM]` | Sentence pause for breathing | 0.6s |\n",
                "| `[PAUSE-LONG]` | Dramatic pause | 1.0s |\n",
                "| `[BREATH]` | Natural breath sound | - |\n",
                "| `[TONE: X]` | Emotional tone (calm, excited, etc.) | - |\n",
                "| `[EMPHASIS: word]` | Stress on specific word | - |\n",
                "| `[PACE: X]` | Reading speed (slow, normal, fast) | - |\n",
                "\n",
                "### Provider Comparison\n",
                "\n",
                "| Feature | HuggingFace | Ollama |\n",
                "|---------|-------------|--------|\n",
                "| **Setup** | Easy (recommended) | Requires server setup |\n",
                "| **Speed on Colab** | Fast with GPU | Moderate |\n",
                "| **Best Models** | gemma-2-2b-it, Airavata | gemma2:9b, aya:8b |\n",
                "| **Memory** | Uses GPU memory | Uses CPU/RAM |\n",
                "\n",
                "---\n",
                "\n",
                "âœ¨ **Tip**: Feed the generated transcription to your TTS model for natural, human-like audio!"
            ],
            "metadata": {
                "id": "notes"
            }
        }
    ]
}
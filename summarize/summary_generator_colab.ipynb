{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Context-Aware Book Summarization Tool\n",
    "\n",
    "**Enhanced version with length control and improved prompting**\n",
    "\n",
    "This notebook allows you to:\n",
    "1. üì§ Upload a text file to summarize\n",
    "2. ü§ñ Select AI provider (Ollama / HuggingFace) and model\n",
    "3. üìè Choose summary length (Short / Medium / Long)\n",
    "4. üìù Generate high-quality summaries\n",
    "5. üíæ Download the generated summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch accelerate colorama ollama\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Color support\n",
    "try:\n",
    "    from colorama import init, Fore, Style\n",
    "    init(autoreset=True)\n",
    "    C = True\n",
    "except ImportError:\n",
    "    C = False\n",
    "    class Fore: RED = GREEN = YELLOW = CYAN = MAGENTA = RESET = \"\"\n",
    "    class Style: BRIGHT = RESET_ALL = \"\"\n",
    "\n",
    "# HuggingFace support\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "    import torch\n",
    "    HF_AVAILABLE = True\n",
    "    print(f\"{Fore.GREEN}‚úÖ HuggingFace Transformers available{Style.RESET_ALL}\")\n",
    "except ImportError:\n",
    "    HF_AVAILABLE = False\n",
    "    print(f\"{Fore.YELLOW}‚ö†Ô∏è HuggingFace Transformers not available{Style.RESET_ALL}\")\n",
    "\n",
    "# Ollama support check\n",
    "try:\n",
    "    import ollama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "    print(f\"{Fore.GREEN}‚úÖ Ollama package available{Style.RESET_ALL}\")\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(f\"{Fore.YELLOW}‚ö†Ô∏è Ollama package not available{Style.RESET_ALL}\")\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"{Fore.GREEN}üöÄ GPU Available: {torch.cuda.get_device_name(0)}{Style.RESET_ALL}\")\n",
    "else:\n",
    "    print(f\"{Fore.YELLOW}üíª Running on CPU{Style.RESET_ALL}\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Summarization Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- LENGTH SPECIFICATIONS ----------------\n",
    "LENGTH_SPECS = {\n",
    "    \"SHORT\": {\n",
    "        \"chunk_target\": \"2-3 sentences\",\n",
    "        \"final_ratio\": \"5-8%\",\n",
    "        \"description\": \"Brief overview hitting only the most critical points\"\n",
    "    },\n",
    "    \"MEDIUM\": {\n",
    "        \"chunk_target\": \"4-6 sentences\", \n",
    "        \"final_ratio\": \"10-15%\",\n",
    "        \"description\": \"Balanced summary covering main ideas and key details\"\n",
    "    },\n",
    "    \"LONG\": {\n",
    "        \"chunk_target\": \"8-12 sentences\",\n",
    "        \"final_ratio\": \"20-25%\", \n",
    "        \"description\": \"Comprehensive summary preserving nuance and context\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---------------- SUMMARY PROMPTS ----------------\n",
    "SUMMARY_PROMPTS = {\n",
    "    \"BASIC\": {\n",
    "        \"system\": \"\"\"You are a professional book summarizer focused on accuracy and clarity.\n",
    "\n",
    "CORE PRINCIPLES:\n",
    "- Preserve factual accuracy above all else\n",
    "- Capture main ideas, events, and arguments in order\n",
    "- Use clear, direct language\n",
    "- NO interpretation, NO opinions, NO embellishment\n",
    "- Maintain the author's voice and perspective\"\"\",\n",
    "        \n",
    "        \"chunk_user\": \"\"\"TEXT TO SUMMARIZE:\n",
    "\\\"\\\"\\\"\n",
    "{chunk}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "CONTEXT FROM PREVIOUS SECTIONS:\n",
    "{context}\n",
    "\n",
    "TARGET LENGTH: {length_target}\n",
    "\n",
    "Create a factual summary that:\n",
    "1. Captures the key information in this section\n",
    "2. Connects naturally with what came before\n",
    "3. Preserves important names, events, and details\n",
    "4. Uses approximately {length_target}\n",
    "5. Maintains chronological order\n",
    "\n",
    "Summary:\"\"\",\n",
    "\n",
    "        \"final_user\": \"\"\"You are creating a final cohesive summary from chunk summaries.\n",
    "\n",
    "TARGET: {final_ratio} of original length\n",
    "APPROACH: Create ONE flowing narrative (not a list of sections)\n",
    "\n",
    "CHUNK SUMMARIES:\n",
    "{summaries}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Combine all information into one seamless narrative\n",
    "2. Remove ALL repetitions and redundancies\n",
    "3. Maintain chronological/logical flow throughout\n",
    "4. Preserve all key facts, names, events, and arguments\n",
    "5. Connect ideas smoothly with transitions\n",
    "6. Write as if summarizing the complete text directly\n",
    "7. Target length: {final_ratio} of the original\n",
    "\n",
    "Create the final summary:\"\"\"\n",
    "    },\n",
    "\n",
    "    \"INTERMEDIATE\": {\n",
    "        \"system\": \"\"\"You are an expert analytical summarizer who captures both content and context.\n",
    "\n",
    "GOALS:\n",
    "- Capture ideas, arguments, and their relationships\n",
    "- Preserve narrative/logical flow and transitions\n",
    "- Identify themes and patterns\n",
    "- Connect new content with previous context\n",
    "- Balance detail with conciseness\n",
    "- Maintain the author's argumentative structure\"\"\",\n",
    "        \n",
    "        \"chunk_user\": \"\"\"TEXT TO SUMMARIZE:\n",
    "\\\"\\\"\\\"\n",
    "{chunk}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "PREVIOUS CONTEXT:\n",
    "{context}\n",
    "\n",
    "TARGET LENGTH: {length_target}\n",
    "\n",
    "Create a thematic summary that:\n",
    "1. Identifies main ideas and how they connect\n",
    "2. Preserves the logical flow and argumentation\n",
    "3. Notes significant transitions or shifts\n",
    "4. Maintains connection with previous context\n",
    "5. Uses approximately {length_target}\n",
    "6. Captures both explicit and implicit themes\n",
    "\n",
    "Summary:\"\"\",\n",
    "\n",
    "        \"final_user\": \"\"\"Create a cohesive analytical summary from these chunk summaries.\n",
    "\n",
    "TARGET: {final_ratio} of original length\n",
    "FOCUS: Themes, arguments, and narrative flow\n",
    "\n",
    "CHUNK SUMMARIES:\n",
    "{summaries}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Synthesize into one flowing analytical narrative\n",
    "2. Highlight thematic connections and patterns\n",
    "3. Preserve the author's argumentative arc\n",
    "4. Remove redundancies while keeping nuance\n",
    "5. Show how ideas develop and connect\n",
    "6. Write as a unified, coherent analysis\n",
    "7. Target approximately {final_ratio} of original length\n",
    "\n",
    "Create the cohesive summary:\"\"\"\n",
    "    },\n",
    "\n",
    "    \"ADVANCED\": {\n",
    "        \"system\": \"\"\"You are a senior literary analyst creating publication-quality summaries.\n",
    "\n",
    "EXPERTISE:\n",
    "- Capture intent, subtext, and rhetorical structure\n",
    "- Explain WHY ideas matter, not just WHAT they are\n",
    "- Identify authorial choices and their effects\n",
    "- Preserve logical progression and development\n",
    "- Synthesize rather than merely condense\n",
    "- Think like a literary editor preparing reader guides\"\"\",\n",
    "        \n",
    "        \"chunk_user\": \"\"\"TEXT TO ANALYZE:\n",
    "\\\"\\\"\\\"\n",
    "{chunk}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "NARRATIVE CONTEXT:\n",
    "{context}\n",
    "\n",
    "TARGET LENGTH: {length_target}\n",
    "\n",
    "Create a sophisticated summary that:\n",
    "1. Captures both surface content and deeper significance\n",
    "2. Explains the function of this section in the larger work\n",
    "3. Notes rhetorical choices and structural elements\n",
    "4. Preserves the author's logic and progression\n",
    "5. Identifies subtext and implications\n",
    "6. Uses approximately {length_target}\n",
    "7. Connects meaningfully with prior context\n",
    "\n",
    "Analysis:\"\"\",\n",
    "\n",
    "        \"final_user\": \"\"\"Synthesize a sophisticated, publication-quality summary from these analytical summaries.\n",
    "\n",
    "TARGET: {final_ratio} of original length\n",
    "STANDARD: Literary analysis quality\n",
    "\n",
    "CHUNK SUMMARIES:\n",
    "{summaries}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Create ONE seamless narrative synthesis\n",
    "2. Preserve the work's intellectual architecture\n",
    "3. Show how ideas develop and interconnect\n",
    "4. Capture both explicit content and deeper significance\n",
    "5. Eliminate redundancy while preserving nuance\n",
    "6. Write with the polish of a published analysis\n",
    "7. Target approximately {final_ratio} of original length\n",
    "8. Think like you're writing for a literary journal\n",
    "\n",
    "Create the final synthesis:\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---------------- UTILITY FUNCTIONS ----------------\n",
    "def detect_device():\n",
    "    \"\"\"Auto-detect available device (CUDA, ROCm, or CPU).\"\"\"\n",
    "    try:\n",
    "        if HF_AVAILABLE and torch.cuda.is_available():\n",
    "            if hasattr(torch.version, 'hip') and torch.version.hip:\n",
    "                print(\"üîç ROCm (AMD GPU) detected\")\n",
    "                return \"cuda\"\n",
    "            else:\n",
    "                print(\"üîç CUDA (NVIDIA GPU) detected\")\n",
    "                return \"cuda\"\n",
    "    except:\n",
    "        pass\n",
    "    print(\"üîç No GPU detected, using CPU\")\n",
    "    return \"cpu\"\n",
    "\n",
    "def chunk_text(text, chunk_words=400, overlap=80):\n",
    "    \"\"\"Chunk text with overlap for context preservation\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_words\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "def clean_output(text):\n",
    "    \"\"\"Remove thinking tags and code blocks from output\"\"\"\n",
    "    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'^(Summary:|Analysis:|Here is.*?:|Here\\'s.*?:)\\s*', '', text, flags=re.IGNORECASE)\n",
    "    return text.strip()\n",
    "\n",
    "def estimate_word_count(text):\n",
    "    \"\"\"Estimate word count from text\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "print(\"‚úÖ Summarization engine defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Model Provider Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Provider:\n",
    "    \"\"\"AI Model Provider for Ollama and HuggingFace\"\"\"\n",
    "    \n",
    "    def __init__(self, provider, model, device=\"auto\"):\n",
    "        self.provider = provider\n",
    "        self.model = model\n",
    "        self.device = detect_device() if device == \"auto\" else device\n",
    "        self.pipeline = None\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load the specified model\"\"\"\n",
    "        if self.provider == \"ollama\":\n",
    "            if not OLLAMA_AVAILABLE:\n",
    "                raise RuntimeError(\"Ollama package not installed. Please install with: pip install ollama\")\n",
    "            try:\n",
    "                ollama.show(self.model)\n",
    "                print(f\"{Fore.GREEN}‚úÖ Ollama model '{self.model}' ready{Style.RESET_ALL}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"{Fore.YELLOW}‚ö†Ô∏è Model not found locally. Pulling {self.model}...{Style.RESET_ALL}\")\n",
    "                os.system(f\"ollama pull {self.model}\")\n",
    "                return True\n",
    "\n",
    "        # HuggingFace provider\n",
    "        if not HF_AVAILABLE:\n",
    "            raise RuntimeError(\"HuggingFace Transformers not installed\")\n",
    "\n",
    "        print(f\"{Fore.CYAN}üîÑ Loading HuggingFace model: {self.model}...{Style.RESET_ALL}\")\n",
    "        print(\"(This may take a few minutes for large models)\")\n",
    "        \n",
    "        # Use appropriate dtype based on device\n",
    "        if self.device == \"cuda\":\n",
    "            torch_dtype = torch.float16\n",
    "            device_map = \"auto\"\n",
    "        else:\n",
    "            torch_dtype = torch.float32\n",
    "            device_map = None\n",
    "        \n",
    "        tok = AutoTokenizer.from_pretrained(self.model)\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=torch_dtype\n",
    "        )\n",
    "        self.pipeline = pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n",
    "        \n",
    "        device_type = \"GPU\" if self.device == \"cuda\" else \"CPU\"\n",
    "        print(f\"{Fore.GREEN}‚úÖ Model loaded on {device_type}{Style.RESET_ALL}\")\n",
    "        return True\n",
    "\n",
    "    def generate(self, system, user, max_tokens=1500):\n",
    "        \"\"\"Generate text with the loaded model\"\"\"\n",
    "        if self.provider == \"ollama\":\n",
    "            out = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": user}\n",
    "                ],\n",
    "                options={\n",
    "                    \"temperature\": 0.2,\n",
    "                    \"num_ctx\": 8192,\n",
    "                    \"num_predict\": max_tokens\n",
    "                }\n",
    "            )\n",
    "            return out[\"message\"][\"content\"]\n",
    "\n",
    "        # HuggingFace generation\n",
    "        prompt = f\"System:\\n{system}\\n\\nUser:\\n{user}\\n\\nAssistant:\"\n",
    "        r = self.pipeline(prompt, max_new_tokens=max_tokens, temperature=0.2)\n",
    "        return r[0][\"generated_text\"].split(\"Assistant:\")[-1]\n",
    "\n",
    "print(\"‚úÖ Provider class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Upload Your Text File üì§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Please upload your text file to summarize:\")\n",
    "print(\"(Supported formats: .txt)\")\n",
    "print()\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded file\n",
    "if uploaded:\n",
    "    uploaded_filename = list(uploaded.keys())[0]\n",
    "    input_text = uploaded[uploaded_filename].decode('utf-8')\n",
    "    word_count = estimate_word_count(input_text)\n",
    "    \n",
    "    print()\n",
    "    print(f\"{Fore.GREEN}‚úÖ File uploaded successfully!{Style.RESET_ALL}\")\n",
    "    print(f\"üìÑ Filename: {uploaded_filename}\")\n",
    "    print(f\"üìä Word count: {word_count:,} words\")\n",
    "    print(f\"üìù Preview (first 500 chars):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(input_text[:500] + \"...\" if len(input_text) > 500 else input_text)\n",
    "else:\n",
    "    print(f\"{Fore.RED}‚ùå No file uploaded. Please run this cell again.{Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Summarization Options üéõÔ∏è\n",
    "\n",
    "Run the cell below and use the interactive widgets to select your preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Create configuration widgets\n",
    "provider_dropdown = widgets.Dropdown(\n",
    "    options=['huggingface', 'ollama'],\n",
    "    value='huggingface',\n",
    "    description='AI Provider:',\n",
    "    style={'description_width': '120px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "model_text = widgets.Text(\n",
    "    value='Qwen/Qwen2.5-1.5B-Instruct',\n",
    "    placeholder='Enter model name',\n",
    "    description='Model:',\n",
    "    style={'description_width': '120px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "length_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('üîπ SHORT - Brief overview (5-8% of original)', 'SHORT'),\n",
    "        ('üî∏ MEDIUM - Balanced summary (10-15% of original)', 'MEDIUM'),\n",
    "        ('üî∂ LONG - Comprehensive (20-25% of original)', 'LONG')\n",
    "    ],\n",
    "    value='MEDIUM',\n",
    "    description='Length:',\n",
    "    style={'description_width': '120px'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "tier_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('üìò BASIC - Factual, straightforward', 'BASIC'),\n",
    "        ('üìó INTERMEDIATE - Thematic, analytical', 'INTERMEDIATE'),\n",
    "        ('üìï ADVANCED - Publication-quality', 'ADVANCED')\n",
    "    ],\n",
    "    value='INTERMEDIATE',\n",
    "    description='Quality Tier:',\n",
    "    style={'description_width': '120px'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "chunk_words_slider = widgets.IntSlider(\n",
    "    value=500,\n",
    "    min=200,\n",
    "    max=1000,\n",
    "    step=50,\n",
    "    description='Chunk Size:',\n",
    "    style={'description_width': '120px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "overlap_slider = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=20,\n",
    "    max=200,\n",
    "    step=10,\n",
    "    description='Overlap:',\n",
    "    style={'description_width': '120px'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Model suggestions based on provider\n",
    "model_suggestions = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <div style='background: #f0f7ff; padding: 10px; border-radius: 8px; margin-top: 10px;'>\n",
    "    <b>üìã Suggested HuggingFace Models:</b><br>\n",
    "    ‚Ä¢ <code>Qwen/Qwen2.5-1.5B-Instruct</code> (Fast, good quality)<br>\n",
    "    ‚Ä¢ <code>microsoft/Phi-3-mini-4k-instruct</code> (Balanced)<br>\n",
    "    ‚Ä¢ <code>meta-llama/Llama-3.2-1B-Instruct</code> (Efficient)<br>\n",
    "    ‚Ä¢ <code>google/gemma-2-2b-it</code> (High quality)<br>\n",
    "    <br>\n",
    "    <b>üìã Suggested Ollama Models:</b><br>\n",
    "    ‚Ä¢ <code>llama3.1</code> (Recommended)<br>\n",
    "    ‚Ä¢ <code>qwen2.5:7b</code> (Fast, accurate)<br>\n",
    "    ‚Ä¢ <code>mistral</code> (Good balance)<br>\n",
    "    ‚Ä¢ <code>phi3:mini</code> (Lightweight)\n",
    "    </div>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Display configuration\n",
    "print(\"üéõÔ∏è Configure Your Summarization Settings:\")\n",
    "print(\"=\" * 50)\n",
    "display(provider_dropdown)\n",
    "display(model_text)\n",
    "display(length_dropdown)\n",
    "display(tier_dropdown)\n",
    "display(chunk_words_slider)\n",
    "display(overlap_slider)\n",
    "display(model_suggestions)\n",
    "\n",
    "print(\"\\n‚úÖ Configure your settings above, then run the next cell to generate the summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Summary üöÄ\n",
    "\n",
    "This cell will process your text and generate the summary based on your configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configuration from widgets\n",
    "selected_provider = provider_dropdown.value\n",
    "selected_model = model_text.value\n",
    "selected_length = length_dropdown.value\n",
    "selected_tier = tier_dropdown.value\n",
    "chunk_words = chunk_words_slider.value\n",
    "overlap = overlap_slider.value\n",
    "\n",
    "# Validate input\n",
    "if 'input_text' not in dir() or not input_text:\n",
    "    print(f\"{Fore.RED}‚ùå No text file uploaded! Please run Step 5 first.{Style.RESET_ALL}\")\n",
    "else:\n",
    "    print(f\"{Fore.CYAN}{Style.BRIGHT}=== Book Summarization Tool ==={Style.RESET_ALL}\")\n",
    "    print(f\"Provider: {selected_provider}\")\n",
    "    print(f\"Model: {selected_model}\")\n",
    "    print(f\"Tier: {selected_tier}\")\n",
    "    print(f\"Length: {selected_length} ({LENGTH_SPECS[selected_length]['description']})\")\n",
    "    print(f\"Chunk size: {chunk_words} words, Overlap: {overlap} words\")\n",
    "    print()\n",
    "\n",
    "    # Initialize provider\n",
    "    print(f\"{Fore.YELLOW}üîÑ Initializing AI provider...{Style.RESET_ALL}\")\n",
    "    prov = Provider(selected_provider, selected_model)\n",
    "    prov.load()\n",
    "    print()\n",
    "\n",
    "    # Chunk text\n",
    "    original_words = estimate_word_count(input_text)\n",
    "    chunks = chunk_text(input_text, chunk_words, overlap)\n",
    "    print(f\"{Fore.GREEN}üìä Text loaded: {original_words:,} words ‚Üí {len(chunks)} chunks{Style.RESET_ALL}\")\n",
    "    print()\n",
    "\n",
    "    # Get prompts and length specs\n",
    "    prompts = SUMMARY_PROMPTS[selected_tier]\n",
    "    length_spec = LENGTH_SPECS[selected_length]\n",
    "\n",
    "    # Process chunks\n",
    "    context = \"\"\n",
    "    chunk_summaries = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"{Fore.YELLOW}{Style.BRIGHT}‚ñ∫ Processing chunk {i}/{len(chunks)}...{Style.RESET_ALL}\", end=\" \")\n",
    "        \n",
    "        chunk_prompt = prompts[\"chunk_user\"].format(\n",
    "            chunk=chunk,\n",
    "            context=context[-2000:],\n",
    "            length_target=length_spec[\"chunk_target\"]\n",
    "        )\n",
    "\n",
    "        result = prov.generate(prompts[\"system\"], chunk_prompt, max_tokens=800)\n",
    "        result = clean_output(result)\n",
    "        \n",
    "        chunk_summaries.append(result)\n",
    "        context += \"\\n\\n\" + result\n",
    "        \n",
    "        print(f\"{Fore.GREEN}‚úì ({estimate_word_count(result)} words){Style.RESET_ALL}\")\n",
    "\n",
    "    # Generate final cohesive summary\n",
    "    print()\n",
    "    print(f\"{Fore.CYAN}{Style.BRIGHT}‚ñ∫ Generating final cohesive summary...{Style.RESET_ALL}\")\n",
    "    \n",
    "    final_prompt = prompts[\"final_user\"].format(\n",
    "        summaries=\"\\n\\n---\\n\\n\".join(chunk_summaries),\n",
    "        final_ratio=length_spec[\"final_ratio\"]\n",
    "    )\n",
    "    \n",
    "    final_summary = prov.generate(\n",
    "        prompts[\"system\"],\n",
    "        final_prompt,\n",
    "        max_tokens=2500\n",
    "    )\n",
    "    final_summary = clean_output(final_summary)\n",
    "    \n",
    "    # Calculate stats\n",
    "    elapsed_time = time.time() - start_time\n",
    "    final_words = estimate_word_count(final_summary)\n",
    "    compression = (final_words / original_words) * 100\n",
    "    \n",
    "    print()\n",
    "    print(f\"{Fore.GREEN}{Style.BRIGHT}\" + \"=\" * 50 + f\"{Style.RESET_ALL}\")\n",
    "    print(f\"{Fore.GREEN}{Style.BRIGHT}‚úÖ SUMMARY COMPLETE!{Style.RESET_ALL}\")\n",
    "    print(f\"{Fore.GREEN}{Style.BRIGHT}\" + \"=\" * 50 + f\"{Style.RESET_ALL}\")\n",
    "    print(f\"üìä Original: {original_words:,} words\")\n",
    "    print(f\"üìù Summary: {final_words:,} words ({compression:.1f}% of original)\")\n",
    "    print(f\"‚è±Ô∏è Time: {elapsed_time:.1f} seconds\")\n",
    "    print()\n",
    "    print(f\"{Fore.CYAN}=== GENERATED SUMMARY ==={Style.RESET_ALL}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(final_summary)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download Summary üíæ\n",
    "\n",
    "Run this cell to save and download your generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from datetime import datetime\n",
    "\n",
    "if 'final_summary' not in dir() or not final_summary:\n",
    "    print(f\"{Fore.RED}‚ùå No summary generated yet! Please run Step 7 first.{Style.RESET_ALL}\")\n",
    "else:\n",
    "    # Generate output filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_name = uploaded_filename.rsplit('.', 1)[0] if 'uploaded_filename' in dir() else 'document'\n",
    "    output_filename = f\"{base_name}_summary_{selected_length.lower()}_{timestamp}.txt\"\n",
    "    \n",
    "    # Create summary content with metadata\n",
    "    summary_content = f\"\"\"================================================================================\n",
    "SUMMARY GENERATED BY CONTEXT-AWARE BOOK SUMMARIZATION TOOL\n",
    "================================================================================\n",
    "\n",
    "Source File: {uploaded_filename if 'uploaded_filename' in dir() else 'Unknown'}\n",
    "Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "Provider: {selected_provider}\n",
    "Model: {selected_model}\n",
    "Quality Tier: {selected_tier}\n",
    "Length Setting: {selected_length} ({LENGTH_SPECS[selected_length]['description']})\n",
    "\n",
    "Original Word Count: {original_words:,}\n",
    "Summary Word Count: {final_words:,}\n",
    "Compression Ratio: {compression:.1f}%\n",
    "\n",
    "================================================================================\n",
    "SUMMARY\n",
    "================================================================================\n",
    "\n",
    "{final_summary}\n",
    "\n",
    "================================================================================\n",
    "END OF SUMMARY\n",
    "================================================================================\n",
    "\"\"\"\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_content)\n",
    "    \n",
    "    print(f\"{Fore.GREEN}‚úÖ Summary saved!{Style.RESET_ALL}\")\n",
    "    print(f\"üìÑ Filename: {output_filename}\")\n",
    "    print()\n",
    "    print(\"üì• Starting download...\")\n",
    "    \n",
    "    # Download the file\n",
    "    files.download(output_filename)\n",
    "    \n",
    "    print(f\"\\n{Fore.GREEN}‚úÖ Download initiated! Check your browser's downloads.{Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Generate Another Summary (Optional)\n",
    "\n",
    "Want to try different settings? Run the cell below to reset and start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset for new summarization\n",
    "reset_confirm = input(\"‚ö†Ô∏è Are you sure you want to reset? This will clear the current summary. (yes/no): \")\n",
    "\n",
    "if reset_confirm.lower() in ['yes', 'y']:\n",
    "    # Clear variables\n",
    "    if 'final_summary' in dir():\n",
    "        del final_summary\n",
    "    if 'chunk_summaries' in dir():\n",
    "        del chunk_summaries\n",
    "    if 'input_text' in dir():\n",
    "        del input_text\n",
    "    \n",
    "    print(f\"{Fore.GREEN}‚úÖ Reset complete! Go back to Step 5 to upload a new file.{Style.RESET_ALL}\")\n",
    "else:\n",
    "    print(\"Reset cancelled. Your current summary is preserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Quick Reference\n",
    "\n",
    "### Length Options:\n",
    "| Setting | Chunk Target | Final Ratio | Best For |\n",
    "|---------|-------------|-------------|----------|\n",
    "| **SHORT** | 2-3 sentences | 5-8% | Quick overviews, abstracts |\n",
    "| **MEDIUM** | 4-6 sentences | 10-15% | Balanced summaries |\n",
    "| **LONG** | 8-12 sentences | 20-25% | Detailed comprehension |\n",
    "\n",
    "### Quality Tiers:\n",
    "| Tier | Style | Best For |\n",
    "|------|-------|----------|\n",
    "| **BASIC** | Factual, straightforward | News, reports, documentation |\n",
    "| **INTERMEDIATE** | Thematic, analytical | Books, essays, articles |\n",
    "| **ADVANCED** | Publication-quality | Literary works, academic texts |\n",
    "\n",
    "### Tips:\n",
    "- üîπ For faster processing, use smaller HuggingFace models or Ollama\n",
    "- üîπ Increase chunk overlap for better context preservation\n",
    "- üîπ Use ADVANCED tier for complex literary texts\n",
    "- üîπ GPU acceleration significantly speeds up HuggingFace models"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udf10 Hindi Literary Translation Generator for Google Colab\n",
        "\n",
        "**Enhanced Multilingual Translation with Real-time Streaming**\n",
        "\n",
        "This notebook allows you to:\n",
        "1. Upload your text file to translate\n",
        "2. Select AI provider (HuggingFace or Ollama) and model\n",
        "3. Choose target language and translation quality tier\n",
        "4. Generate translation and download the result\n",
        "\n",
        "**Supported Providers:**\n",
        "- \ud83e\udd17 HuggingFace - Various translation models (faster on GPU)\n",
        "- \ud83e\udd99 Ollama - Run LLMs locally in Colab (supports many models)\n",
        "\n",
        "**Recommended Models for Hindi:**\n",
        "- HuggingFace: `facebook/nllb-200-distilled-600M` - Fast, multilingual\n",
        "- HuggingFace: `ai4bharat/indictrans2-en-indic-1B` - Best for English\u2192Hindi\n",
        "- Ollama: `qwen2.5:3b` - Fast, good quality translations\n",
        "- Ollama: `qwen2.5:7b` - Better quality, balanced speed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udce6 Step 1: Install Dependencies\n",
        "Run this cell to install all required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch transformers accelerate sentencepiece\n",
        "!pip install -q colorama huggingface-hub\n",
        "\n",
        "# Install Ollama Python client\n",
        "!pip install -q ollama\n",
        "\n",
        "# Setup HuggingFace login for gated models\n",
        "print(\"\\n\ud83d\udd10 HuggingFace Login (for gated models like TranslateGemma):\")\n",
        "print(\"   If you need access to gated models, run the next cell to login.\")\n",
        "print(\"   Otherwise, skip to Step 2.\\n\")\n",
        "print(\"\u2705 All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd10 (Optional) HuggingFace Login\n",
        "Run this cell to login to HuggingFace for accessing gated models like `google/translategemma-27b-it`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace Login for Gated Models\n",
        "from huggingface_hub import login, HfFolder\n",
        "import os\n",
        "\n",
        "print(\"\ud83d\udd10 HuggingFace Login Options:\")\n",
        "print(\"   1. Interactive Login - Opens a browser/token prompt\")\n",
        "print(\"   2. Token Login - Paste your HF token directly\\n\")\n",
        "\n",
        "# Check if already logged in\n",
        "existing_token = HfFolder.get_token()\n",
        "if existing_token:\n",
        "    print(f\"\u2705 Already logged in to HuggingFace!\")\n",
        "    print(f\"   Token: {existing_token[:10]}...{existing_token[-5:]}\")\n",
        "else:\n",
        "    print(\"\ud83d\udcdd Not logged in. Choose a login method:\")\n",
        "    print(\"\\n\ud83d\udd39 Option 1: Interactive Login (recommended)\")\n",
        "    print(\"   Uncomment the line below and run this cell:\")\n",
        "    print(\"   # login()\\n\")\n",
        "    print(\"\ud83d\udd39 Option 2: Token Login\")\n",
        "    print(\"   Get your token from: https://huggingface.co/settings/tokens\")\n",
        "    print(\"   Then paste it in the HF Token field in Step 3\\n\")\n",
        "\n",
        "# Uncomment ONE of the following lines to login:\n",
        "\n",
        "# Option 1: Interactive login (will prompt for token)\n",
        "# login()\n",
        "\n",
        "# Option 2: Direct token login (paste your token)\n",
        "# login(token=\"hf_YOUR_TOKEN_HERE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83e\udd99 (Optional) Ollama Setup\n",
        "Run these cells if you want to use Ollama models. Skip if using HuggingFace only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and start Ollama server (required for Ollama models)\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"\ud83e\udd99 Installing Ollama...\")\n",
        "\n",
        "# Install zstd first (required for Ollama extraction)\n",
        "!apt-get update -qq && apt-get install -y -qq zstd > /dev/null 2>&1\n",
        "\n",
        "# Download and install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"\\n\ud83d\ude80 Starting Ollama server in background...\")\n",
        "\n",
        "# Start Ollama server in background\n",
        "os.environ['OLLAMA_HOST'] = '127.0.0.1:11434'\n",
        "subprocess.Popen(['/usr/local/bin/ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Verify server is running\n",
        "try:\n",
        "    import ollama\n",
        "    ollama.list()\n",
        "    print(\"\u2705 Ollama server is running!\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f Ollama server may not be ready yet. Error: {e}\")\n",
        "    print(\"   Please wait a few seconds and try running the next cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull Ollama model (run this cell to download your chosen model)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"\ud83e\udd99 Ollama Model Download\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Model selection for pulling\n",
        "OLLAMA_MODELS_TO_PULL = {\n",
        "    \"qwen2.5:3b (Fast, ~2GB)\": \"qwen2.5:3b\",\n",
        "    \"qwen2.5:7b (Balanced, ~4GB)\": \"qwen2.5:7b\",\n",
        "    \"deepseek-r1:7b (Reasoning, ~4GB)\": \"deepseek-r1:7b\",\n",
        "    \"llama3.2:3b (Fast, ~2GB)\": \"llama3.2:3b\",\n",
        "    \"mistral:7b (Quality, ~4GB)\": \"mistral:7b\",\n",
        "    \"gemma2:2b (Compact, ~1.5GB)\": \"gemma2:2b\",\n",
        "    \"phi3:mini (Compact, ~2GB)\": \"phi3:mini\"\n",
        "}\n",
        "\n",
        "model_pull_dropdown = widgets.Dropdown(\n",
        "    options=list(OLLAMA_MODELS_TO_PULL.keys()),\n",
        "    value=\"qwen2.5:3b (Fast, ~2GB)\",\n",
        "    description='Model to Pull:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "custom_ollama_pull = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Or enter custom model name (e.g., llama3:8b)',\n",
        "    description='Custom Model:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "display(model_pull_dropdown)\n",
        "display(custom_ollama_pull)\n",
        "print(\"\\n\ud83d\udca1 Select a model and run the next cell to download it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actually pull the selected model\n",
        "import ollama\n",
        "\n",
        "# Get model to pull\n",
        "if custom_ollama_pull.value.strip():\n",
        "    model_to_pull = custom_ollama_pull.value.strip()\n",
        "else:\n",
        "    model_to_pull = OLLAMA_MODELS_TO_PULL[model_pull_dropdown.value]\n",
        "\n",
        "print(f\"\ud83d\udce5 Pulling model: {model_to_pull}\")\n",
        "print(\"   This may take several minutes depending on model size...\\n\")\n",
        "\n",
        "try:\n",
        "    # Pull with progress\n",
        "    current_digest = ''\n",
        "    for progress in ollama.pull(model_to_pull, stream=True):\n",
        "        digest = progress.get('digest', '')\n",
        "        if digest != current_digest and current_digest:\n",
        "            print()  # Newline between layers\n",
        "        current_digest = digest\n",
        "        \n",
        "        status = progress.get('status', '')\n",
        "        if 'completed' in progress and 'total' in progress:\n",
        "            completed = progress['completed']\n",
        "            total = progress['total']\n",
        "            pct = (completed / total * 100) if total > 0 else 0\n",
        "            print(f\"\\r   {status}: {pct:.1f}% ({completed}/{total})\", end='', flush=True)\n",
        "        else:\n",
        "            print(f\"\\r   {status}\", end='', flush=True)\n",
        "    \n",
        "    print(f\"\\n\\n\u2705 Model '{model_to_pull}' pulled successfully!\")\n",
        "    \n",
        "    # List available models\n",
        "    print(\"\\n\ud83d\udccb Available Ollama models:\")\n",
        "    models = ollama.list()\n",
        "    for model in models.get('models', []):\n",
        "        name = model.get('name', 'unknown')\n",
        "        size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
        "        print(f\"   \u2022 {name} ({size:.2f} GB)\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Error pulling model: {e}\")\n",
        "    print(\"   Make sure Ollama server is running (run the previous cell first).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udce4 Step 2: Upload Your Text File\n",
        "Upload the text file you want to translate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"\ud83d\udce4 Please upload your text file to translate:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name\n",
        "UPLOADED_FILE = list(uploaded.keys())[0]\n",
        "print(f\"\\n\u2705 Uploaded: {UPLOADED_FILE}\")\n",
        "print(f\"\ud83d\udcc4 File size: {len(uploaded[UPLOADED_FILE])} bytes\")\n",
        "\n",
        "# Display preview\n",
        "with open(UPLOADED_FILE, 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "    word_count = len(content.split())\n",
        "    char_count = len(content)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Content stats:\")\n",
        "print(f\"   Words: {word_count:,}\")\n",
        "print(f\"   Characters: {char_count:,}\")\n",
        "print(f\"\\n\ud83d\udcdd Preview (first 500 chars):\\n{content[:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2699\ufe0f Step 3: Select AI Provider, Model & Language\n",
        "Choose your preferred translation model and settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Provider options\n",
        "PROVIDER_OPTIONS = {\n",
        "    \"HuggingFace (Recommended for Colab)\": \"huggingface\",\n",
        "    \"Ollama (Local only)\": \"ollama\"\n",
        "}\n",
        "\n",
        "# Model options by provider\n",
        "HF_MODEL_OPTIONS = {\n",
        "    \"facebook/nllb-200-distilled-600M (Fast, Multilingual)\": \"facebook/nllb-200-distilled-600M\",\n",
        "    \"facebook/nllb-200-1.3B (Better Quality)\": \"facebook/nllb-200-1.3B\",\n",
        "    \"ai4bharat/indictrans2-en-indic-1B (Best English\u2192Hindi)\": \"ai4bharat/indictrans2-en-indic-1B\",\n",
        "    \"google/madlad400-3b-mt (High Quality, Slow)\": \"google/madlad400-3b-mt\",\n",
        "    \"Helsinki-NLP/opus-mt-en-hi (Simple EN\u2192HI)\": \"Helsinki-NLP/opus-mt-en-hi\",\n",
        "    \"tencent/HY-MT1.5-7B (Hunyuan MT - Best Quality)\": \"tencent/HY-MT1.5-7B\",\n",
        "    \"tencent/HY-MT1.5-1.8B (Hunyuan MT - Fast)\": \"tencent/HY-MT1.5-1.8B\",\n",
        "    \"google/translategemma-27b-it (\ud83d\udd10 Gated - Requires Login)\": \"google/translategemma-27b-it\",\n",
        "    \"Custom Model (enter below)\": \"custom\"\n",
        "}\n",
        "\n",
        "OLLAMA_MODEL_OPTIONS = {\n",
        "    \"qwen2.5:3b (Fast)\": \"qwen2.5:3b\",\n",
        "    \"qwen2.5:7b (Balanced)\": \"qwen2.5:7b\",\n",
        "    \"deepseek-r1:7b (Reasoning)\": \"deepseek-r1:7b\",\n",
        "    \"llama3.2:3b (Fast)\": \"llama3.2:3b\",\n",
        "    \"Custom Model (enter below)\": \"custom\"\n",
        "}\n",
        "\n",
        "# Language options (NLLB language codes)\n",
        "LANGUAGE_OPTIONS = {\n",
        "    \"Hindi (\u0939\u093f\u0928\u094d\u0926\u0940)\": \"hin_Deva\",\n",
        "    \"Bengali (\u09ac\u09be\u0982\u09b2\u09be)\": \"ben_Beng\",\n",
        "    \"Tamil (\u0ba4\u0bae\u0bbf\u0bb4\u0bcd)\": \"tam_Taml\",\n",
        "    \"Telugu (\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41)\": \"tel_Telu\",\n",
        "    \"Marathi (\u092e\u0930\u093e\u0920\u0940)\": \"mar_Deva\",\n",
        "    \"Gujarati (\u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0)\": \"guj_Gujr\",\n",
        "    \"Kannada (\u0c95\u0ca8\u0ccd\u0ca8\u0ca1)\": \"kan_Knda\",\n",
        "    \"Malayalam (\u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02)\": \"mal_Mlym\",\n",
        "    \"Punjabi (\u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40)\": \"pan_Guru\",\n",
        "    \"Odia (\u0b13\u0b21\u0b3c\u0b3f\u0b06)\": \"ory_Orya\",\n",
        "    \"Urdu (\u0627\u0631\u062f\u0648)\": \"urd_Arab\",\n",
        "    \"Spanish (Espa\u00f1ol)\": \"spa_Latn\",\n",
        "    \"French (Fran\u00e7ais)\": \"fra_Latn\",\n",
        "    \"German (Deutsch)\": \"deu_Latn\",\n",
        "    \"Chinese (\u4e2d\u6587)\": \"zho_Hans\",\n",
        "    \"Japanese (\u65e5\u672c\u8a9e)\": \"jpn_Jpan\",\n",
        "    \"Custom (enter code below)\": \"custom\"\n",
        "}\n",
        "\n",
        "# Translation quality tiers\n",
        "TIER_OPTIONS = {\n",
        "    \"BASIC - Fast, good quality\": \"BASIC\",\n",
        "    \"INTERMEDIATE - Balanced (recommended)\": \"INTERMEDIATE\",\n",
        "    \"ADVANCED - Best quality, slower\": \"ADVANCED\"\n",
        "}\n",
        "\n",
        "# Provider dropdown\n",
        "provider_dropdown = widgets.Dropdown(\n",
        "    options=list(PROVIDER_OPTIONS.keys()),\n",
        "    value=\"HuggingFace (Recommended for Colab)\",\n",
        "    description='Provider:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "# Model dropdown (HuggingFace by default)\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=list(HF_MODEL_OPTIONS.keys()),\n",
        "    value=\"facebook/nllb-200-distilled-600M (Fast, Multilingual)\",\n",
        "    description='Model:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='500px')\n",
        ")\n",
        "\n",
        "# Custom model input\n",
        "custom_model_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter HuggingFace model name',\n",
        "    description='Custom Model:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='500px')\n",
        ")\n",
        "\n",
        "# Target language dropdown\n",
        "language_dropdown = widgets.Dropdown(\n",
        "    options=list(LANGUAGE_OPTIONS.keys()),\n",
        "    value=\"Hindi (\u0939\u093f\u0928\u094d\u0926\u0940)\",\n",
        "    description='Target Language:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "# Custom language code input\n",
        "custom_language_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter NLLB language code (e.g., hin_Deva)',\n",
        "    description='Custom Lang:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "# Translation tier dropdown\n",
        "tier_dropdown = widgets.Dropdown(\n",
        "    options=list(TIER_OPTIONS.keys()),\n",
        "    value=\"INTERMEDIATE - Balanced (recommended)\",\n",
        "    description='Quality:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "# HuggingFace token (optional)\n",
        "hf_token_input = widgets.Password(\n",
        "    value='',\n",
        "    placeholder='Optional: HF token for gated models',\n",
        "    description='HF Token:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "# Chunk size slider\n",
        "chunk_size_slider = widgets.IntSlider(\n",
        "    value=350,\n",
        "    min=100,\n",
        "    max=1000,\n",
        "    step=50,\n",
        "    description='Chunk Size:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "display(HTML(\"<h3>\ud83c\udf9b\ufe0f Configure Translation Settings</h3>\"))\n",
        "display(provider_dropdown)\n",
        "display(model_dropdown)\n",
        "display(custom_model_input)\n",
        "display(HTML(\"<br>\"))\n",
        "display(language_dropdown)\n",
        "display(custom_language_input)\n",
        "display(HTML(\"<br>\"))\n",
        "display(tier_dropdown)\n",
        "display(chunk_size_slider)\n",
        "display(hf_token_input)\n",
        "\n",
        "\n",
        "# Handle provider change to update model dropdown\n",
        "def on_provider_change(change):\n",
        "    if change['new'] == \"HuggingFace (Recommended for Colab)\":\n",
        "        model_dropdown.options = list(HF_MODEL_OPTIONS.keys())\n",
        "        model_dropdown.value = \"facebook/nllb-200-distilled-600M (Fast, Multilingual)\"\n",
        "        custom_model_input.placeholder = 'Enter HuggingFace model name'\n",
        "    else:\n",
        "        model_dropdown.options = list(OLLAMA_MODEL_OPTIONS.keys())\n",
        "        model_dropdown.value = \"qwen2.5:3b (Fast)\"\n",
        "        custom_model_input.placeholder = 'Enter Ollama model name'\n",
        "\n",
        "provider_dropdown.observe(on_provider_change, names='value')\n",
        "print(\"\\n\ud83d\udca1 Tip: facebook/nllb-200-distilled-600M is recommended for fast Hindi translation!\")\n",
        "print(\"\ud83d\udd10 Note: Models marked with \ud83d\udd10 require HuggingFace login. Run the login cell above first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store the selected configuration\n",
        "SELECTED_PROVIDER = PROVIDER_OPTIONS[provider_dropdown.value]\n",
        "\n",
        "# Get model based on provider\n",
        "if SELECTED_PROVIDER == \"huggingface\":\n",
        "    selected_model_key = model_dropdown.value\n",
        "    SELECTED_MODEL = HF_MODEL_OPTIONS.get(selected_model_key, \"custom\")\n",
        "else:\n",
        "    SELECTED_MODEL = OLLAMA_MODEL_OPTIONS.get(model_dropdown.value, \"custom\")\n",
        "\n",
        "if SELECTED_MODEL == \"custom\":\n",
        "    SELECTED_MODEL = custom_model_input.value\n",
        "    if not SELECTED_MODEL:\n",
        "        raise ValueError(\"Please enter a custom model name!\")\n",
        "\n",
        "# Get target language\n",
        "TARGET_LANGUAGE = LANGUAGE_OPTIONS[language_dropdown.value]\n",
        "if TARGET_LANGUAGE == \"custom\":\n",
        "    TARGET_LANGUAGE = custom_language_input.value\n",
        "    if not TARGET_LANGUAGE:\n",
        "        raise ValueError(\"Please enter a custom language code!\")\n",
        "\n",
        "TRANSLATION_TIER = TIER_OPTIONS[tier_dropdown.value]\n",
        "CHUNK_SIZE = chunk_size_slider.value\n",
        "HF_TOKEN = hf_token_input.value if hf_token_input.value else None\n",
        "\n",
        "print(f\"\\n\u2705 Configuration saved:\")\n",
        "print(f\"   \ud83e\udd16 Provider: {SELECTED_PROVIDER}\")\n",
        "print(f\"   \ud83d\udce6 Model: {SELECTED_MODEL}\")\n",
        "print(f\"   \ud83c\udf10 Target Language: {TARGET_LANGUAGE}\")\n",
        "print(f\"   \ud83c\udfaf Quality Tier: {TRANSLATION_TIER}\")\n",
        "print(f\"   \ud83d\udce6 Chunk Size: {CHUNK_SIZE} words\")\n",
        "print(f\"   \ud83d\udd11 HF Token: {'Provided' if HF_TOKEN else 'Not provided'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Step 4: Translation Engine Setup\n",
        "This cell contains the complete translation engine code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Enhanced Translation Engine for Google Colab\n",
        "Supports HuggingFace models with multiple language targets\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Check GPU availability\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\ud83d\udda5\ufe0f Using device: {DEVICE}\")\n",
        "if DEVICE == \"cuda\":\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "# Translation Prompts for LLM-based models\n",
        "TRANSLATION_PROMPTS = {\n",
        "    \"BASIC\": {\n",
        "        \"system\": \"\"\"You are a professional translator. Translate the text accurately.\"\"\",\n",
        "        \"user\": \"\"\"Translate the following text to {target_lang}:\\n\\n{chunk}\\n\\nTranslation:\"\"\"\n",
        "    },\n",
        "    \"INTERMEDIATE\": {\n",
        "        \"system\": \"\"\"You are an expert literary translator. Create translations that feel natural in the target language while preserving all meaning and nuance.\"\"\",\n",
        "        \"user\": \"\"\"Translate the following text to {target_lang}. Maintain all details, dialogue, and descriptions:\\n\\n{chunk}\\n\\nComplete Translation:\"\"\"\n",
        "    },\n",
        "    \"ADVANCED\": {\n",
        "        \"system\": \"\"\"You are a master literary translator. Your translations should feel like they were originally written in the target language by a native speaker. Preserve every sentence, every detail, every nuance.\"\"\",\n",
        "        \"user\": \"\"\"Translate the COMPLETE passage below to {target_lang}.\\n\\nRequirements:\\n- Translate EVERY sentence\\n- Maintain ALL dialogue\\n- Preserve ALL descriptions\\n- Keep similar length\\n\\nText:\\n{chunk}\\n\\nComplete Translation:\"\"\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Language name mappings\n",
        "LANG_NAMES = {\n",
        "    'hin_Deva': 'Hindi', 'ben_Beng': 'Bengali', 'tam_Taml': 'Tamil',\n",
        "    'tel_Telu': 'Telugu', 'mar_Deva': 'Marathi', 'guj_Gujr': 'Gujarati',\n",
        "    'kan_Knda': 'Kannada', 'mal_Mlym': 'Malayalam', 'pan_Guru': 'Punjabi',\n",
        "    'ory_Orya': 'Odia', 'urd_Arab': 'Urdu', 'spa_Latn': 'Spanish',\n",
        "    'fra_Latn': 'French', 'deu_Latn': 'German', 'zho_Hans': 'Chinese',\n",
        "    'jpn_Jpan': 'Japanese'\n",
        "}\n",
        "\n",
        "\n",
        "def chunk_text(text, chunk_words=350):\n",
        "    \"\"\"Split text into chunks at paragraph boundaries.\"\"\"\n",
        "    paragraph_patterns = [\n",
        "        r'\\n\\s*\\n',\n",
        "        r'\\r\\n\\s*\\r\\n',\n",
        "        r'\\n\\s{2,}\\n',\n",
        "    ]\n",
        "    paragraph_split_pattern = '|'.join(paragraph_patterns)\n",
        "    paragraphs = re.split(paragraph_split_pattern, text)\n",
        "    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_count = 0\n",
        "\n",
        "    for para in paragraphs:\n",
        "        para_words = para.split()\n",
        "        para_count = len(para_words)\n",
        "\n",
        "        if para_count > chunk_words:\n",
        "            if current_chunk:\n",
        "                chunks.append('\\n\\n'.join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_count = 0\n",
        "\n",
        "            words = para.split()\n",
        "            for i in range(0, len(words), chunk_words):\n",
        "                chunk_words_list = words[i:i + chunk_words]\n",
        "                chunk_text = ' '.join(chunk_words_list)\n",
        "                chunks.append(chunk_text)\n",
        "        else:\n",
        "            if current_count + para_count > chunk_words and current_chunk:\n",
        "                chunks.append('\\n\\n'.join(current_chunk))\n",
        "                current_chunk = [para]\n",
        "                current_count = para_count\n",
        "            else:\n",
        "                current_chunk.append(para)\n",
        "                current_count += para_count\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append('\\n\\n'.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def clean_translation(text):\n",
        "    \"\"\"Clean up translation artifacts.\"\"\"\n",
        "    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'```\\w*\\n?', '', text)\n",
        "    text = re.sub(r'(Translation:|Hindi Translation:|Here\\'s the translation:)', '', text, flags=re.IGNORECASE)\n",
        "    lines = [line.strip() for line in text.split('\\n')]\n",
        "    text = '\\n\\n'.join(line for line in lines if line)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "class TranslationEngine:\n",
        "    \"\"\"Translation engine with HuggingFace support.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, target_lang, device=\"cuda\", hf_token=None):\n",
        "        self.model_name = model_name\n",
        "        self.target_lang = target_lang\n",
        "        self.device = device\n",
        "        self.hf_token = hf_token\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.translator = None\n",
        "        self.model_type = self._detect_model_type(model_name)\n",
        "        \n",
        "        # Try to get token from HuggingFace Hub if not provided\n",
        "        if not self.hf_token:\n",
        "            try:\n",
        "                from huggingface_hub import HfFolder\n",
        "                self.hf_token = HfFolder.get_token()\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        if self.hf_token:\n",
        "            os.environ['HF_TOKEN'] = self.hf_token\n",
        "        \n",
        "        self.load_model()\n",
        "    \n",
        "    def _detect_model_type(self, model_name):\n",
        "        \"\"\"Detect model type from name.\"\"\"\n",
        "        model_lower = model_name.lower()\n",
        "        if 'nllb' in model_lower:\n",
        "            return 'nllb'\n",
        "        elif 'indictrans' in model_lower:\n",
        "            return 'indictrans'\n",
        "        elif 'opus-mt' in model_lower or 'helsinki' in model_lower:\n",
        "            return 'opus'\n",
        "        elif 'madlad' in model_lower:\n",
        "            return 'madlad'\n",
        "        elif 'mbart' in model_lower:\n",
        "            return 'mbart'\n",
        "        elif 'hy-mt' in model_lower or 'hunyuan' in model_lower:\n",
        "            return 'hymt'  # Tencent Hunyuan MT\n",
        "        elif 'translategemma' in model_lower:\n",
        "            return 'translategemma'  # Google TranslateGemma\n",
        "        else:\n",
        "            return 'causal'  # LLM-based translation\n",
        "    \n",
        "    def load_model(self):\n",
        "        \"\"\"Load translation model.\"\"\"\n",
        "        print(f\"\ud83d\udce5 Loading model: {self.model_name}\")\n",
        "        print(f\"   Model type: {self.model_type}\")\n",
        "        \n",
        "        try:\n",
        "            if self.model_type in ['nllb', 'opus', 'mbart']:\n",
        "                self._load_seq2seq_model()\n",
        "            elif self.model_type == 'indictrans':\n",
        "                self._load_indictrans_model()\n",
        "            elif self.model_type == 'madlad':\n",
        "                self._load_madlad_model()\n",
        "            elif self.model_type == 'hymt':\n",
        "                self._load_hymt_model()\n",
        "            elif self.model_type == 'translategemma':\n",
        "                self._load_translategemma_model()\n",
        "            else:\n",
        "                self._load_causal_model()\n",
        "            \n",
        "            print(\"\u2705 Model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c Failed to load model: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def _load_seq2seq_model(self):\n",
        "        \"\"\"Load Seq2Seq translation model (NLLB, OPUS, mBART).\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            src_lang=\"eng_Latn\" if self.model_type == 'nllb' else None\n",
        "        )\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "        )\n",
        "        \n",
        "        if self.device != \"cuda\":\n",
        "            self.model = self.model.to(self.device)\n",
        "        \n",
        "        self.translator = pipeline(\n",
        "            \"translation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            src_lang=\"eng_Latn\" if self.model_type == 'nllb' else \"en\",\n",
        "            tgt_lang=self.target_lang if self.model_type == 'nllb' else None,\n",
        "            max_length=1024,\n",
        "            device=0 if self.device == \"cuda\" else -1\n",
        "        )\n",
        "    \n",
        "    def _load_indictrans_model(self):\n",
        "        \"\"\"Load IndicTrans2 model.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "        )\n",
        "        \n",
        "        if self.device == \"cuda\":\n",
        "            self.model = self.model.cuda()\n",
        "    \n",
        "    def _load_madlad_model(self):\n",
        "        \"\"\"Load MADLAD-400 model.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token\n",
        "        )\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "        )\n",
        "    \n",
        "    def _load_hymt_model(self):\n",
        "        \"\"\"Load Tencent Hunyuan MT model.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "        )\n",
        "        \n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "    \n",
        "    def _load_translategemma_model(self):\n",
        "        \"\"\"Load Google TranslateGemma model (gated).\"\"\"\n",
        "        # Check for token\n",
        "        if not self.hf_token:\n",
        "            raise ValueError(\"TranslateGemma is a gated model. Please login to HuggingFace first (run the login cell) or provide HF Token.\")\n",
        "        \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token\n",
        "        )\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "        )\n",
        "        \n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "    \n",
        "    def _load_causal_model(self):\n",
        "        \"\"\"Load causal LM for translation.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token\n",
        "        )\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "        )\n",
        "        \n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "    \n",
        "    def _filter_inputs(self, inputs):\n",
        "        \"\"\"Filter out unsupported model kwargs like token_type_ids.\"\"\"\n",
        "        # Remove token_type_ids if present (many models don't use them)\n",
        "        if 'token_type_ids' in inputs:\n",
        "            del inputs['token_type_ids']\n",
        "        return inputs\n",
        "    \n",
        "    def translate(self, text, tier=\"INTERMEDIATE\"):\n",
        "        \"\"\"Translate text based on model type.\"\"\"\n",
        "        if self.model_type in ['nllb', 'opus', 'mbart']:\n",
        "            return self._translate_seq2seq(text)\n",
        "        elif self.model_type == 'indictrans':\n",
        "            return self._translate_indictrans(text)\n",
        "        elif self.model_type == 'madlad':\n",
        "            return self._translate_madlad(text)\n",
        "        elif self.model_type == 'hymt':\n",
        "            return self._translate_hymt(text)\n",
        "        elif self.model_type == 'translategemma':\n",
        "            return self._translate_translategemma(text)\n",
        "        else:\n",
        "            return self._translate_causal(text, tier)\n",
        "    \n",
        "    def _translate_seq2seq(self, text):\n",
        "        \"\"\"Translate using Seq2Seq model.\"\"\"\n",
        "        result = self.translator(text, max_length=1024)\n",
        "        return result[0]['translation_text']\n",
        "    \n",
        "    def _translate_indictrans(self, text):\n",
        "        \"\"\"Translate using IndicTrans2.\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = self._filter_inputs(inputs)\n",
        "        \n",
        "        if self.device == \"cuda\":\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=512,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=1\n",
        "            )\n",
        "        \n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    def _translate_madlad(self, text):\n",
        "        \"\"\"Translate using MADLAD-400.\"\"\"\n",
        "        # MADLAD uses language tags like <2hi> for Hindi\n",
        "        lang_code = self.target_lang.split('_')[0][:2]  # Extract 2-letter code\n",
        "        tagged_text = f\"<2{lang_code}> {text}\"\n",
        "        \n",
        "        inputs = self.tokenizer(tagged_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = self._filter_inputs(inputs)\n",
        "        \n",
        "        if self.device == \"cuda\":\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=512,\n",
        "                num_beams=4\n",
        "            )\n",
        "        \n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    def _translate_hymt(self, text):\n",
        "        \"\"\"Translate using Tencent Hunyuan MT.\"\"\"\n",
        "        lang_name = LANG_NAMES.get(self.target_lang, self.target_lang)\n",
        "        \n",
        "        # Hunyuan MT uses a simple prompt format\n",
        "        prompt = f\"Translate the following text to {lang_name}:\\n{text}\\n\\nTranslation:\"\n",
        "        \n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
        "        inputs = self._filter_inputs(inputs)  # Remove token_type_ids\n",
        "        \n",
        "        if self.device == \"cuda\":\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract translation after the prompt\n",
        "        if 'Translation:' in generated:\n",
        "            translation = generated.split('Translation:')[-1].strip()\n",
        "        else:\n",
        "            translation = generated[len(prompt):].strip()\n",
        "        \n",
        "        return translation\n",
        "    \n",
        "    def _translate_translategemma(self, text):\n",
        "        \"\"\"Translate using Google TranslateGemma.\"\"\"\n",
        "        lang_name = LANG_NAMES.get(self.target_lang, self.target_lang)\n",
        "        \n",
        "        # TranslateGemma instruction format\n",
        "        prompt = f\"Translate the following text from English to {lang_name}:\\n\\n{text}\\n\\nTranslation:\"\n",
        "        \n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
        "        inputs = self._filter_inputs(inputs)  # Remove token_type_ids\n",
        "        \n",
        "        if self.device == \"cuda\":\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.pad_token_id\n",
        "            )\n",
        "        \n",
        "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract translation after the prompt\n",
        "        if 'Translation:' in generated:\n",
        "            translation = generated.split('Translation:')[-1].strip()\n",
        "        else:\n",
        "            translation = generated[len(prompt):].strip()\n",
        "        \n",
        "        return translation\n",
        "    \n",
        "    def _translate_causal(self, text, tier):\n",
        "        \"\"\"Translate using causal LM with prompts.\"\"\"\n",
        "        prompts = TRANSLATION_PROMPTS[tier]\n",
        "        \n",
        "        # Get language name from code\n",
        "        lang_name = LANG_NAMES.get(self.target_lang, self.target_lang)\n",
        "        \n",
        "        full_prompt = f\"{prompts['system']}\\n\\n{prompts['user'].format(target_lang=lang_name, chunk=text)}\"\n",
        "        \n",
        "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
        "        inputs = self._filter_inputs(inputs)  # Remove token_type_ids\n",
        "        \n",
        "        if self.device == \"cuda\":\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.4,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.pad_token_id\n",
        "            )\n",
        "        \n",
        "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract translation from response\n",
        "        if \"Translation:\" in generated:\n",
        "            return generated.split(\"Translation:\")[-1].strip()\n",
        "        return generated[len(full_prompt):].strip()\n",
        "\n",
        "\n",
        "class TranslationGenerator:\n",
        "    \"\"\"Main translation generator class.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, target_lang, device=\"cuda\", output_dir=\".\", tier=\"INTERMEDIATE\", chunk_size=350, hf_token=None):\n",
        "        self.model_name = model_name\n",
        "        self.target_lang = target_lang\n",
        "        self.device = device\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.tier = tier\n",
        "        self.chunk_size = chunk_size\n",
        "        \n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.engine = TranslationEngine(model_name, target_lang, device, hf_token)\n",
        "    \n",
        "    def translate_file(self, input_file):\n",
        "        \"\"\"Translate entire file.\"\"\"\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"\ud83c\udf10 TRANSLATION GENERATOR\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"\ud83d\udcc4 Input: {input_file}\")\n",
        "        print(f\"\ud83e\udd16 Model: {self.model_name}\")\n",
        "        print(f\"\ud83c\udf10 Target: {self.target_lang}\")\n",
        "        print(f\"\ud83c\udfaf Quality: {self.tier}\")\n",
        "        print(f\"\ud83d\udda5\ufe0f Device: {self.device}\")\n",
        "        print(f\"{'=' * 70}\\n\")\n",
        "        \n",
        "        # Read input\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        \n",
        "        # Clean markers\n",
        "        lines = text.split('\\n')\n",
        "        cleaned = [l for l in lines if not (l.strip().startswith('===') and l.strip().endswith('==='))]\n",
        "        text = '\\n'.join(cleaned).strip()\n",
        "        \n",
        "        orig_words = len(text.split())\n",
        "        orig_chars = len(text)\n",
        "        print(f\"\ud83d\udcca Input: {orig_chars:,} chars, {orig_words:,} words\")\n",
        "        \n",
        "        # Chunk text\n",
        "        print(f\"\\n\ud83d\udce6 Creating chunks ({self.chunk_size} words each)...\")\n",
        "        chunks = chunk_text(text, self.chunk_size)\n",
        "        print(f\"\u2705 Created {len(chunks)} chunks\")\n",
        "        \n",
        "        # Translate chunks\n",
        "        print(f\"\\n\ud83c\udfaf STARTING TRANSLATION\\n\")\n",
        "        \n",
        "        translations = []\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for i, chunk in enumerate(chunks, 1):\n",
        "            chunk_start = time.time()\n",
        "            \n",
        "            print(f\"\\n{'=' * 50}\")\n",
        "            print(f\"\ud83d\udcc4 Chunk {i}/{len(chunks)}\")\n",
        "            print(f\"   Input: {len(chunk.split())} words, {len(chunk)} chars\")\n",
        "            \n",
        "            try:\n",
        "                translated = self.engine.translate(chunk, self.tier)\n",
        "                translated = clean_translation(translated)\n",
        "                translations.append(translated)\n",
        "                \n",
        "                chunk_time = time.time() - chunk_start\n",
        "                print(f\"   Output: {len(translated)} chars\")\n",
        "                print(f\"   \u2705 Completed in {chunk_time:.1f}s\")\n",
        "                \n",
        "                # Progress\n",
        "                elapsed = time.time() - start_time\n",
        "                avg = elapsed / i\n",
        "                remaining = len(chunks) - i\n",
        "                eta = remaining * avg\n",
        "                print(f\"   \ud83d\udcc8 Progress: {i/len(chunks)*100:.1f}% | ETA: {eta/60:.1f}m\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   \u274c Error: {e}\")\n",
        "                translations.append(f\"[TRANSLATION ERROR: {e}]\")\n",
        "        \n",
        "        # Combine translations\n",
        "        final_translation = \"\\n\\n\".join(translations)\n",
        "        \n",
        "        # Save output\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        lang_code = self.target_lang.split('_')[0]\n",
        "        output_file = self.output_dir / f\"translation_{lang_code}_{timestamp}.txt\"\n",
        "        \n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_translation)\n",
        "        \n",
        "        # Summary\n",
        "        total_time = time.time() - start_time\n",
        "        trans_chars = len(final_translation)\n",
        "        \n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"\ud83c\udf89 TRANSLATION COMPLETE!\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"\u23f1\ufe0f Time: {total_time/60:.1f} minutes\")\n",
        "        print(f\"\ud83d\udce6 Chunks: {len(chunks)}\")\n",
        "        print(f\"\u26a1 Avg/chunk: {total_time/len(chunks):.1f}s\")\n",
        "        print(f\"\ud83d\udcdd Input: {orig_chars:,} chars\")\n",
        "        print(f\"\ud83d\udcdd Output: {trans_chars:,} chars\")\n",
        "        print(f\"\ud83d\udcca Ratio: {trans_chars/orig_chars:.2f}x\")\n",
        "        print(f\"\ud83d\udcbe Output: {output_file}\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        \n",
        "        return str(output_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============= OLLAMA TRANSLATION ENGINE =============\n",
        "\n",
        "class OllamaTranslationEngine:\n",
        "    \"\"\"Translation engine using Ollama local models.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, target_lang, tier=\"INTERMEDIATE\"):\n",
        "        self.model_name = model_name\n",
        "        self.target_lang = target_lang\n",
        "        self.tier = tier\n",
        "        self.lang_name = LANG_NAMES.get(target_lang, target_lang)\n",
        "        \n",
        "        print(f\"\ud83d\udce5 Initializing Ollama engine with model: {model_name}\")\n",
        "        print(f\"   Target language: {self.lang_name}\")\n",
        "        \n",
        "        # Verify model is available\n",
        "        try:\n",
        "            import ollama\n",
        "            self.client = ollama\n",
        "            models = ollama.list()\n",
        "            available = [m.get('name', '').split(':')[0] for m in models.get('models', [])]\n",
        "            model_base = model_name.split(':')[0]\n",
        "            \n",
        "            if not any(model_base in m for m in available):\n",
        "                print(f\"\u26a0\ufe0f Model '{model_name}' not found locally. Attempting to pull...\")\n",
        "                ollama.pull(model_name)\n",
        "                print(f\"\u2705 Model '{model_name}' pulled successfully!\")\n",
        "            else:\n",
        "                print(f\"\u2705 Model '{model_name}' is available!\")\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c Error initializing Ollama: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def translate(self, text, tier=None):\n",
        "        \"\"\"Translate text using Ollama model.\"\"\"\n",
        "        if tier:\n",
        "            self.tier = tier\n",
        "        \n",
        "        prompts = TRANSLATION_PROMPTS[self.tier]\n",
        "        \n",
        "        system_prompt = prompts['system']\n",
        "        user_prompt = prompts['user'].format(target_lang=self.lang_name, chunk=text)\n",
        "        \n",
        "        try:\n",
        "            response = self.client.chat(\n",
        "                model=self.model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                options={\n",
        "                    \"temperature\": 0.3,\n",
        "                    \"num_predict\": 2048,\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            translation = response['message']['content']\n",
        "            return clean_translation(translation)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\u274c Ollama translation error: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "class OllamaTranslationGenerator:\n",
        "    \"\"\"Translation generator using Ollama models.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, target_lang, output_dir=\".\", tier=\"INTERMEDIATE\", chunk_size=350):\n",
        "        self.model_name = model_name\n",
        "        self.target_lang = target_lang\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.tier = tier\n",
        "        self.chunk_size = chunk_size\n",
        "        \n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.engine = OllamaTranslationEngine(model_name, target_lang, tier)\n",
        "    \n",
        "    def translate_file(self, input_file):\n",
        "        \"\"\"Translate entire file using Ollama.\"\"\"\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"\ud83c\udf10 OLLAMA TRANSLATION GENERATOR\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"\ud83d\udcc4 Input: {input_file}\")\n",
        "        print(f\"\ud83e\udd99 Model: {self.model_name}\")\n",
        "        print(f\"\ud83c\udf10 Target: {self.target_lang}\")\n",
        "        print(f\"\ud83c\udfaf Quality: {self.tier}\")\n",
        "        print(f\"{'=' * 70}\\n\")\n",
        "        \n",
        "        # Read input\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        \n",
        "        # Clean markers\n",
        "        lines = text.split('\\n')\n",
        "        cleaned = [l for l in lines if not (l.strip().startswith('===') and l.strip().endswith('==='))]\n",
        "        text = '\\n'.join(cleaned).strip()\n",
        "        \n",
        "        orig_words = len(text.split())\n",
        "        orig_chars = len(text)\n",
        "        print(f\"\ud83d\udcca Input: {orig_chars:,} chars, {orig_words:,} words\")\n",
        "        \n",
        "        # Chunk text\n",
        "        print(f\"\\n\ud83d\udce6 Creating chunks ({self.chunk_size} words each)...\")\n",
        "        chunks = chunk_text(text, self.chunk_size)\n",
        "        print(f\"\u2705 Created {len(chunks)} chunks\")\n",
        "        \n",
        "        # Translate chunks\n",
        "        print(f\"\\n\ud83c\udfaf STARTING TRANSLATION\\n\")\n",
        "        \n",
        "        translations = []\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for i, chunk in enumerate(chunks, 1):\n",
        "            chunk_start = time.time()\n",
        "            \n",
        "            print(f\"\\n{'=' * 50}\")\n",
        "            print(f\"\ud83d\udcc4 Chunk {i}/{len(chunks)}\")\n",
        "            print(f\"   Input: {len(chunk.split())} words, {len(chunk)} chars\")\n",
        "            \n",
        "            try:\n",
        "                translated = self.engine.translate(chunk, self.tier)\n",
        "                translations.append(translated)\n",
        "                \n",
        "                chunk_time = time.time() - chunk_start\n",
        "                print(f\"   Output: {len(translated)} chars\")\n",
        "                print(f\"   \u2705 Completed in {chunk_time:.1f}s\")\n",
        "                \n",
        "                # Progress\n",
        "                elapsed = time.time() - start_time\n",
        "                avg = elapsed / i\n",
        "                remaining = len(chunks) - i\n",
        "                eta = remaining * avg\n",
        "                print(f\"   \ud83d\udcc8 Progress: {i/len(chunks)*100:.1f}% | ETA: {eta/60:.1f}m\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   \u274c Error: {e}\")\n",
        "                translations.append(f\"[TRANSLATION ERROR: {e}]\")\n",
        "        \n",
        "        # Combine translations\n",
        "        final_translation = \"\\n\\n\".join(translations)\n",
        "        \n",
        "        # Save output\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        lang_code = self.target_lang.split('_')[0]\n",
        "        output_file = self.output_dir / f\"translation_{lang_code}_{timestamp}.txt\"\n",
        "        \n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_translation)\n",
        "        \n",
        "        # Summary\n",
        "        total_time = time.time() - start_time\n",
        "        trans_chars = len(final_translation)\n",
        "        \n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"\ud83c\udf89 TRANSLATION COMPLETE!\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"\u23f1\ufe0f Time: {total_time/60:.1f} minutes\")\n",
        "        print(f\"\ud83d\udce6 Chunks: {len(chunks)}\")\n",
        "        print(f\"\u26a1 Avg/chunk: {total_time/len(chunks):.1f}s\")\n",
        "        print(f\"\ud83d\udcdd Input: {orig_chars:,} chars\")\n",
        "        print(f\"\ud83d\udcdd Output: {trans_chars:,} chars\")\n",
        "        print(f\"\ud83d\udcca Ratio: {trans_chars/orig_chars:.2f}x\")\n",
        "        print(f\"\ud83d\udcbe Output: {output_file}\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        \n",
        "        return str(output_file)\n",
        "\n",
        "\n",
        "print(\"\u2705 Translation Engine loaded and ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf10 Step 5: Generate Translation\n",
        "Run this cell to translate your uploaded file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "OUTPUT_DIR = \"./translation_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Initialize the generator based on provider\n",
        "print(\"\ud83d\ude80 Initializing Translation Generator...\")\n",
        "print(f\"   Provider: {SELECTED_PROVIDER}\")\n",
        "print(f\"   Model: {SELECTED_MODEL}\")\n",
        "\n",
        "if SELECTED_PROVIDER == \"ollama\":\n",
        "    # Use Ollama generator\n",
        "    generator = OllamaTranslationGenerator(\n",
        "        model_name=SELECTED_MODEL,\n",
        "        target_lang=TARGET_LANGUAGE,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        tier=TRANSLATION_TIER,\n",
        "        chunk_size=CHUNK_SIZE\n",
        "    )\n",
        "else:\n",
        "    # Use HuggingFace generator\n",
        "    generator = TranslationGenerator(\n",
        "        model_name=SELECTED_MODEL,\n",
        "        target_lang=TARGET_LANGUAGE,\n",
        "        device=DEVICE,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        tier=TRANSLATION_TIER,\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        hf_token=HF_TOKEN\n",
        "    )\n",
        "\n",
        "# Translate\n",
        "print(f\"\\n\ud83c\udf10 Starting translation...\")\n",
        "OUTPUT_FILE = generator.translate_file(UPLOADED_FILE)\n",
        "\n",
        "print(f\"\\n\u2705 Translation file generated: {OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcd6 Step 6: Preview & Download Translation\n",
        "View your translation and download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "import os\n",
        "\n",
        "if os.path.exists(OUTPUT_FILE):\n",
        "    # Read and display translation\n",
        "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "        translation = f.read()\n",
        "    \n",
        "    file_size = os.path.getsize(OUTPUT_FILE) / 1024  # KB\n",
        "    word_count = len(translation.split())\n",
        "    \n",
        "    print(f\"\ud83d\udcca Translation stats:\")\n",
        "    print(f\"   Words: {word_count:,}\")\n",
        "    print(f\"   Characters: {len(translation):,}\")\n",
        "    print(f\"   File size: {file_size:.2f} KB\")\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcd6 Preview (first 1000 chars):\")\n",
        "    print(f\"{'=' * 50}\")\n",
        "    print(translation[:1000])\n",
        "    print(f\"{'=' * 50}\")\n",
        "    if len(translation) > 1000:\n",
        "        print(f\"... [truncated, {len(translation) - 1000:,} more chars]\")\n",
        "else:\n",
        "    print(\"\u274c Output file not found. Please run the translation step again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the translated file\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\ud83d\udce5 Downloading your translated file...\")\n",
        "files.download(OUTPUT_FILE)\n",
        "print(\"\u2705 Download started! Check your browser's download folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcbe (Optional) Save to Google Drive\n",
        "If you want to save the translation to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "print(\"\ud83d\udcc2 Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output folder in Drive\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/Translation_Output\"\n",
        "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Copy file to Drive\n",
        "drive_output_path = os.path.join(DRIVE_OUTPUT_DIR, os.path.basename(OUTPUT_FILE))\n",
        "shutil.copy(OUTPUT_FILE, drive_output_path)\n",
        "\n",
        "print(f\"\\n\u2705 Translation saved to Google Drive:\")\n",
        "print(f\"   \ud83d\udcc1 {drive_output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83d\udcda Quick Reference\n",
        "\n",
        "### Supported Languages (NLLB Codes):\n",
        "| Language | Code |\n",
        "|----------|------|\n",
        "| Hindi | `hin_Deva` |\n",
        "| Bengali | `ben_Beng` |\n",
        "| Tamil | `tam_Taml` |\n",
        "| Telugu | `tel_Telu` |\n",
        "| Marathi | `mar_Deva` |\n",
        "| Gujarati | `guj_Gujr` |\n",
        "| Spanish | `spa_Latn` |\n",
        "| French | `fra_Latn` |\n",
        "| German | `deu_Latn` |\n",
        "\n",
        "### Recommended Models:\n",
        "| Model | Best For | Speed |\n",
        "|-------|----------|-------|\n",
        "| `facebook/nllb-200-distilled-600M` | Fast multilingual | \u26a1 Fast |\n",
        "| `facebook/nllb-200-1.3B` | Better quality | \ud83d\udd04 Medium |\n",
        "| `ai4bharat/indictrans2-en-indic-1B` | Best EN\u2192Hindi | \ud83d\udd04 Medium |\n",
        "| `google/madlad400-3b-mt` | Highest quality | \ud83d\udc22 Slow |\n",
        "\n",
        "### Quality Tiers:\n",
        "- **BASIC**: Fast, good for simple texts\n",
        "- **INTERMEDIATE**: Balanced quality and speed (recommended)\n",
        "- **ADVANCED**: Best quality, preserves all nuances\n",
        "\n",
        "### Tips:\n",
        "- Use Colab GPU for faster translation\n",
        "- For long texts, use smaller chunk sizes (200-300 words)\n",
        "- NLLB models are best for multilingual translation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
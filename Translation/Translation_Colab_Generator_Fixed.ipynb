{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwlPvEdHo_To"
      },
      "source": [
        "# üåê Hindi Literary Translation Generator for Google Colab\n",
        "\n",
        "**Enhanced Multilingual Translation with Real-time Streaming**\n",
        "\n",
        "This notebook allows you to:\n",
        "1. Upload your text file to translate\n",
        "2. Select AI provider (HuggingFace or Ollama) and model\n",
        "3. Choose target language and translation quality tier\n",
        "4. Generate translation and download the result\n",
        "\n",
        "**Supported Providers:**\n",
        "- ü§ó HuggingFace - Various translation models (faster on GPU)\n",
        "- ü¶ô Ollama - Run LLMs locally in Colab (supports many models)\n",
        "\n",
        "**Recommended Models for Hindi:**\n",
        "- HuggingFace: `facebook/nllb-200-distilled-600M` - Fast, multilingual\n",
        "- HuggingFace: `ai4bharat/indictrans2-en-indic-1B` - Best for English‚ÜíHindi\n",
        "- Ollama: `qwen2.5:3b` - Fast, good quality translations\n",
        "- Ollama: `qwen2.5:7b` - Better quality, balanced speed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qFe0O4Qo_Ty"
      },
      "source": [
        "## üì¶ Step 1: Install Dependencies\n",
        "Run this cell to install all required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7lGtgwxo_Tz",
        "outputId": "e952706b-ce4a-4347-810d-170bdd435852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîê HuggingFace Login (for gated models like TranslateGemma):\n",
            "   If you need access to gated models, run the next cell to login.\n",
            "   Otherwise, skip to Step 2.\n",
            "\n",
            "‚úÖ All dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch transformers accelerate sentencepiece\n",
        "!pip install -q colorama huggingface-hub\n",
        "\n",
        "# Install Ollama Python client\n",
        "!pip install -q ollama\n",
        "\n",
        "# Setup HuggingFace login for gated models\n",
        "print(\"\\nüîê HuggingFace Login (for gated models like TranslateGemma):\")\n",
        "print(\"   If you need access to gated models, run the next cell to login.\")\n",
        "print(\"   Otherwise, skip to Step 2.\\n\")\n",
        "print(\"‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTamaEkbo_T2"
      },
      "source": [
        "### üîê (Optional) HuggingFace Login\n",
        "Run this cell to login to HuggingFace for accessing gated models like `google/translategemma-27b-it`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h75DYZico_T3"
      },
      "outputs": [],
      "source": [
        "# HuggingFace Login for Gated Models\n",
        "from huggingface_hub import login, HfFolder\n",
        "import os\n",
        "\n",
        "print(\"üîê HuggingFace Login Options:\")\n",
        "print(\"   1. Interactive Login - Opens a browser/token prompt\")\n",
        "print(\"   2. Token Login - Paste your HF token directly\\n\")\n",
        "\n",
        "# Check if already logged in\n",
        "existing_token = HfFolder.get_token()\n",
        "if existing_token:\n",
        "    print(f\"‚úÖ Already logged in to HuggingFace!\")\n",
        "    print(f\"   Token: {existing_token[:10]}...{existing_token[-5:]}\")\n",
        "else:\n",
        "    print(\"üìù Not logged in. Choose a login method:\")\n",
        "    print(\"\\nüîπ Option 1: Interactive Login (recommended)\")\n",
        "    print(\"   Uncomment the line below and run this cell:\")\n",
        "    print(\"   # login()\\n\")\n",
        "    print(\"üîπ Option 2: Token Login\")\n",
        "    print(\"   Get your token from: https://huggingface.co/settings/tokens\")\n",
        "    print(\"   Then paste it in the HF Token field in Step 3\\n\")\n",
        "\n",
        "# Uncomment ONE of the following lines to login:\n",
        "\n",
        "# Option 1: Interactive login (will prompt for token)\n",
        "# login()\n",
        "\n",
        "# Option 2: Direct token login (paste your token)\n",
        "# login(token=\"hf_YOUR_TOKEN_HERE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ToUekA_o_T3"
      },
      "source": [
        "### ü¶ô (Optional) Ollama Setup\n",
        "Run these cells if you want to use Ollama models. Skip if using HuggingFace only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx0gmXRLo_T4",
        "outputId": "af1c5c4a-30f5-402d-dcd0-9ec04c2dd7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶ô Installing Ollama...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\n",
            "üöÄ Starting Ollama server in background...\n",
            "‚úÖ Ollama server is running!\n"
          ]
        }
      ],
      "source": [
        "# Install and start Ollama server (required for Ollama models)\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"ü¶ô Installing Ollama...\")\n",
        "\n",
        "# Install zstd first (required for Ollama extraction)\n",
        "!apt-get update -qq && apt-get install -y -qq zstd > /dev/null 2>&1\n",
        "\n",
        "# Download and install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"\\nüöÄ Starting Ollama server in background...\")\n",
        "\n",
        "# Start Ollama server in background\n",
        "os.environ['OLLAMA_HOST'] = '127.0.0.1:11434'\n",
        "subprocess.Popen(['/usr/local/bin/ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Verify server is running\n",
        "try:\n",
        "    import ollama\n",
        "    ollama.list()\n",
        "    print(\"‚úÖ Ollama server is running!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Ollama server may not be ready yet. Error: {e}\")\n",
        "    print(\"   Please wait a few seconds and try running the next cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148,
          "referenced_widgets": [
            "304220edff9f4b83ac02b6b715d2eb1f",
            "cb3161a39d0e4436bc1369d910221dbd",
            "6ff0d5cf33c34fc4ba9300a93f4f826c",
            "abda3bc4bd824f538dc5a1830b5cfd94",
            "adddb530d6f64bc2a070cf0c2d7d140a",
            "02dd83c99a86459bb2e14fbc65c78677"
          ]
        },
        "id": "2mXHkQ4bo_T7",
        "outputId": "4ad65112-3512-478d-ffa6-9b1eaf7a5d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶ô Ollama Model Download\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Model to Pull:', layout=Layout(width='400px'), options=('qwen2.5:3b (Fast, ~2GB)', 'qwen‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "304220edff9f4b83ac02b6b715d2eb1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Custom Model:', layout=Layout(width='400px'), placeholder='Or enter custom model n‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abda3bc4bd824f538dc5a1830b5cfd94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üí° Select a model and run the next cell to download it.\n"
          ]
        }
      ],
      "source": [
        "# Pull Ollama model (run this cell to download your chosen model)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"ü¶ô Ollama Model Download\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Model selection for pulling\n",
        "OLLAMA_MODELS_TO_PULL = {\n",
        "    \"qwen2.5:3b (Fast, ~2GB)\": \"qwen2.5:3b\",\n",
        "    \"qwen2.5:7b (Balanced, ~4GB)\": \"qwen2.5:7b\",\n",
        "    \"deepseek-r1:7b (Reasoning, ~4GB)\": \"deepseek-r1:7b\",\n",
        "    \"llama3.2:3b (Fast, ~2GB)\": \"llama3.2:3b\",\n",
        "    \"mistral:7b (Quality, ~4GB)\": \"mistral:7b\",\n",
        "    \"gemma2:2b (Compact, ~1.5GB)\": \"gemma2:2b\",\n",
        "    \"phi3:mini (Compact, ~2GB)\": \"phi3:mini\"\n",
        "}\n",
        "\n",
        "model_pull_dropdown = widgets.Dropdown(\n",
        "    options=list(OLLAMA_MODELS_TO_PULL.keys()),\n",
        "    value=\"qwen2.5:3b (Fast, ~2GB)\",\n",
        "    description='Model to Pull:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "custom_ollama_pull = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Or enter custom model name (e.g., llama3:8b)',\n",
        "    description='Custom Model:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "display(model_pull_dropdown)\n",
        "display(custom_ollama_pull)\n",
        "print(\"\\nüí° Select a model and run the next cell to download it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqs_sQJwo_T7",
        "outputId": "e439d379-9370-454b-f538-3b1e7a5df6a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Pulling model: translategemma:27b\n",
            "   This may take several minutes depending on model size...\n",
            "\n",
            "   pulling d87ec9a33af4: 100.0% (17358640928/17358640928)\n",
            "   pulling e0a42594d802: 100.0% (358/358)\n",
            "   pulling 3e2c24001f9e: 100.0% (8431/8431)\n",
            "   pulling 339e884a40f6: 100.0% (61/61)\n",
            "   pulling 80f9232a32b9: 100.0% (490/490)\n",
            "   success\n",
            "\n",
            "‚úÖ Model 'translategemma:27b' pulled successfully!\n",
            "\n",
            "üìã Available Ollama models:\n",
            "   ‚Ä¢ unknown (16.17 GB)\n",
            "   ‚Ä¢ unknown (3.07 GB)\n"
          ]
        }
      ],
      "source": [
        "# Actually pull the selected model\n",
        "import ollama\n",
        "\n",
        "# Get model to pull\n",
        "if custom_ollama_pull.value.strip():\n",
        "    model_to_pull = custom_ollama_pull.value.strip()\n",
        "else:\n",
        "    model_to_pull = OLLAMA_MODELS_TO_PULL[model_pull_dropdown.value]\n",
        "\n",
        "print(f\"üì• Pulling model: {model_to_pull}\")\n",
        "print(\"   This may take several minutes depending on model size...\\n\")\n",
        "\n",
        "try:\n",
        "    # Pull with progress\n",
        "    current_digest = ''\n",
        "    for progress in ollama.pull(model_to_pull, stream=True):\n",
        "        digest = progress.get('digest', '')\n",
        "        if digest != current_digest and current_digest:\n",
        "            print()  # Newline between layers\n",
        "        current_digest = digest\n",
        "\n",
        "        status = progress.get('status', '')\n",
        "        if 'completed' in progress and 'total' in progress:\n",
        "            completed = progress['completed']\n",
        "            total = progress['total']\n",
        "            pct = (completed / total * 100) if total > 0 else 0\n",
        "            print(f\"\\r   {status}: {pct:.1f}% ({completed}/{total})\", end='', flush=True)\n",
        "        else:\n",
        "            print(f\"\\r   {status}\", end='', flush=True)\n",
        "\n",
        "    print(f\"\\n\\n‚úÖ Model '{model_to_pull}' pulled successfully!\")\n",
        "\n",
        "    # List available models\n",
        "    print(\"\\nüìã Available Ollama models:\")\n",
        "    models = ollama.list()\n",
        "    for model in models.get('models', []):\n",
        "        name = model.get('name', 'unknown')\n",
        "        size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
        "        print(f\"   ‚Ä¢ {name} ({size:.2f} GB)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error pulling model: {e}\")\n",
        "    print(\"   Make sure Ollama server is running (run the previous cell first).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8jvSSwso_T8"
      },
      "source": [
        "## üì§ Step 2: Upload Your Text File\n",
        "Upload the text file you want to translate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "D93OROyno_T9",
        "outputId": "ac40bd82-33f9-42f7-f1be-aecccc1c6d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Please upload your text file to translate:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-44816721-9fe3-4ced-8a94-8127b75fc52e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-44816721-9fe3-4ced-8a94-8127b75fc52e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CHAPTER 1.txt to CHAPTER 1 (1).txt\n",
            "\n",
            "‚úÖ Uploaded: CHAPTER 1 (1).txt\n",
            "üìÑ File size: 17212 bytes\n",
            "\n",
            "üìä Content stats:\n",
            "   Words: 2,949\n",
            "   Characters: 17,184\n",
            "\n",
            "üìù Preview (first 500 chars):\n",
            "CHAPTER 1 \n",
            "I am by birth a Genevese; and my family is one of the most distinguished of that republic. My ancestors had been for\n",
            "many years counsellors and syndics; and my father had filled several public situations with honour and reputation. He\n",
            "was respected by all who knew him for his integrity and indefatigable attention to public business. He passed his\n",
            "younger days perpetually occupied by the affairs of his country; and it was not until the decline of life that he thought of\n",
            "marrying, and b...\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"üì§ Please upload your text file to translate:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name\n",
        "UPLOADED_FILE = list(uploaded.keys())[0]\n",
        "print(f\"\\n‚úÖ Uploaded: {UPLOADED_FILE}\")\n",
        "print(f\"üìÑ File size: {len(uploaded[UPLOADED_FILE])} bytes\")\n",
        "\n",
        "# Display preview\n",
        "with open(UPLOADED_FILE, 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "    word_count = len(content.split())\n",
        "    char_count = len(content)\n",
        "\n",
        "print(f\"\\nüìä Content stats:\")\n",
        "print(f\"   Words: {word_count:,}\")\n",
        "print(f\"   Characters: {char_count:,}\")\n",
        "print(f\"\\nüìù Preview (first 500 chars):\\n{content[:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzRzM0qdo_T-"
      },
      "source": [
        "## ‚öôÔ∏è Step 3: Select AI Provider, Model & Language\n",
        "Choose your preferred translation model and settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "63a4fdc27166469e93eca220438fc390",
            "bcede039456e467e897f2b36accaee83",
            "c9fff22fee8e4aa0a0c58799965fe57a",
            "33ee3d6ab257469483537d9484b8d5a6",
            "a3d0cd9391394c7f8e72db7681cc6f76",
            "606c76b495af4577bbff68aeecb2c830",
            "c132b8b4b6804dd89cd3779d4205ace4",
            "3d78679d496d451292148d6614aaee05",
            "f051987cf1ef4260824fd4b75780c91f",
            "62e92d90e80044c89b13e498e5f503a8",
            "8e019dcd108c4ff2963b889d36fc8427",
            "79ca78e919884694b417e036c369c8cf",
            "9fb9a05b8edb4fc7b2ac2f104731c9ac",
            "34e8ba6ada154738a1405cb7c4976289",
            "aa821a52115847fb8bf04416563333aa",
            "a8fb6abf483f47229c0d0747f88251ee",
            "e24bb27cc39544a9b5f672ff7872f258",
            "aab930ac1a1043f69fbbd306cb818666",
            "44d51432f18241afb41d07f73df76c6d",
            "d270b543e09a40989ce4fad17ce9f12e",
            "b54ede47a75b49cf9024ad76c9fe4905",
            "990007db384e4eb29db1a8866016bb92",
            "47e690f6e8e44ad6b1196346d8ec8e01",
            "36870561a94241c8bd637e4e0b0492db"
          ]
        },
        "id": "ij9MOt-Ho_T-",
        "outputId": "be407ef0-418e-4d89-a7f6-c6fb18166f60"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>üéõÔ∏è Configure Translation Settings</h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Provider:', layout=Layout(width='400px'), options=('HuggingFace (Recommended for Colab)'‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63a4fdc27166469e93eca220438fc390"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Model:', layout=Layout(width='500px'), options=('facebook/nllb-200-distilled-600M (Fast,‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33ee3d6ab257469483537d9484b8d5a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Custom Model:', layout=Layout(width='500px'), placeholder='Enter HuggingFace model‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c132b8b4b6804dd89cd3779d4205ace4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Target Language:', layout=Layout(width='400px'), options=('Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)', 'Bengali (‡¶¨‡¶æ‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62e92d90e80044c89b13e498e5f503a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Custom Lang:', layout=Layout(width='400px'), placeholder='Enter NLLB language code‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fb9a05b8edb4fc7b2ac2f104731c9ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Quality:', index=1, layout=Layout(width='400px'), options=('BASIC - Fast, good quality',‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8fb6abf483f47229c0d0747f88251ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntSlider(value=350, description='Chunk Size:', layout=Layout(width='400px'), max=1000, min=100, step=50, styl‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44d51432f18241afb41d07f73df76c6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Password(description='HF Token:', layout=Layout(width='400px'), placeholder='Optional: HF token for gated mode‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "990007db384e4eb29db1a8866016bb92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üí° Tip: facebook/nllb-200-distilled-600M is recommended for fast Hindi translation!\n",
            "üîê Note: Models marked with üîê require HuggingFace login. Run the login cell above first.\n"
          ]
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Provider options\n",
        "PROVIDER_OPTIONS = {\n",
        "    \"HuggingFace (Recommended for Colab)\": \"huggingface\",\n",
        "    \"Ollama (Local only)\": \"ollama\"\n",
        "}\n",
        "\n",
        "# Model options by provider\n",
        "HF_MODEL_OPTIONS = {\n",
        "    \"facebook/nllb-200-distilled-600M (Fast, Multilingual)\": \"facebook/nllb-200-distilled-600M\",\n",
        "    \"facebook/nllb-200-1.3B (Better Quality)\": \"facebook/nllb-200-1.3B\",\n",
        "    \"ai4bharat/indictrans2-en-indic-1B (Best English‚ÜíHindi)\": \"ai4bharat/indictrans2-en-indic-1B\",\n",
        "    \"google/madlad400-3b-mt (High Quality, Slow)\": \"google/madlad400-3b-mt\",\n",
        "    \"Helsinki-NLP/opus-mt-en-hi (Simple EN‚ÜíHI)\": \"Helsinki-NLP/opus-mt-en-hi\",\n",
        "    \"tencent/HY-MT1.5-7B (Hunyuan MT - Best Quality)\": \"tencent/HY-MT1.5-7B\",\n",
        "    \"tencent/HY-MT1.5-1.8B (Hunyuan MT - Fast)\": \"tencent/HY-MT1.5-1.8B\",\n",
        "    \"google/translategemma-27b-it (üîê Gated - Requires Login)\": \"google/translategemma-27b-it\",\n",
        "    \"Custom Model (enter below)\": \"custom\"\n",
        "}\n",
        "\n",
        "OLLAMA_MODEL_OPTIONS = {\n",
        "    \"qwen2.5:3b (Fast)\": \"qwen2.5:3b\",\n",
        "    \"qwen2.5:7b (Balanced)\": \"qwen2.5:7b\",\n",
        "    \"deepseek-r1:7b (Reasoning)\": \"deepseek-r1:7b\",\n",
        "    \"llama3.2:3b (Fast)\": \"llama3.2:3b\",\n",
        "    \"Custom Model (enter below)\": \"custom\"\n",
        "}\n",
        "\n",
        "# Language options (NLLB language codes)\n",
        "LANGUAGE_OPTIONS = {\n",
        "    \"Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)\": \"hin_Deva\",\n",
        "    \"Bengali (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)\": \"ben_Beng\",\n",
        "    \"Tamil (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)\": \"tam_Taml\",\n",
        "    \"Telugu (‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å)\": \"tel_Telu\",\n",
        "    \"Marathi (‡§Æ‡§∞‡§æ‡§†‡•Ä)\": \"mar_Deva\",\n",
        "    \"Gujarati (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä)\": \"guj_Gujr\",\n",
        "    \"Kannada (‡≤ï‡≤®‡≥ç‡≤®‡≤°)\": \"kan_Knda\",\n",
        "    \"Malayalam (‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç)\": \"mal_Mlym\",\n",
        "    \"Punjabi (‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä)\": \"pan_Guru\",\n",
        "    \"Odia (‡¨ì‡¨°‡¨º‡¨ø‡¨Ü)\": \"ory_Orya\",\n",
        "    \"Urdu (ÿßÿ±ÿØŸà)\": \"urd_Arab\",\n",
        "    \"Spanish (Espa√±ol)\": \"spa_Latn\",\n",
        "    \"French (Fran√ßais)\": \"fra_Latn\",\n",
        "    \"German (Deutsch)\": \"deu_Latn\",\n",
        "    \"Chinese (‰∏≠Êñá)\": \"zho_Hans\",\n",
        "    \"Japanese (Êó•Êú¨Ë™û)\": \"jpn_Jpan\",\n",
        "    \"Custom (enter code below)\": \"custom\"\n",
        "}\n",
        "\n",
        "# Translation quality tiers\n",
        "TIER_OPTIONS = {\n",
        "    \"BASIC - Fast, good quality\": \"BASIC\",\n",
        "    \"INTERMEDIATE - Balanced (recommended)\": \"INTERMEDIATE\",\n",
        "    \"ADVANCED - Best quality, slower\": \"ADVANCED\"\n",
        "}\n",
        "\n",
        "# Provider dropdown\n",
        "provider_dropdown = widgets.Dropdown(\n",
        "    options=list(PROVIDER_OPTIONS.keys()),\n",
        "    value=\"HuggingFace (Recommended for Colab)\",\n",
        "    description='Provider:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "# Model dropdown (HuggingFace by default)\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=list(HF_MODEL_OPTIONS.keys()),\n",
        "    value=\"facebook/nllb-200-distilled-600M (Fast, Multilingual)\",\n",
        "    description='Model:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='500px')\n",
        ")\n",
        "\n",
        "# Custom model input\n",
        "custom_model_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter HuggingFace model name',\n",
        "    description='Custom Model:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='500px')\n",
        ")\n",
        "\n",
        "# Target language dropdown\n",
        "language_dropdown = widgets.Dropdown(\n",
        "    options=list(LANGUAGE_OPTIONS.keys()),\n",
        "    value=\"Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)\",\n",
        "    description='Target Language:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "# Custom language code input\n",
        "custom_language_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter NLLB language code (e.g., hin_Deva)',\n",
        "    description='Custom Lang:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "# Translation tier dropdown\n",
        "tier_dropdown = widgets.Dropdown(\n",
        "    options=list(TIER_OPTIONS.keys()),\n",
        "    value=\"INTERMEDIATE - Balanced (recommended)\",\n",
        "    description='Quality:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "# HuggingFace token (optional)\n",
        "hf_token_input = widgets.Password(\n",
        "    value='',\n",
        "    placeholder='Optional: HF token for gated models',\n",
        "    description='HF Token:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "# Chunk size slider\n",
        "chunk_size_slider = widgets.IntSlider(\n",
        "    value=350,\n",
        "    min=100,\n",
        "    max=1000,\n",
        "    step=50,\n",
        "    description='Chunk Size:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "display(HTML(\"<h3>üéõÔ∏è Configure Translation Settings</h3>\"))\n",
        "display(provider_dropdown)\n",
        "display(model_dropdown)\n",
        "display(custom_model_input)\n",
        "display(HTML(\"<br>\"))\n",
        "display(language_dropdown)\n",
        "display(custom_language_input)\n",
        "display(HTML(\"<br>\"))\n",
        "display(tier_dropdown)\n",
        "display(chunk_size_slider)\n",
        "display(hf_token_input)\n",
        "\n",
        "\n",
        "# Handle provider change to update model dropdown\n",
        "def on_provider_change(change):\n",
        "    if change['new'] == \"HuggingFace (Recommended for Colab)\":\n",
        "        model_dropdown.options = list(HF_MODEL_OPTIONS.keys())\n",
        "        model_dropdown.value = \"facebook/nllb-200-distilled-600M (Fast, Multilingual)\"\n",
        "        custom_model_input.placeholder = 'Enter HuggingFace model name'\n",
        "    else:\n",
        "        model_dropdown.options = list(OLLAMA_MODEL_OPTIONS.keys())\n",
        "        model_dropdown.value = \"qwen2.5:3b (Fast)\"\n",
        "        custom_model_input.placeholder = 'Enter Ollama model name'\n",
        "\n",
        "provider_dropdown.observe(on_provider_change, names='value')\n",
        "print(\"\\nüí° Tip: facebook/nllb-200-distilled-600M is recommended for fast Hindi translation!\")\n",
        "print(\"üîê Note: Models marked with üîê require HuggingFace login. Run the login cell above first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_LiRbodo_UG",
        "outputId": "c70f9272-1c29-497f-cd71-7c005c694402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Configuration saved:\n",
            "   ü§ñ Provider: ollama\n",
            "   üì¶ Model: translategemma:27b\n",
            "   üåê Target Language: hin_Deva\n",
            "   üéØ Quality Tier: ADVANCED\n",
            "   üì¶ Chunk Size: 350 words\n",
            "   üîë HF Token: Not provided\n"
          ]
        }
      ],
      "source": [
        "# Store the selected configuration\n",
        "SELECTED_PROVIDER = PROVIDER_OPTIONS[provider_dropdown.value]\n",
        "\n",
        "# Get model based on provider\n",
        "if SELECTED_PROVIDER == \"huggingface\":\n",
        "    selected_model_key = model_dropdown.value\n",
        "    SELECTED_MODEL = HF_MODEL_OPTIONS.get(selected_model_key, \"custom\")\n",
        "else:\n",
        "    SELECTED_MODEL = OLLAMA_MODEL_OPTIONS.get(model_dropdown.value, \"custom\")\n",
        "\n",
        "if SELECTED_MODEL == \"custom\":\n",
        "    SELECTED_MODEL = custom_model_input.value\n",
        "    if not SELECTED_MODEL:\n",
        "        raise ValueError(\"Please enter a custom model name!\")\n",
        "\n",
        "# Get target language\n",
        "TARGET_LANGUAGE = LANGUAGE_OPTIONS[language_dropdown.value]\n",
        "if TARGET_LANGUAGE == \"custom\":\n",
        "    TARGET_LANGUAGE = custom_language_input.value\n",
        "    if not TARGET_LANGUAGE:\n",
        "        raise ValueError(\"Please enter a custom language code!\")\n",
        "\n",
        "TRANSLATION_TIER = TIER_OPTIONS[tier_dropdown.value]\n",
        "CHUNK_SIZE = chunk_size_slider.value\n",
        "HF_TOKEN = hf_token_input.value if hf_token_input.value else None\n",
        "\n",
        "print(f\"\\n‚úÖ Configuration saved:\")\n",
        "print(f\"   ü§ñ Provider: {SELECTED_PROVIDER}\")\n",
        "print(f\"   üì¶ Model: {SELECTED_MODEL}\")\n",
        "print(f\"   üåê Target Language: {TARGET_LANGUAGE}\")\n",
        "print(f\"   üéØ Quality Tier: {TRANSLATION_TIER}\")\n",
        "print(f\"   üì¶ Chunk Size: {CHUNK_SIZE} words\")\n",
        "print(f\"   üîë HF Token: {'Provided' if HF_TOKEN else 'Not provided'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84JRP7rpo_UH"
      },
      "source": [
        "## üöÄ Step 4: Translation Engine Setup\n",
        "This cell contains the complete translation engine code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Zbs1fjGo_UH",
        "outputId": "41095ff9-fa6f-4779-97d3-d73445c64227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñ•Ô∏è Using device: cuda\n",
            "   GPU: Tesla T4\n",
            "   Memory: 15.83 GB\n",
            "‚úÖ Translation Engine loaded and ready!\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Enhanced Translation Engine for Google Colab\n",
        "Supports HuggingFace models with multiple language targets\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Check GPU availability\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è Using device: {DEVICE}\")\n",
        "if DEVICE == \"cuda\":\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "# Translation Prompts for LLM-based models\n",
        "TRANSLATION_PROMPTS = {\n",
        "    \"BASIC\": {\n",
        "        \"system\": \"\"\"You are a professional translator. Translate the text accurately.\"\"\",\n",
        "        \"user\": \"\"\"Translate the following text to {target_lang}:\\n\\n{chunk}\\n\\nTranslation:\"\"\"\n",
        "    },\n",
        "    \"INTERMEDIATE\": {\n",
        "        \"system\": \"\"\"You are an expert literary translator. Create translations that feel natural in the target language while preserving all meaning and nuance.\"\"\",\n",
        "        \"user\": \"\"\"Translate the following text to {target_lang}. Maintain all details, dialogue, and descriptions:\\n\\n{chunk}\\n\\nComplete Translation:\"\"\"\n",
        "    },\n",
        "    \"ADVANCED\": {\n",
        "        \"system\": \"\"\"You are a master literary translator. Your translations should feel like they were originally written in the target language by a native speaker. Preserve every sentence, every detail, every nuance.\\nRequirements:\\n- Translate EVERY sentence\\n- Maintain ALL dialogue\\n- Preserve ALL descriptions\\n- Keep similar length\\n\\n\"\"\",\n",
        "        \"user\": \"\"\"Translate the COMPLETE passage below to {target_lang}.\\n\\nText:\\n\\n{chunk}\"\"\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Language name mappings\n",
        "LANG_NAMES = {\n",
        "    'hin_Deva': 'Hindi', 'ben_Beng': 'Bengali', 'tam_Taml': 'Tamil',\n",
        "    'tel_Telu': 'Telugu', 'mar_Deva': 'Marathi', 'guj_Gujr': 'Gujarati',\n",
        "    'kan_Knda': 'Kannada', 'mal_Mlym': 'Malayalam', 'pan_Guru': 'Punjabi',\n",
        "    'ory_Orya': 'Odia', 'urd_Arab': 'Urdu', 'spa_Latn': 'Spanish',\n",
        "    'fra_Latn': 'French', 'deu_Latn': 'German', 'zho_Hans': 'Chinese',\n",
        "    'jpn_Jpan': 'Japanese'\n",
        "}\n",
        "\n",
        "\n",
        "def chunk_text(text, chunk_words=350):\n",
        "    \"\"\"Split text into chunks at paragraph boundaries.\"\"\"\n",
        "    paragraph_patterns = [\n",
        "        r'\\n\\s*\\n',\n",
        "        r'\\r\\n\\s*\\r\\n',\n",
        "        r'\\n\\s{2,}\\n',\n",
        "    ]\n",
        "    paragraph_split_pattern = '|'.join(paragraph_patterns)\n",
        "    paragraphs = re.split(paragraph_split_pattern, text)\n",
        "    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_count = 0\n",
        "\n",
        "    for para in paragraphs:\n",
        "        para_words = para.split()\n",
        "        para_count = len(para_words)\n",
        "\n",
        "        if para_count > chunk_words:\n",
        "            if current_chunk:\n",
        "                chunks.append('\\n\\n'.join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_count = 0\n",
        "\n",
        "            words = para.split()\n",
        "            for i in range(0, len(words), chunk_words):\n",
        "                chunk_words_list = words[i:i + chunk_words]\n",
        "                chunk_text = ' '.join(chunk_words_list)\n",
        "                chunks.append(chunk_text)\n",
        "        else:\n",
        "            if current_count + para_count > chunk_words and current_chunk:\n",
        "                chunks.append('\\n\\n'.join(current_chunk))\n",
        "                current_chunk = [para]\n",
        "                current_count = para_count\n",
        "            else:\n",
        "                current_chunk.append(para)\n",
        "                current_count += para_count\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append('\\n\\n'.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def clean_translation(text):\n",
        "    \"\"\"Clean up translation artifacts.\"\"\"\n",
        "    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'```\\w*\\n?', '', text)\n",
        "    text = re.sub(r'(Translation:|Hindi Translation:|Here\\'s the translation:)', '', text, flags=re.IGNORECASE)\n",
        "    lines = [line.strip() for line in text.split('\\n')]\n",
        "    text = '\\n\\n'.join(line for line in lines if line)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "class TranslationEngine:\n",
        "    \"\"\"Translation engine with HuggingFace support.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, target_lang, device=\"cuda\", hf_token=None):\n",
        "        self.model_name = model_name\n",
        "        self.target_lang = target_lang\n",
        "        self.device = device\n",
        "        self.hf_token = hf_token\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.translator = None\n",
        "        self.model_type = self._detect_model_type(model_name)\n",
        "\n",
        "        # Try to get token from HuggingFace Hub if not provided\n",
        "        if not self.hf_token:\n",
        "            try:\n",
        "                from huggingface_hub import HfFolder\n",
        "                self.hf_token = HfFolder.get_token()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if self.hf_token:\n",
        "            os.environ['HF_TOKEN'] = self.hf_token\n",
        "\n",
        "        self.load_model()\n",
        "\n",
        "    def _detect_model_type(self, model_name):\n",
        "        \"\"\"Detect model type from name.\"\"\"\n",
        "        model_lower = model_name.lower()\n",
        "        if 'nllb' in model_lower:\n",
        "            return 'nllb'\n",
        "        elif 'indictrans' in model_lower:\n",
        "            return 'indictrans'\n",
        "        elif 'opus-mt' in model_lower or 'helsinki' in model_lower:\n",
        "            return 'opus'\n",
        "        elif 'madlad' in model_lower:\n",
        "            return 'madlad'\n",
        "        elif 'mbart' in model_lower:\n",
        "            return 'mbart'\n",
        "        elif 'hy-mt' in model_lower or 'hunyuan' in model_lower:\n",
        "            return 'hymt'  # Tencent Hunyuan MT\n",
        "        elif 'translategemma' in model_lower:\n",
        "            return 'translategemma'  # Google TranslateGemma\n",
        "        else:\n",
        "            return 'causal'  # LLM-based translation\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load translation model.\"\"\"\n",
        "        print(f\"üì• Loading model: {self.model_name}\")\n",
        "        print(f\"   Model type: {self.model_type}\")\n",
        "\n",
        "        try:\n",
        "            if self.model_type in ['nllb', 'opus', 'mbart']:\n",
        "                self._load_seq2seq_model()\n",
        "            elif self.model_type == 'indictrans':\n",
        "                self._load_indictrans_model()\n",
        "            elif self.model_type == 'madlad':\n",
        "                self._load_madlad_model()\n",
        "            elif self.model_type == 'hymt':\n",
        "                self._load_hymt_model()\n",
        "            elif self.model_type == 'translategemma':\n",
        "                self._load_translategemma_model()\n",
        "            else:\n",
        "                self._load_causal_model()\n",
        "\n",
        "            print(\"‚úÖ Model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _load_seq2seq_model(self):\n",
        "        \"\"\"Load Seq2Seq translation model (NLLB, OPUS, mBART).\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            src_lang=\"eng_Latn\" if self.model_type == 'nllb' else None\n",
        "        )\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "        )\n",
        "\n",
        "        if self.device != \"cuda\":\n",
        "            self.model = self.model.to(self.device)\n",
        "\n",
        "        self.translator = pipeline(\n",
        "            \"translation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            src_lang=\"eng_Latn\" if self.model_type == 'nllb' else \"en\",\n",
        "            tgt_lang=self.target_lang if self.model_type == 'nllb' else None,\n",
        "            max_length=1024,\n",
        "            device=0 if self.device == \"cuda\" else -1\n",
        "        )\n",
        "\n",
        "    def _load_indictrans_model(self):\n",
        "        \"\"\"Load IndicTrans2 model.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "        )\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            self.model = self.model.cuda()\n",
        "\n",
        "    def _load_madlad_model(self):\n",
        "        \"\"\"Load MADLAD-400 model.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token\n",
        "        )\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "        )\n",
        "\n",
        "    def _load_hymt_model(self):\n",
        "        \"\"\"Load Tencent Hunyuan MT model.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def _load_translategemma_model(self):\n",
        "        \"\"\"Load Google TranslateGemma model (gated).\"\"\"\n",
        "        # Check for token\n",
        "        if not self.hf_token:\n",
        "            raise ValueError(\"TranslateGemma is a gated model. Please login to HuggingFace first (run the login cell) or provide HF Token.\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token\n",
        "        )\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def _load_causal_model(self):\n",
        "        \"\"\"Load causal LM for translation.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token\n",
        "        )\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            token=self.hf_token,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def _filter_inputs(self, inputs):\n",
        "        \"\"\"Filter out unsupported model kwargs like token_type_ids.\"\"\"\n",
        "        # Remove token_type_ids if present (many models don't use them)\n",
        "        if 'token_type_ids' in inputs:\n",
        "            del inputs['token_type_ids']\n",
        "        return inputs\n",
        "\n",
        "    def translate(self, text, tier=\"INTERMEDIATE\"):\n",
        "        \"\"\"Translate text based on model type.\"\"\"\n",
        "        if self.model_type in ['nllb', 'opus', 'mbart']:\n",
        "            return self._translate_seq2seq(text)\n",
        "        elif self.model_type == 'indictrans':\n",
        "            return self._translate_indictrans(text)\n",
        "        elif self.model_type == 'madlad':\n",
        "            return self._translate_madlad(text)\n",
        "        elif self.model_type == 'hymt':\n",
        "            return self._translate_hymt(text)\n",
        "        elif self.model_type == 'translategemma':\n",
        "            return self._translate_translategemma(text)\n",
        "        else:\n",
        "            return self._translate_causal(text, tier)\n",
        "\n",
        "    def _translate_seq2seq(self, text):\n",
        "        \"\"\"Translate using Seq2Seq model.\"\"\"\n",
        "        result = self.translator(text, max_length=1024)\n",
        "        return result[0]['translation_text']\n",
        "\n",
        "    def _translate_indictrans(self, text):\n",
        "        \"\"\"Translate using IndicTrans2.\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = self._filter_inputs(inputs)\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=512,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=1\n",
        "            )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def _translate_madlad(self, text):\n",
        "        \"\"\"Translate using MADLAD-400.\"\"\"\n",
        "        # MADLAD uses language tags like <2hi> for Hindi\n",
        "        lang_code = self.target_lang.split('_')[0][:2]  # Extract 2-letter code\n",
        "        tagged_text = f\"<2{lang_code}> {text}\"\n",
        "\n",
        "        inputs = self.tokenizer(tagged_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = self._filter_inputs(inputs)\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=512,\n",
        "                num_beams=4\n",
        "            )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def _translate_hymt(self, text):\n",
        "        \"\"\"Translate using Tencent Hunyuan MT.\"\"\"\n",
        "        lang_name = LANG_NAMES.get(self.target_lang, self.target_lang)\n",
        "\n",
        "        # Hunyuan MT uses a simple prompt format\n",
        "        prompt = f\"Translate the following text to {lang_name}:\\n{text}\\n\\nTranslation:\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
        "        inputs = self._filter_inputs(inputs)  # Remove token_type_ids\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract translation after the prompt\n",
        "        if 'Translation:' in generated:\n",
        "            translation = generated.split('Translation:')[-1].strip()\n",
        "        else:\n",
        "            translation = generated[len(prompt):].strip()\n",
        "\n",
        "        return translation\n",
        "\n",
        "    def _translate_translategemma(self, text):\n",
        "        \"\"\"Translate using Google TranslateGemma.\"\"\"\n",
        "        lang_name = LANG_NAMES.get(self.target_lang, self.target_lang)\n",
        "\n",
        "        # TranslateGemma instruction format\n",
        "        prompt = f\"Translate the following text from English to {lang_name}:\\n\\n{text}\\n\\nTranslation:\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
        "        inputs = self._filter_inputs(inputs)  # Remove token_type_ids\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract translation after the prompt\n",
        "        if 'Translation:' in generated:\n",
        "            translation = generated.split('Translation:')[-1].strip()\n",
        "        else:\n",
        "            translation = generated[len(prompt):].strip()\n",
        "\n",
        "        return translation\n",
        "\n",
        "    def _translate_causal(self, text, tier):\n",
        "        \"\"\"Translate using causal LM with prompts.\"\"\"\n",
        "        prompts = TRANSLATION_PROMPTS[tier]\n",
        "\n",
        "        # Get language name from code\n",
        "        lang_name = LANG_NAMES.get(self.target_lang, self.target_lang)\n",
        "\n",
        "        full_prompt = f\"{prompts['system']}\\n\\n{prompts['user'].format(target_lang=lang_name, chunk=text)}\"\n",
        "\n",
        "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
        "        inputs = self._filter_inputs(inputs)  # Remove token_type_ids\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.4,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract translation from response\n",
        "        if \"Translation:\" in generated:\n",
        "            return generated.split(\"Translation:\")[-1].strip()\n",
        "        return generated[len(full_prompt):].strip()\n",
        "\n",
        "\n",
        "class TranslationGenerator:\n",
        "    \"\"\"Main translation generator class.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, target_lang, device=\"cuda\", output_dir=\".\", tier=\"INTERMEDIATE\", chunk_size=350, hf_token=None):\n",
        "        self.model_name = model_name\n",
        "        self.target_lang = target_lang\n",
        "        self.device = device\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.tier = tier\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.engine = TranslationEngine(model_name, target_lang, device, hf_token)\n",
        "\n",
        "    def translate_file(self, input_file):\n",
        "        \"\"\"Translate entire file.\"\"\"\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"üåê TRANSLATION GENERATOR\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"üìÑ Input: {input_file}\")\n",
        "        print(f\"ü§ñ Model: {self.model_name}\")\n",
        "        print(f\"üåê Target: {self.target_lang}\")\n",
        "        print(f\"üéØ Quality: {self.tier}\")\n",
        "        print(f\"üñ•Ô∏è Device: {self.device}\")\n",
        "        print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "        # Read input\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Clean markers\n",
        "        lines = text.split('\\n')\n",
        "        cleaned = [l for l in lines if not (l.strip().startswith('===') and l.strip().endswith('==='))]\n",
        "        text = '\\n'.join(cleaned).strip()\n",
        "\n",
        "        orig_words = len(text.split())\n",
        "        orig_chars = len(text)\n",
        "        print(f\"üìä Input: {orig_chars:,} chars, {orig_words:,} words\")\n",
        "\n",
        "        # Chunk text\n",
        "        print(f\"\\nüì¶ Creating chunks ({self.chunk_size} words each)...\")\n",
        "        chunks = chunk_text(text, self.chunk_size)\n",
        "        print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "\n",
        "        # Translate chunks\n",
        "        print(f\"\\nüéØ STARTING TRANSLATION\\n\")\n",
        "\n",
        "        translations = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, chunk in enumerate(chunks, 1):\n",
        "            chunk_start = time.time()\n",
        "\n",
        "            print(f\"\\n{'=' * 50}\")\n",
        "            print(f\"üìÑ Chunk {i}/{len(chunks)}\")\n",
        "            print(f\"   Input: {len(chunk.split())} words, {len(chunk)} chars\")\n",
        "\n",
        "            try:\n",
        "                translated = self.engine.translate(chunk, self.tier)\n",
        "                translated = clean_translation(translated)\n",
        "                translations.append(translated)\n",
        "\n",
        "                chunk_time = time.time() - chunk_start\n",
        "                print(f\"   Output: {len(translated)} chars\")\n",
        "                print(f\"   ‚úÖ Completed in {chunk_time:.1f}s\")\n",
        "\n",
        "                # Progress\n",
        "                elapsed = time.time() - start_time\n",
        "                avg = elapsed / i\n",
        "                remaining = len(chunks) - i\n",
        "                eta = remaining * avg\n",
        "                print(f\"   üìà Progress: {i/len(chunks)*100:.1f}% | ETA: {eta/60:.1f}m\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error: {e}\")\n",
        "                translations.append(f\"[TRANSLATION ERROR: {e}]\")\n",
        "\n",
        "        # Combine translations\n",
        "        final_translation = \"\\n\\n\".join(translations)\n",
        "\n",
        "        # Save output\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        lang_code = self.target_lang.split('_')[0]\n",
        "        output_file = self.output_dir / f\"translation_{lang_code}_{timestamp}.txt\"\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_translation)\n",
        "\n",
        "        # Summary\n",
        "        total_time = time.time() - start_time\n",
        "        trans_chars = len(final_translation)\n",
        "\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"üéâ TRANSLATION COMPLETE!\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"‚è±Ô∏è Time: {total_time/60:.1f} minutes\")\n",
        "        print(f\"üì¶ Chunks: {len(chunks)}\")\n",
        "        print(f\"‚ö° Avg/chunk: {total_time/len(chunks):.1f}s\")\n",
        "        print(f\"üìù Input: {orig_chars:,} chars\")\n",
        "        print(f\"üìù Output: {trans_chars:,} chars\")\n",
        "        print(f\"üìä Ratio: {trans_chars/orig_chars:.2f}x\")\n",
        "        print(f\"üíæ Output: {output_file}\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "\n",
        "        return str(output_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============= OLLAMA TRANSLATION ENGINE =============\n",
        "\n",
        "class OllamaTranslationEngine:\n",
        "    \"\"\"Translation engine using Ollama local models.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, target_lang, tier=\"INTERMEDIATE\"):\n",
        "        self.model_name = model_name\n",
        "        self.target_lang = target_lang\n",
        "        self.tier = tier\n",
        "        self.lang_name = LANG_NAMES.get(target_lang, target_lang)\n",
        "\n",
        "        print(f\"üì• Initializing Ollama engine with model: {model_name}\")\n",
        "        print(f\"   Target language: {self.lang_name}\")\n",
        "\n",
        "        # Verify model is available\n",
        "        try:\n",
        "            import ollama\n",
        "            self.client = ollama\n",
        "            models = ollama.list()\n",
        "            available = [m.get('name', '').split(':')[0] for m in models.get('models', [])]\n",
        "            model_base = model_name.split(':')[0]\n",
        "\n",
        "            if not any(model_base in m for m in available):\n",
        "                print(f\"‚ö†Ô∏è Model '{model_name}' not found locally. Attempting to pull...\")\n",
        "                ollama.pull(model_name)\n",
        "                print(f\"‚úÖ Model '{model_name}' pulled successfully!\")\n",
        "            else:\n",
        "                print(f\"‚úÖ Model '{model_name}' is available!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error initializing Ollama: {e}\")\n",
        "            raise\n",
        "\n",
        "    def translate(self, text, tier=None):\n",
        "        \"\"\"Translate text using Ollama model.\"\"\"\n",
        "        if tier:\n",
        "            self.tier = tier\n",
        "\n",
        "        prompts = TRANSLATION_PROMPTS[self.tier]\n",
        "\n",
        "        system_prompt = prompts['system']\n",
        "        user_prompt = prompts['user'].format(target_lang=self.lang_name, chunk=text)\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat(\n",
        "                model=self.model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                options={\n",
        "                    \"temperature\": 0.3,\n",
        "                    \"num_predict\": 2048,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            translation = response['message']['content']\n",
        "            return clean_translation(translation)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Ollama translation error: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "class OllamaTranslationGenerator:\n",
        "    \"\"\"Translation generator using Ollama models.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, target_lang, output_dir=\".\", tier=\"INTERMEDIATE\", chunk_size=350):\n",
        "        self.model_name = model_name\n",
        "        self.target_lang = target_lang\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.tier = tier\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.engine = OllamaTranslationEngine(model_name, target_lang, tier)\n",
        "\n",
        "    def translate_file(self, input_file):\n",
        "        \"\"\"Translate entire file using Ollama.\"\"\"\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"üåê OLLAMA TRANSLATION GENERATOR\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"üìÑ Input: {input_file}\")\n",
        "        print(f\"ü¶ô Model: {self.model_name}\")\n",
        "        print(f\"üåê Target: {self.target_lang}\")\n",
        "        print(f\"üéØ Quality: {self.tier}\")\n",
        "        print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "        # Read input\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Clean markers\n",
        "        lines = text.split('\\n')\n",
        "        cleaned = [l for l in lines if not (l.strip().startswith('===') and l.strip().endswith('==='))]\n",
        "        text = '\\n'.join(cleaned).strip()\n",
        "\n",
        "        orig_words = len(text.split())\n",
        "        orig_chars = len(text)\n",
        "        print(f\"üìä Input: {orig_chars:,} chars, {orig_words:,} words\")\n",
        "\n",
        "        # Chunk text\n",
        "        print(f\"\\nüì¶ Creating chunks ({self.chunk_size} words each)...\")\n",
        "        chunks = chunk_text(text, self.chunk_size)\n",
        "        print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "\n",
        "        # Translate chunks\n",
        "        print(f\"\\nüéØ STARTING TRANSLATION\\n\")\n",
        "\n",
        "        translations = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, chunk in enumerate(chunks, 1):\n",
        "            chunk_start = time.time()\n",
        "\n",
        "            print(f\"\\n{'=' * 50}\")\n",
        "            print(f\"üìÑ Chunk {i}/{len(chunks)}\")\n",
        "            print(f\"   Input: {len(chunk.split())} words, {len(chunk)} chars\")\n",
        "\n",
        "            try:\n",
        "                translated = self.engine.translate(chunk, self.tier)\n",
        "                translations.append(translated)\n",
        "\n",
        "                chunk_time = time.time() - chunk_start\n",
        "                print(f\"   Output: {len(translated)} chars\")\n",
        "                print(f\"   ‚úÖ Completed in {chunk_time:.1f}s\")\n",
        "\n",
        "                # Progress\n",
        "                elapsed = time.time() - start_time\n",
        "                avg = elapsed / i\n",
        "                remaining = len(chunks) - i\n",
        "                eta = remaining * avg\n",
        "                print(f\"   üìà Progress: {i/len(chunks)*100:.1f}% | ETA: {eta/60:.1f}m\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error: {e}\")\n",
        "                translations.append(f\"[TRANSLATION ERROR: {e}]\")\n",
        "\n",
        "        # Combine translations\n",
        "        final_translation = \"\\n\\n\".join(translations)\n",
        "\n",
        "        # Save output\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        lang_code = self.target_lang.split('_')[0]\n",
        "        output_file = self.output_dir / f\"translation_{lang_code}_{timestamp}.txt\"\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_translation)\n",
        "\n",
        "        # Summary\n",
        "        total_time = time.time() - start_time\n",
        "        trans_chars = len(final_translation)\n",
        "\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"üéâ TRANSLATION COMPLETE!\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"‚è±Ô∏è Time: {total_time/60:.1f} minutes\")\n",
        "        print(f\"üì¶ Chunks: {len(chunks)}\")\n",
        "        print(f\"‚ö° Avg/chunk: {total_time/len(chunks):.1f}s\")\n",
        "        print(f\"üìù Input: {orig_chars:,} chars\")\n",
        "        print(f\"üìù Output: {trans_chars:,} chars\")\n",
        "        print(f\"üìä Ratio: {trans_chars/orig_chars:.2f}x\")\n",
        "        print(f\"üíæ Output: {output_file}\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "\n",
        "        return str(output_file)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Translation Engine loaded and ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXiB0swZo_UK"
      },
      "source": [
        "## üåê Step 5: Generate Translation\n",
        "Run this cell to translate your uploaded file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iBw0e4To_UK",
        "outputId": "d92951b2-0838-45b0-a3cb-be28026eb3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initializing Translation Generator...\n",
            "   Provider: ollama\n",
            "   Model: translategemma:27b\n",
            "üì• Initializing Ollama engine with model: translategemma:27b\n",
            "   Target language: Hindi\n",
            "‚ö†Ô∏è Model 'translategemma:27b' not found locally. Attempting to pull...\n",
            "‚úÖ Model 'translategemma:27b' pulled successfully!\n",
            "\n",
            "üåê Starting translation...\n",
            "\n",
            "======================================================================\n",
            "üåê OLLAMA TRANSLATION GENERATOR\n",
            "======================================================================\n",
            "üìÑ Input: CHAPTER 1 (1).txt\n",
            "ü¶ô Model: translategemma:27b\n",
            "üåê Target: hin_Deva\n",
            "üéØ Quality: ADVANCED\n",
            "======================================================================\n",
            "\n",
            "üìä Input: 17,181 chars, 2,949 words\n",
            "\n",
            "üì¶ Creating chunks (350 words each)...\n",
            "‚úÖ Created 9 chunks\n",
            "\n",
            "üéØ STARTING TRANSLATION\n",
            "\n",
            "\n",
            "==================================================\n",
            "üìÑ Chunk 1/9\n",
            "   Input: 350 words, 2067 chars\n",
            "   Output: 2009 chars\n",
            "   ‚úÖ Completed in 285.9s\n",
            "   üìà Progress: 11.1% | ETA: 38.1m\n",
            "\n",
            "==================================================\n",
            "üìÑ Chunk 2/9\n",
            "   Input: 350 words, 1988 chars\n",
            "   Output: 1958 chars\n",
            "   ‚úÖ Completed in 219.6s\n",
            "   üìà Progress: 22.2% | ETA: 29.5m\n",
            "\n",
            "==================================================\n",
            "üìÑ Chunk 3/9\n",
            "   Input: 350 words, 2023 chars\n",
            "   Output: 1982 chars\n",
            "   ‚úÖ Completed in 226.7s\n",
            "   üìà Progress: 33.3% | ETA: 24.4m\n",
            "\n",
            "==================================================\n",
            "üìÑ Chunk 4/9\n",
            "   Input: 350 words, 1984 chars\n",
            "   Output: 1886 chars\n",
            "   ‚úÖ Completed in 211.9s\n",
            "   üìà Progress: 44.4% | ETA: 19.7m\n",
            "\n",
            "==================================================\n",
            "üìÑ Chunk 5/9\n",
            "   Input: 350 words, 1978 chars\n",
            "   Output: 1951 chars\n",
            "   ‚úÖ Completed in 212.2s\n",
            "   üìà Progress: 55.6% | ETA: 15.4m\n",
            "\n",
            "==================================================\n",
            "üìÑ Chunk 6/9\n",
            "   Input: 350 words, 2048 chars\n",
            "   Output: 1985 chars\n",
            "   ‚úÖ Completed in 213.9s\n",
            "   üìà Progress: 66.7% | ETA: 11.4m\n",
            "\n",
            "==================================================\n",
            "üìÑ Chunk 7/9\n",
            "   Input: 350 words, 2050 chars\n",
            "   Output: 1862 chars\n",
            "   ‚úÖ Completed in 218.0s\n",
            "   üìà Progress: 77.8% | ETA: 7.6m\n",
            "\n",
            "==================================================\n",
            "üìÑ Chunk 8/9\n",
            "   Input: 350 words, 2115 chars\n",
            "   Output: 2035 chars\n",
            "   ‚úÖ Completed in 232.4s\n",
            "   üìà Progress: 88.9% | ETA: 3.8m\n",
            "\n",
            "==================================================\n",
            "üìÑ Chunk 9/9\n",
            "   Input: 149 words, 892 chars\n",
            "   Output: 844 chars\n",
            "   ‚úÖ Completed in 95.8s\n",
            "   üìà Progress: 100.0% | ETA: 0.0m\n",
            "\n",
            "======================================================================\n",
            "üéâ TRANSLATION COMPLETE!\n",
            "======================================================================\n",
            "‚è±Ô∏è Time: 31.9 minutes\n",
            "üì¶ Chunks: 9\n",
            "‚ö° Avg/chunk: 212.9s\n",
            "üìù Input: 17,181 chars\n",
            "üìù Output: 16,528 chars\n",
            "üìä Ratio: 0.96x\n",
            "üíæ Output: translation_output/translation_hin_20260129_041924.txt\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Translation file generated: translation_output/translation_hin_20260129_041924.txt\n"
          ]
        }
      ],
      "source": [
        "# Create output directory\n",
        "OUTPUT_DIR = \"./translation_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Initialize the generator based on provider\n",
        "print(\"üöÄ Initializing Translation Generator...\")\n",
        "print(f\"   Provider: {SELECTED_PROVIDER}\")\n",
        "print(f\"   Model: {SELECTED_MODEL}\")\n",
        "\n",
        "if SELECTED_PROVIDER == \"ollama\":\n",
        "    # Use Ollama generator\n",
        "    generator = OllamaTranslationGenerator(\n",
        "        model_name=SELECTED_MODEL,\n",
        "        target_lang=TARGET_LANGUAGE,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        tier=TRANSLATION_TIER,\n",
        "        chunk_size=CHUNK_SIZE\n",
        "    )\n",
        "else:\n",
        "    # Use HuggingFace generator\n",
        "    generator = TranslationGenerator(\n",
        "        model_name=SELECTED_MODEL,\n",
        "        target_lang=TARGET_LANGUAGE,\n",
        "        device=DEVICE,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        tier=TRANSLATION_TIER,\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        hf_token=HF_TOKEN\n",
        "    )\n",
        "\n",
        "# Translate\n",
        "print(f\"\\nüåê Starting translation...\")\n",
        "OUTPUT_FILE = generator.translate_file(UPLOADED_FILE)\n",
        "\n",
        "print(f\"\\n‚úÖ Translation file generated: {OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTnmIfao_UL"
      },
      "source": [
        "## üìñ Step 6: Preview & Download Translation\n",
        "View your translation and download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8j2bpi5o_UL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "import os\n",
        "\n",
        "if os.path.exists(OUTPUT_FILE):\n",
        "    # Read and display translation\n",
        "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "        translation = f.read()\n",
        "\n",
        "    file_size = os.path.getsize(OUTPUT_FILE) / 1024  # KB\n",
        "    word_count = len(translation.split())\n",
        "\n",
        "    print(f\"üìä Translation stats:\")\n",
        "    print(f\"   Words: {word_count:,}\")\n",
        "    print(f\"   Characters: {len(translation):,}\")\n",
        "    print(f\"   File size: {file_size:.2f} KB\")\n",
        "\n",
        "    print(f\"\\nüìñ Preview (first 1000 chars):\")\n",
        "    print(f\"{'=' * 50}\")\n",
        "    print(translation[:1000])\n",
        "    print(f\"{'=' * 50}\")\n",
        "    if len(translation) > 1000:\n",
        "        print(f\"... [truncated, {len(translation) - 1000:,} more chars]\")\n",
        "else:\n",
        "    print(\"‚ùå Output file not found. Please run the translation step again.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "V3lunR7Bo_UL",
        "outputId": "e33b6761-9951-498d-e93c-2b36c4721c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading your translated file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75ce3b2c-eb04-4c20-9c57-44f1be343538\", \"translation_hin_20260129_041924.txt\", 42340)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Download started! Check your browser's download folder.\n"
          ]
        }
      ],
      "source": [
        "# Download the translated file\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üì• Downloading your translated file...\")\n",
        "files.download(OUTPUT_FILE)\n",
        "print(\"‚úÖ Download started! Check your browser's download folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN_2qXYWo_UL"
      },
      "source": [
        "## üíæ (Optional) Save to Google Drive\n",
        "If you want to save the translation to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og4yi8Sqo_UM"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "print(\"üìÇ Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output folder in Drive\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/Translation_Output\"\n",
        "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Copy file to Drive\n",
        "drive_output_path = os.path.join(DRIVE_OUTPUT_DIR, os.path.basename(OUTPUT_FILE))\n",
        "shutil.copy(OUTPUT_FILE, drive_output_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Translation saved to Google Drive:\")\n",
        "print(f\"   üìÅ {drive_output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_1kMeBVo_UM"
      },
      "source": [
        "---\n",
        "\n",
        "## üìö Quick Reference\n",
        "\n",
        "### Supported Languages (NLLB Codes):\n",
        "| Language | Code |\n",
        "|----------|------|\n",
        "| Hindi | `hin_Deva` |\n",
        "| Bengali | `ben_Beng` |\n",
        "| Tamil | `tam_Taml` |\n",
        "| Telugu | `tel_Telu` |\n",
        "| Marathi | `mar_Deva` |\n",
        "| Gujarati | `guj_Gujr` |\n",
        "| Spanish | `spa_Latn` |\n",
        "| French | `fra_Latn` |\n",
        "| German | `deu_Latn` |\n",
        "\n",
        "### Recommended Models:\n",
        "| Model | Best For | Speed |\n",
        "|-------|----------|-------|\n",
        "| `facebook/nllb-200-distilled-600M` | Fast multilingual | ‚ö° Fast |\n",
        "| `facebook/nllb-200-1.3B` | Better quality | üîÑ Medium |\n",
        "| `ai4bharat/indictrans2-en-indic-1B` | Best EN‚ÜíHindi | üîÑ Medium |\n",
        "| `google/madlad400-3b-mt` | Highest quality | üê¢ Slow |\n",
        "\n",
        "### Quality Tiers:\n",
        "- **BASIC**: Fast, good for simple texts\n",
        "- **INTERMEDIATE**: Balanced quality and speed (recommended)\n",
        "- **ADVANCED**: Best quality, preserves all nuances\n",
        "\n",
        "### Tips:\n",
        "- Use Colab GPU for faster translation\n",
        "- For long texts, use smaller chunk sizes (200-300 words)\n",
        "- NLLB models are best for multilingual translation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "304220edff9f4b83ac02b6b715d2eb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "qwen2.5:3b (Fast, ~2GB)",
              "qwen2.5:7b (Balanced, ~4GB)",
              "deepseek-r1:7b (Reasoning, ~4GB)",
              "llama3.2:3b (Fast, ~2GB)",
              "mistral:7b (Quality, ~4GB)",
              "gemma2:2b (Compact, ~1.5GB)",
              "phi3:mini (Compact, ~2GB)"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model to Pull:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_cb3161a39d0e4436bc1369d910221dbd",
            "style": "IPY_MODEL_6ff0d5cf33c34fc4ba9300a93f4f826c"
          }
        },
        "cb3161a39d0e4436bc1369d910221dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "400px"
          }
        },
        "6ff0d5cf33c34fc4ba9300a93f4f826c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "abda3bc4bd824f538dc5a1830b5cfd94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Custom Model:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_adddb530d6f64bc2a070cf0c2d7d140a",
            "placeholder": "Or enter custom model name (e.g., llama3:8b)",
            "style": "IPY_MODEL_02dd83c99a86459bb2e14fbc65c78677",
            "value": "translategemma:27b"
          }
        },
        "adddb530d6f64bc2a070cf0c2d7d140a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "400px"
          }
        },
        "02dd83c99a86459bb2e14fbc65c78677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "63a4fdc27166469e93eca220438fc390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "HuggingFace (Recommended for Colab)",
              "Ollama (Local only)"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Provider:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_bcede039456e467e897f2b36accaee83",
            "style": "IPY_MODEL_c9fff22fee8e4aa0a0c58799965fe57a"
          }
        },
        "bcede039456e467e897f2b36accaee83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "400px"
          }
        },
        "c9fff22fee8e4aa0a0c58799965fe57a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "33ee3d6ab257469483537d9484b8d5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "qwen2.5:3b (Fast)",
              "qwen2.5:7b (Balanced)",
              "deepseek-r1:7b (Reasoning)",
              "llama3.2:3b (Fast)",
              "Custom Model (enter below)"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 4,
            "layout": "IPY_MODEL_a3d0cd9391394c7f8e72db7681cc6f76",
            "style": "IPY_MODEL_606c76b495af4577bbff68aeecb2c830"
          }
        },
        "a3d0cd9391394c7f8e72db7681cc6f76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        },
        "606c76b495af4577bbff68aeecb2c830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "c132b8b4b6804dd89cd3779d4205ace4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Custom Model:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3d78679d496d451292148d6614aaee05",
            "placeholder": "Enter Ollama model name",
            "style": "IPY_MODEL_f051987cf1ef4260824fd4b75780c91f",
            "value": "translategemma:27b"
          }
        },
        "3d78679d496d451292148d6614aaee05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        },
        "f051987cf1ef4260824fd4b75780c91f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "62e92d90e80044c89b13e498e5f503a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)",
              "Bengali (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)",
              "Tamil (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)",
              "Telugu (‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å)",
              "Marathi (‡§Æ‡§∞‡§æ‡§†‡•Ä)",
              "Gujarati (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä)",
              "Kannada (‡≤ï‡≤®‡≥ç‡≤®‡≤°)",
              "Malayalam (‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç)",
              "Punjabi (‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä)",
              "Odia (‡¨ì‡¨°‡¨º‡¨ø‡¨Ü)",
              "Urdu (ÿßÿ±ÿØŸà)",
              "Spanish (Espa√±ol)",
              "French (Fran√ßais)",
              "German (Deutsch)",
              "Chinese (‰∏≠Êñá)",
              "Japanese (Êó•Êú¨Ë™û)",
              "Custom (enter code below)"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Target Language:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_8e019dcd108c4ff2963b889d36fc8427",
            "style": "IPY_MODEL_79ca78e919884694b417e036c369c8cf"
          }
        },
        "8e019dcd108c4ff2963b889d36fc8427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "400px"
          }
        },
        "79ca78e919884694b417e036c369c8cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "9fb9a05b8edb4fc7b2ac2f104731c9ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Custom Lang:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_34e8ba6ada154738a1405cb7c4976289",
            "placeholder": "Enter NLLB language code (e.g., hin_Deva)",
            "style": "IPY_MODEL_aa821a52115847fb8bf04416563333aa",
            "value": ""
          }
        },
        "34e8ba6ada154738a1405cb7c4976289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "400px"
          }
        },
        "aa821a52115847fb8bf04416563333aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "a8fb6abf483f47229c0d0747f88251ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "BASIC - Fast, good quality",
              "INTERMEDIATE - Balanced (recommended)",
              "ADVANCED - Best quality, slower"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Quality:",
            "description_tooltip": null,
            "disabled": false,
            "index": 2,
            "layout": "IPY_MODEL_e24bb27cc39544a9b5f672ff7872f258",
            "style": "IPY_MODEL_aab930ac1a1043f69fbbd306cb818666"
          }
        },
        "e24bb27cc39544a9b5f672ff7872f258": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "400px"
          }
        },
        "aab930ac1a1043f69fbbd306cb818666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "44d51432f18241afb41d07f73df76c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "Chunk Size:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_d270b543e09a40989ce4fad17ce9f12e",
            "max": 1000,
            "min": 100,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 50,
            "style": "IPY_MODEL_b54ede47a75b49cf9024ad76c9fe4905",
            "value": 350
          }
        },
        "d270b543e09a40989ce4fad17ce9f12e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "400px"
          }
        },
        "b54ede47a75b49cf9024ad76c9fe4905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial",
            "handle_color": null
          }
        },
        "990007db384e4eb29db1a8866016bb92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "HF Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_47e690f6e8e44ad6b1196346d8ec8e01",
            "placeholder": "Optional: HF token for gated models",
            "style": "IPY_MODEL_36870561a94241c8bd637e4e0b0492db",
            "value": ""
          }
        },
        "47e690f6e8e44ad6b1196346d8ec8e01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "400px"
          }
        },
        "36870561a94241c8bd637e4e0b0492db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
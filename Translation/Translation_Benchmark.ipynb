{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Translation Benchmark Notebook\n",
        "**Test translation speed & performance across models, parameters, and languages**\n\n",
        "Upload books, configure models, translate, and download results.\n",
        "Helps you decide if Colab Pro+ is worth it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 1Ô∏è‚É£ Setup & Installation\n",
        "#@markdown Run this cell first\n\n",
        "!pip install -q transformers torch accelerate sentencepiece protobuf PyPDF2 ebooklib beautifulsoup4\n\n",
        "from google.colab import files as colab_files\n",
        "import os, time, json, re\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n\n",
        "OUTPUT_DIR = '/content/translations'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print('‚úÖ Setup complete!')\n\n",
        "BENCHMARK_LOG = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 2Ô∏è‚É£ Model Provider Selection\n",
        "#@markdown Choose provider (local models are FREE)\n\n",
        "import torch\n\n",
        "provider = 'huggingface_local' #@param ['huggingface_local', 'groq_free', 'google_free']\n\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f'üéÆ GPU: {gpu_name} ({gpu_mem:.1f} GB)')\n",
        "else:\n",
        "    print('‚ö†Ô∏è No GPU - using CPU')\n\n",
        "api_key = '' #@param {type:'string'}\n",
        "if provider in ['groq_free', 'google_free'] and not api_key:\n",
        "    print('‚ö†Ô∏è API key needed for cloud providers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 3Ô∏è‚É£ Model Configuration\n\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-hi' #@param {type:'string'}\n",
        "#@markdown Suggested: `Helsinki-NLP/opus-mt-en-hi`, `facebook/nllb-200-distilled-600M`, `google/flan-t5-base`\n\n",
        "temperature = 0.3 #@param {type:'slider', min:0, max:1, step:0.1}\n",
        "top_p = 0.9 #@param {type:'slider', min:0, max:1, step:0.1}\n",
        "max_tokens = 4096 #@param {type:'slider', min:512, max:8192, step:512}\n\n",
        "print(f'üì¶ Model: {model_name}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 4Ô∏è‚É£ Translation Settings\n\n",
        "source_language = 'English' #@param {type:'string'}\n",
        "target_language = 'Hindi' #@param {type:'string'}\n",
        "#@markdown Enter any language your model supports\n\n",
        "tier = 'BASIC' #@param ['BASIC', 'INTERMEDIATE', 'ADVANCED']\n",
        "chunk_words = 350 #@param {type:'slider', min:100, max:600, step:50}\n\n",
        "print(f'üåç {source_language} ‚Üí {target_language}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 5Ô∏è‚É£ Chunk Overlapping\n",
        "#@markdown Enable for better context between chunks\n\n",
        "enable_overlap = True #@param {type:'boolean'}\n",
        "overlap_words = 50 #@param {type:'slider', min:10, max:150, step:10}\n\n",
        "if enable_overlap:\n",
        "    print(f'‚úÖ Overlap: {overlap_words} words')\n",
        "else:\n",
        "    print('‚ö†Ô∏è Overlapping disabled')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 6Ô∏è‚É£ Upload Book\n\n",
        "import PyPDF2\n",
        "from io import BytesIO\n\n",
        "print('üì§ Upload your book...')\n",
        "uploaded = colab_files.upload()\n\n",
        "book_name = list(uploaded.keys())[0]\n",
        "book_content = uploaded[book_name]\n\n",
        "if book_name.endswith('.pdf'):\n",
        "    pdf = PyPDF2.PdfReader(BytesIO(book_content))\n",
        "    text = '\\n'.join([p.extract_text() or '' for p in pdf.pages])\n",
        "elif book_name.endswith('.epub'):\n",
        "    import ebooklib\n",
        "    from ebooklib import epub\n",
        "    from bs4 import BeautifulSoup\n",
        "    book = epub.read_epub(BytesIO(book_content))\n",
        "    text = ''.join([BeautifulSoup(i.get_content(), 'html.parser').get_text() for i in book.get_items_of_type(ebooklib.ITEM_DOCUMENT)])\n",
        "else:\n",
        "    text = book_content.decode('utf-8')\n\n",
        "word_count = len(text.split())\n",
        "print(f'üìñ {book_name}: {word_count:,} words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 7Ô∏è‚É£ Load Model\n\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
        "import requests\n\n",
        "translator = None\n\n",
        "if provider == 'huggingface_local':\n",
        "    print(f'üì• Loading {model_name}...')\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if 't5' in model_name.lower() or 'mt' in model_name.lower() or 'nllb' in model_name.lower():\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\n",
        "            translator = pipeline('translation', model=model, tokenizer=tokenizer, device_map='auto')\n",
        "        else:\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\n",
        "            translator = pipeline('text-generation', model=model, tokenizer=tokenizer, device_map='auto', max_new_tokens=max_tokens)\n",
        "        print('‚úÖ Model loaded!')\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Error: {e}')\n",
        "else:\n",
        "    print('‚úÖ API ready')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 8Ô∏è‚É£ Run Translation\n\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n\n",
        "def chunk_text(text, chunk_words, overlap_words, enable_overlap):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    step = chunk_words - overlap_words if enable_overlap else chunk_words\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunks.append(' '.join(words[i:i+chunk_words]))\n",
        "        i += step\n",
        "    return chunks\n\n",
        "def api_translate(chunk, provider, model_name, api_key, target_lang):\n",
        "    if provider == 'groq_free':\n",
        "        r = requests.post('https://api.groq.com/openai/v1/chat/completions',\n",
        "            headers={'Authorization': f'Bearer {api_key}'},\n",
        "            json={'model': model_name, 'messages': [{'role': 'user', 'content': f'Translate to {target_lang}: {chunk}'}]})\n",
        "        return r.json()['choices'][0]['message']['content']\n",
        "    elif provider == 'google_free':\n",
        "        r = requests.post(f'https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}',\n",
        "            json={'contents': [{'parts': [{'text': f'Translate to {target_lang}: {chunk}'}]}]})\n",
        "        return r.json()['candidates'][0]['content']['parts'][0]['text']\n",
        "    return chunk\n\n",
        "chunks = chunk_text(text, chunk_words, overlap_words, enable_overlap)\n",
        "print(f'üì¶ {len(chunks)} chunks')\n\n",
        "translations = []\n",
        "chunk_times = []\n",
        "start_time = time.time()\n\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    t0 = time.time()\n",
        "    print(f'\\r‚è≥ {i}/{len(chunks)}', end='')\n",
        "    try:\n",
        "        if provider == 'huggingface_local' and translator:\n",
        "            if 't5' in model_name.lower() or 'mt' in model_name.lower() or 'nllb' in model_name.lower():\n",
        "                result = translator(chunk, max_length=max_tokens)[0]['translation_text']\n",
        "            else:\n",
        "                result = translator(f'Translate to {target_language}: {chunk}')[0]['generated_text']\n",
        "        else:\n",
        "            result = api_translate(chunk, provider, model_name, api_key, target_language)\n",
        "        translations.append(result)\n",
        "    except Exception as e:\n",
        "        translations.append(f'[ERROR: {e}]')\n",
        "    chunk_times.append(time.time() - t0)\n\n",
        "total_time = time.time() - start_time\n",
        "print(f'\\n‚úÖ Done in {total_time/60:.1f} min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 9Ô∏è‚É£ Performance & Colab Pro+ Analysis\n\n",
        "avg_time = sum(chunk_times) / len(chunk_times)\n",
        "chars_sec = len(text) / total_time\n\n",
        "gpu_mem = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n\n",
        "print('='*50)\n",
        "print('üìä METRICS')\n",
        "print('='*50)\n",
        "print(f'‚è±Ô∏è Total: {total_time/60:.1f} min')\n",
        "print(f'üì¶ Avg chunk: {avg_time:.1f}s')\n",
        "print(f'‚ö° Speed: {chars_sec:.0f} chars/sec')\n",
        "print(f'üéÆ GPU: {gpu_mem:.1f} GB')\n\n",
        "print('\\n' + '='*50)\n",
        "print('üíé COLAB PRO+ COMPARISON')\n",
        "print('='*50)\n\n",
        "gpu_data = {\n",
        "    'L4 GPU': {'x': 2.5, 'vram': '24GB', 'models': 'Up to 13B'},\n",
        "    'A100 GPU': {'x': 5, 'vram': '40GB', 'models': 'Up to 30B'},\n",
        "    'H100 GPU': {'x': 8, 'vram': '80GB', 'models': 'Up to 70B'},\n",
        "    'v6e TPU': {'x': 4, 'vram': '16GB HBM', 'models': 'Transformers optimized'}\n",
        "}\n\n",
        "for gpu, d in gpu_data.items():\n",
        "    print(f\"\\n{gpu}: {total_time/d['x']/60:.1f} min ({d['x']}x faster)\")\n",
        "    print(f\"  üíæ {d['vram']} | ü§ñ {d['models']}\")\n\n",
        "print('\\nüí° ', end='')\n",
        "if total_time > 600:\n",
        "    print('Pro+ recommended for time savings')\n",
        "elif gpu_mem > 12:\n",
        "    print('Pro+ recommended for larger models')\n",
        "else:\n",
        "    print('Free tier works for this workload')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title üîü Download or Copy Translation\n\n",
        "from IPython.display import display, HTML\n\n",
        "# Generate filename\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "book_base = Path(book_name).stem\n",
        "model_short = model_name.split('/')[-1]\n",
        "filename = f'{book_base}_{model_short}_{target_language}_{tier}_{timestamp}'\n\n",
        "full_translation = '\\n\\n'.join(translations)\n\n",
        "# Save locally\n",
        "output_path = f'{OUTPUT_DIR}/{filename}.txt'\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(full_translation)\n\n",
        "# Track benchmark\n",
        "BENCHMARK_LOG.append({'book': book_name, 'model': model_name, 'target_lang': target_language,\n",
        "                      'tier': tier, 'total_time_sec': total_time, 'chars_per_sec': chars_sec})\n\n",
        "print('='*50)\n",
        "print('üì• DOWNLOAD OPTIONS')\n",
        "print('='*50)\n",
        "print(f'üìÑ {filename}.txt ({len(full_translation):,} chars)\\n')\n\n",
        "# Option 1: Download\n",
        "print('üîΩ Option 1: Download file')\n",
        "colab_files.download(output_path)\n\n",
        "# Option 2: Copy button\n",
        "escaped = full_translation.replace('\\\\', '\\\\\\\\').replace('`', '\\\\`').replace('\"', '\\\\\"').replace('\\n', '\\\\n')\n",
        "html = f'''<div style=\"margin:15px 0\">\n",
        "<button onclick=\"navigator.clipboard.writeText(`{escaped}`).then(()=>{{this.innerHTML='‚úÖ Copied!';setTimeout(()=>this.innerHTML='üìã Copy All Text',2000)}})\" \n",
        "style=\"padding:12px 24px;background:linear-gradient(135deg,#667eea,#764ba2);color:white;border:none;border-radius:8px;cursor:pointer;font-size:14px;font-weight:bold\">\n",
        "üìã Copy All Text</button></div>'''\n",
        "print('\\nüìã Option 2: Copy to clipboard')\n",
        "display(HTML(html))\n\n",
        "# Preview\n",
        "print('\\nüëÄ Preview (first 500 chars):\\n')\n",
        "print(full_translation[:500] + '...' if len(full_translation) > 500 else full_translation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title üìä Comparison Dashboard\n\n",
        "import pandas as pd\n\n",
        "if BENCHMARK_LOG:\n",
        "    df = pd.DataFrame(BENCHMARK_LOG)\n",
        "    df['time_min'] = df['total_time_sec'] / 60\n",
        "    display(df[['book', 'model', 'target_lang', 'tier', 'time_min', 'chars_per_sec']].style.background_gradient(cmap='RdYlGn_r'))\n",
        "else:\n",
        "    print('No runs yet')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåê Hindi Literary Translation Generator for Google Colab\n",
                "\n",
                "**Enhanced Multilingual Translation with Real-time Streaming**\n",
                "\n",
                "This notebook allows you to:\n",
                "1. Upload your text file to translate\n",
                "2. Select AI provider (Ollama/HuggingFace) and model\n",
                "3. Choose target language and translation quality tier\n",
                "4. Generate translation and download the result\n",
                "\n",
                "**Supported Providers:**\n",
                "- ü§ó HuggingFace - Various translation models\n",
                "- ü¶ô Ollama - Local models (if running locally)\n",
                "\n",
                "**Recommended Models for Hindi:**\n",
                "- `facebook/nllb-200-distilled-600M` - Fast, multilingual\n",
                "- `ai4bharat/indictrans2-en-indic-1B` - Best for English‚ÜíHindi\n",
                "- `google/madlad400-3b-mt` - High quality, slower"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Step 1: Install Dependencies\n",
                "Run this cell to install all required packages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q torch transformers accelerate sentencepiece\n",
                "!pip install -q colorama huggingface-hub\n",
                "print(\"‚úÖ All dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì§ Step 2: Upload Your Text File\n",
                "Upload the text file you want to translate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "print(\"üì§ Please upload your text file to translate:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Get the uploaded file name\n",
                "UPLOADED_FILE = list(uploaded.keys())[0]\n",
                "print(f\"\\n‚úÖ Uploaded: {UPLOADED_FILE}\")\n",
                "print(f\"üìÑ File size: {len(uploaded[UPLOADED_FILE])} bytes\")\n",
                "\n",
                "# Display preview\n",
                "with open(UPLOADED_FILE, 'r', encoding='utf-8') as f:\n",
                "    content = f.read()\n",
                "    word_count = len(content.split())\n",
                "    char_count = len(content)\n",
                "\n",
                "print(f\"\\nüìä Content stats:\")\n",
                "print(f\"   Words: {word_count:,}\")\n",
                "print(f\"   Characters: {char_count:,}\")\n",
                "print(f\"\\nüìù Preview (first 500 chars):\\n{content[:500]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Step 3: Select AI Provider, Model & Language\n",
                "Choose your preferred translation model and settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ipywidgets as widgets\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "# Provider options\n",
                "PROVIDER_OPTIONS = {\n",
                "    \"HuggingFace (Recommended for Colab)\": \"huggingface\",\n",
                "    \"Ollama (Local only)\": \"ollama\"\n",
                "}\n",
                "\n",
                "# Model options by provider\n",
                "HF_MODEL_OPTIONS = {\n",
                "    \"facebook/nllb-200-distilled-600M (Fast, Multilingual)\": \"facebook/nllb-200-distilled-600M\",\n",
                "    \"facebook/nllb-200-1.3B (Better Quality)\": \"facebook/nllb-200-1.3B\",\n",
                "    \"ai4bharat/indictrans2-en-indic-1B (Best English‚ÜíHindi)\": \"ai4bharat/indictrans2-en-indic-1B\",\n",
                "    \"google/madlad400-3b-mt (High Quality, Slow)\": \"google/madlad400-3b-mt\",\n",
                "    \"Helsinki-NLP/opus-mt-en-hi (Simple EN‚ÜíHI)\": \"Helsinki-NLP/opus-mt-en-hi\",\n",
                "    \"Custom Model (enter below)\": \"custom\"\n",
                "}\n",
                "\n",
                "OLLAMA_MODEL_OPTIONS = {\n",
                "    \"qwen2.5:3b (Fast)\": \"qwen2.5:3b\",\n",
                "    \"qwen2.5:7b (Balanced)\": \"qwen2.5:7b\",\n",
                "    \"deepseek-r1:7b (Reasoning)\": \"deepseek-r1:7b\",\n",
                "    \"llama3.2:3b (Fast)\": \"llama3.2:3b\",\n",
                "    \"Custom Model (enter below)\": \"custom\"\n",
                "}\n",
                "\n",
                "# Language options (NLLB language codes)\n",
                "LANGUAGE_OPTIONS = {\n",
                "    \"Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)\": \"hin_Deva\",\n",
                "    \"Bengali (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)\": \"ben_Beng\",\n",
                "    \"Tamil (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)\": \"tam_Taml\",\n",
                "    \"Telugu (‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å)\": \"tel_Telu\",\n",
                "    \"Marathi (‡§Æ‡§∞‡§æ‡§†‡•Ä)\": \"mar_Deva\",\n",
                "    \"Gujarati (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä)\": \"guj_Gujr\",\n",
                "    \"Kannada (‡≤ï‡≤®‡≥ç‡≤®‡≤°)\": \"kan_Knda\",\n",
                "    \"Malayalam (‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç)\": \"mal_Mlym\",\n",
                "    \"Punjabi (‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä)\": \"pan_Guru\",\n",
                "    \"Odia (‡¨ì‡¨°‡¨º‡¨ø‡¨Ü)\": \"ory_Orya\",\n",
                "    \"Urdu (ÿßÿ±ÿØŸà)\": \"urd_Arab\",\n",
                "    \"Spanish (Espa√±ol)\": \"spa_Latn\",\n",
                "    \"French (Fran√ßais)\": \"fra_Latn\",\n",
                "    \"German (Deutsch)\": \"deu_Latn\",\n",
                "    \"Chinese (‰∏≠Êñá)\": \"zho_Hans\",\n",
                "    \"Japanese (Êó•Êú¨Ë™û)\": \"jpn_Jpan\",\n",
                "    \"Custom (enter code below)\": \"custom\"\n",
                "}\n",
                "\n",
                "# Translation quality tiers\n",
                "TIER_OPTIONS = {\n",
                "    \"BASIC - Fast, good quality\": \"BASIC\",\n",
                "    \"INTERMEDIATE - Balanced (recommended)\": \"INTERMEDIATE\",\n",
                "    \"ADVANCED - Best quality, slower\": \"ADVANCED\"\n",
                "}\n",
                "\n",
                "# Provider dropdown\n",
                "provider_dropdown = widgets.Dropdown(\n",
                "    options=list(PROVIDER_OPTIONS.keys()),\n",
                "    value=\"HuggingFace (Recommended for Colab)\",\n",
                "    description='Provider:',\n",
                "    style={'description_width': 'initial'},\n",
                "    layout=widgets.Layout(width='400px')\n",
                ")\n",
                "\n",
                "# Model dropdown (HuggingFace by default)\n",
                "model_dropdown = widgets.Dropdown(\n",
                "    options=list(HF_MODEL_OPTIONS.keys()),\n",
                "    value=\"facebook/nllb-200-distilled-600M (Fast, Multilingual)\",\n",
                "    description='Model:',\n",
                "    style={'description_width': 'initial'},\n",
                "    layout=widgets.Layout(width='500px')\n",
                ")\n",
                "\n",
                "# Custom model input\n",
                "custom_model_input = widgets.Text(\n",
                "    value='',\n",
                "    placeholder='Enter HuggingFace model name',\n",
                "    description='Custom Model:',\n",
                "    style={'description_width': 'initial'},\n",
                "    layout=widgets.Layout(width='500px')\n",
                ")\n",
                "\n",
                "# Target language dropdown\n",
                "language_dropdown = widgets.Dropdown(\n",
                "    options=list(LANGUAGE_OPTIONS.keys()),\n",
                "    value=\"Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)\",\n",
                "    description='Target Language:',\n",
                "    style={'description_width': 'initial'},\n",
                "    layout=widgets.Layout(width='400px')\n",
                ")\n",
                "\n",
                "# Custom language code input\n",
                "custom_language_input = widgets.Text(\n",
                "    value='',\n",
                "    placeholder='Enter NLLB language code (e.g., hin_Deva)',\n",
                "    description='Custom Lang:',\n",
                "    style={'description_width': 'initial'},\n",
                "    layout=widgets.Layout(width='400px')\n",
                ")\n",
                "\n",
                "# Translation tier dropdown\n",
                "tier_dropdown = widgets.Dropdown(\n",
                "    options=list(TIER_OPTIONS.keys()),\n",
                "    value=\"INTERMEDIATE - Balanced (recommended)\",\n",
                "    description='Quality:',\n",
                "    style={'description_width': 'initial'},\n",
                "    layout=widgets.Layout(width='400px')\n",
                ")\n",
                "\n",
                "# HuggingFace token (optional)\n",
                "hf_token_input = widgets.Password(\n",
                "    value='',\n",
                "    placeholder='Optional: HF token for gated models',\n",
                "    description='HF Token:',\n",
                "    style={'description_width': 'initial'},\n",
                "    layout=widgets.Layout(width='400px')\n",
                ")\n",
                "\n",
                "# Chunk size slider\n",
                "chunk_size_slider = widgets.IntSlider(\n",
                "    value=350,\n",
                "    min=100,\n",
                "    max=1000,\n",
                "    step=50,\n",
                "    description='Chunk Size:',\n",
                "    style={'description_width': 'initial'},\n",
                "    layout=widgets.Layout(width='400px')\n",
                ")\n",
                "\n",
                "display(HTML(\"<h3>üéõÔ∏è Configure Translation Settings</h3>\"))\n",
                "display(provider_dropdown)\n",
                "display(model_dropdown)\n",
                "display(custom_model_input)\n",
                "display(HTML(\"<br>\"))\n",
                "display(language_dropdown)\n",
                "display(custom_language_input)\n",
                "display(HTML(\"<br>\"))\n",
                "display(tier_dropdown)\n",
                "display(chunk_size_slider)\n",
                "display(hf_token_input)\n",
                "\n",
                "print(\"\\nüí° Tip: facebook/nllb-200-distilled-600M is recommended for fast Hindi translation!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Store the selected configuration\n",
                "SELECTED_PROVIDER = PROVIDER_OPTIONS[provider_dropdown.value]\n",
                "\n",
                "# Get model based on provider\n",
                "if SELECTED_PROVIDER == \"huggingface\":\n",
                "    selected_model_key = model_dropdown.value\n",
                "    SELECTED_MODEL = HF_MODEL_OPTIONS.get(selected_model_key, \"custom\")\n",
                "else:\n",
                "    SELECTED_MODEL = OLLAMA_MODEL_OPTIONS.get(model_dropdown.value, \"custom\")\n",
                "\n",
                "if SELECTED_MODEL == \"custom\":\n",
                "    SELECTED_MODEL = custom_model_input.value\n",
                "    if not SELECTED_MODEL:\n",
                "        raise ValueError(\"Please enter a custom model name!\")\n",
                "\n",
                "# Get target language\n",
                "TARGET_LANGUAGE = LANGUAGE_OPTIONS[language_dropdown.value]\n",
                "if TARGET_LANGUAGE == \"custom\":\n",
                "    TARGET_LANGUAGE = custom_language_input.value\n",
                "    if not TARGET_LANGUAGE:\n",
                "        raise ValueError(\"Please enter a custom language code!\")\n",
                "\n",
                "TRANSLATION_TIER = TIER_OPTIONS[tier_dropdown.value]\n",
                "CHUNK_SIZE = chunk_size_slider.value\n",
                "HF_TOKEN = hf_token_input.value if hf_token_input.value else None\n",
                "\n",
                "print(f\"\\n‚úÖ Configuration saved:\")\n",
                "print(f\"   ü§ñ Provider: {SELECTED_PROVIDER}\")\n",
                "print(f\"   üì¶ Model: {SELECTED_MODEL}\")\n",
                "print(f\"   üåê Target Language: {TARGET_LANGUAGE}\")\n",
                "print(f\"   üéØ Quality Tier: {TRANSLATION_TIER}\")\n",
                "print(f\"   üì¶ Chunk Size: {CHUNK_SIZE} words\")\n",
                "print(f\"   üîë HF Token: {'Provided' if HF_TOKEN else 'Not provided'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Step 4: Translation Engine Setup\n",
                "This cell contains the complete translation engine code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!/usr/bin/env python3\n",
                "\"\"\"\n",
                "Enhanced Translation Engine for Google Colab\n",
                "Supports HuggingFace models with multiple language targets\n",
                "\"\"\"\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import json\n",
                "import time\n",
                "import warnings\n",
                "import re\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
                "\n",
                "# Check GPU availability\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"üñ•Ô∏è Using device: {DEVICE}\")\n",
                "if DEVICE == \"cuda\":\n",
                "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
                "\n",
                "\n",
                "# Translation Prompts for LLM-based models\n",
                "TRANSLATION_PROMPTS = {\n",
                "    \"BASIC\": {\n",
                "        \"system\": \"\"\"You are a professional translator. Translate the text accurately.\"\"\",\n",
                "        \"user\": \"\"\"Translate the following text to {target_lang}:\\n\\n{chunk}\\n\\nTranslation:\"\"\"\n",
                "    },\n",
                "    \"INTERMEDIATE\": {\n",
                "        \"system\": \"\"\"You are an expert literary translator. Create translations that feel natural in the target language while preserving all meaning and nuance.\"\"\",\n",
                "        \"user\": \"\"\"Translate the following text to {target_lang}. Maintain all details, dialogue, and descriptions:\\n\\n{chunk}\\n\\nComplete Translation:\"\"\"\n",
                "    },\n",
                "    \"ADVANCED\": {\n",
                "        \"system\": \"\"\"You are a master literary translator. Your translations should feel like they were originally written in the target language by a native speaker. Preserve every sentence, every detail, every nuance.\"\"\",\n",
                "        \"user\": \"\"\"Translate the COMPLETE passage below to {target_lang}.\\n\\nRequirements:\\n- Translate EVERY sentence\\n- Maintain ALL dialogue\\n- Preserve ALL descriptions\\n- Keep similar length\\n\\nText:\\n{chunk}\\n\\nComplete Translation:\"\"\"\n",
                "    }\n",
                "}\n",
                "\n",
                "\n",
                "def chunk_text(text, chunk_words=350):\n",
                "    \"\"\"Split text into chunks at paragraph boundaries.\"\"\"\n",
                "    paragraph_patterns = [\n",
                "        r'\\n\\s*\\n',\n",
                "        r'\\r\\n\\s*\\r\\n',\n",
                "        r'\\n\\s{2,}\\n',\n",
                "    ]\n",
                "    paragraph_split_pattern = '|'.join(paragraph_patterns)\n",
                "    paragraphs = re.split(paragraph_split_pattern, text)\n",
                "    paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
                "\n",
                "    chunks = []\n",
                "    current_chunk = []\n",
                "    current_count = 0\n",
                "\n",
                "    for para in paragraphs:\n",
                "        para_words = para.split()\n",
                "        para_count = len(para_words)\n",
                "\n",
                "        if para_count > chunk_words:\n",
                "            if current_chunk:\n",
                "                chunks.append('\\n\\n'.join(current_chunk))\n",
                "                current_chunk = []\n",
                "                current_count = 0\n",
                "\n",
                "            words = para.split()\n",
                "            for i in range(0, len(words), chunk_words):\n",
                "                chunk_words_list = words[i:i + chunk_words]\n",
                "                chunk_text = ' '.join(chunk_words_list)\n",
                "                chunks.append(chunk_text)\n",
                "        else:\n",
                "            if current_count + para_count > chunk_words and current_chunk:\n",
                "                chunks.append('\\n\\n'.join(current_chunk))\n",
                "                current_chunk = [para]\n",
                "                current_count = para_count\n",
                "            else:\n",
                "                current_chunk.append(para)\n",
                "                current_count += para_count\n",
                "\n",
                "    if current_chunk:\n",
                "        chunks.append('\\n\\n'.join(current_chunk))\n",
                "\n",
                "    return chunks\n",
                "\n",
                "\n",
                "def clean_translation(text):\n",
                "    \"\"\"Clean up translation artifacts.\"\"\"\n",
                "    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
                "    text = re.sub(r'```\\w*\\n?', '', text)\n",
                "    text = re.sub(r'(Translation:|Hindi Translation:|Here\\'s the translation:)', '', text, flags=re.IGNORECASE)\n",
                "    lines = [line.strip() for line in text.split('\\n')]\n",
                "    text = '\\n\\n'.join(line for line in lines if line)\n",
                "    return text.strip()\n",
                "\n",
                "\n",
                "class TranslationEngine:\n",
                "    \"\"\"Translation engine with HuggingFace support.\"\"\"\n",
                "    \n",
                "    def __init__(self, model_name, target_lang, device=\"cuda\", hf_token=None):\n",
                "        self.model_name = model_name\n",
                "        self.target_lang = target_lang\n",
                "        self.device = device\n",
                "        self.hf_token = hf_token\n",
                "        self.model = None\n",
                "        self.tokenizer = None\n",
                "        self.translator = None\n",
                "        self.model_type = self._detect_model_type(model_name)\n",
                "        \n",
                "        if self.hf_token:\n",
                "            os.environ['HF_TOKEN'] = self.hf_token\n",
                "        \n",
                "        self.load_model()\n",
                "    \n",
                "    def _detect_model_type(self, model_name):\n",
                "        \"\"\"Detect model type from name.\"\"\"\n",
                "        model_lower = model_name.lower()\n",
                "        if 'nllb' in model_lower:\n",
                "            return 'nllb'\n",
                "        elif 'indictrans' in model_lower:\n",
                "            return 'indictrans'\n",
                "        elif 'opus-mt' in model_lower or 'helsinki' in model_lower:\n",
                "            return 'opus'\n",
                "        elif 'madlad' in model_lower:\n",
                "            return 'madlad'\n",
                "        elif 'mbart' in model_lower:\n",
                "            return 'mbart'\n",
                "        else:\n",
                "            return 'causal'  # LLM-based translation\n",
                "    \n",
                "    def load_model(self):\n",
                "        \"\"\"Load translation model.\"\"\"\n",
                "        print(f\"üì• Loading model: {self.model_name}\")\n",
                "        print(f\"   Model type: {self.model_type}\")\n",
                "        \n",
                "        try:\n",
                "            if self.model_type in ['nllb', 'opus', 'mbart']:\n",
                "                self._load_seq2seq_model()\n",
                "            elif self.model_type == 'indictrans':\n",
                "                self._load_indictrans_model()\n",
                "            elif self.model_type == 'madlad':\n",
                "                self._load_madlad_model()\n",
                "            else:\n",
                "                self._load_causal_model()\n",
                "            \n",
                "            print(\"‚úÖ Model loaded successfully!\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Failed to load model: {e}\")\n",
                "            raise\n",
                "    \n",
                "    def _load_seq2seq_model(self):\n",
                "        \"\"\"Load Seq2Seq translation model (NLLB, OPUS, mBART).\"\"\"\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
                "            self.model_name,\n",
                "            token=self.hf_token,\n",
                "            src_lang=\"eng_Latn\" if self.model_type == 'nllb' else None\n",
                "        )\n",
                "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
                "            self.model_name,\n",
                "            token=self.hf_token,\n",
                "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
                "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
                "        )\n",
                "        \n",
                "        if self.device != \"cuda\":\n",
                "            self.model = self.model.to(self.device)\n",
                "        \n",
                "        self.translator = pipeline(\n",
                "            \"translation\",\n",
                "            model=self.model,\n",
                "            tokenizer=self.tokenizer,\n",
                "            src_lang=\"eng_Latn\" if self.model_type == 'nllb' else \"en\",\n",
                "            tgt_lang=self.target_lang if self.model_type == 'nllb' else None,\n",
                "            max_length=1024,\n",
                "            device=0 if self.device == \"cuda\" else -1\n",
                "        )\n",
                "    \n",
                "    def _load_indictrans_model(self):\n",
                "        \"\"\"Load IndicTrans2 model.\"\"\"\n",
                "        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
                "        \n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
                "            self.model_name,\n",
                "            token=self.hf_token,\n",
                "            trust_remote_code=True\n",
                "        )\n",
                "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
                "            self.model_name,\n",
                "            token=self.hf_token,\n",
                "            trust_remote_code=True,\n",
                "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
                "        )\n",
                "        \n",
                "        if self.device == \"cuda\":\n",
                "            self.model = self.model.cuda()\n",
                "    \n",
                "    def _load_madlad_model(self):\n",
                "        \"\"\"Load MADLAD-400 model.\"\"\"\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
                "            self.model_name,\n",
                "            token=self.hf_token\n",
                "        )\n",
                "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
                "            self.model_name,\n",
                "            token=self.hf_token,\n",
                "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
                "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
                "        )\n",
                "    \n",
                "    def _load_causal_model(self):\n",
                "        \"\"\"Load causal LM for translation.\"\"\"\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
                "            self.model_name,\n",
                "            token=self.hf_token\n",
                "        )\n",
                "        self.model = AutoModelForCausalLM.from_pretrained(\n",
                "            self.model_name,\n",
                "            token=self.hf_token,\n",
                "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
                "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
                "        )\n",
                "        \n",
                "        if self.tokenizer.pad_token is None:\n",
                "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
                "    \n",
                "    def translate(self, text, tier=\"INTERMEDIATE\"):\n",
                "        \"\"\"Translate text based on model type.\"\"\"\n",
                "        if self.model_type in ['nllb', 'opus', 'mbart']:\n",
                "            return self._translate_seq2seq(text)\n",
                "        elif self.model_type == 'indictrans':\n",
                "            return self._translate_indictrans(text)\n",
                "        elif self.model_type == 'madlad':\n",
                "            return self._translate_madlad(text)\n",
                "        else:\n",
                "            return self._translate_causal(text, tier)\n",
                "    \n",
                "    def _translate_seq2seq(self, text):\n",
                "        \"\"\"Translate using Seq2Seq model.\"\"\"\n",
                "        result = self.translator(text, max_length=1024)\n",
                "        return result[0]['translation_text']\n",
                "    \n",
                "    def _translate_indictrans(self, text):\n",
                "        \"\"\"Translate using IndicTrans2.\"\"\"\n",
                "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
                "        \n",
                "        if self.device == \"cuda\":\n",
                "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = self.model.generate(\n",
                "                **inputs,\n",
                "                max_length=512,\n",
                "                num_beams=5,\n",
                "                num_return_sequences=1\n",
                "            )\n",
                "        \n",
                "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    def _translate_madlad(self, text):\n",
                "        \"\"\"Translate using MADLAD-400.\"\"\"\n",
                "        # MADLAD uses language tags like <2hi> for Hindi\n",
                "        lang_code = self.target_lang.split('_')[0][:2]  # Extract 2-letter code\n",
                "        tagged_text = f\"<2{lang_code}> {text}\"\n",
                "        \n",
                "        inputs = self.tokenizer(tagged_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
                "        \n",
                "        if self.device == \"cuda\":\n",
                "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = self.model.generate(\n",
                "                **inputs,\n",
                "                max_length=512,\n",
                "                num_beams=4\n",
                "            )\n",
                "        \n",
                "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    def _translate_causal(self, text, tier):\n",
                "        \"\"\"Translate using causal LM with prompts.\"\"\"\n",
                "        prompts = TRANSLATION_PROMPTS[tier]\n",
                "        \n",
                "        # Get language name from code\n",
                "        lang_names = {'hin_Deva': 'Hindi', 'ben_Beng': 'Bengali', 'tam_Taml': 'Tamil'}\n",
                "        lang_name = lang_names.get(self.target_lang, self.target_lang)\n",
                "        \n",
                "        full_prompt = f\"{prompts['system']}\\n\\n{prompts['user'].format(target_lang=lang_name, chunk=text)}\"\n",
                "        \n",
                "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
                "        \n",
                "        if self.device == \"cuda\":\n",
                "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = self.model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=1024,\n",
                "                temperature=0.4,\n",
                "                do_sample=True,\n",
                "                pad_token_id=self.tokenizer.pad_token_id\n",
                "            )\n",
                "        \n",
                "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        # Extract translation from response\n",
                "        if \"Translation:\" in generated:\n",
                "            return generated.split(\"Translation:\")[-1].strip()\n",
                "        return generated[len(full_prompt):].strip()\n",
                "\n",
                "\n",
                "class TranslationGenerator:\n",
                "    \"\"\"Main translation generator class.\"\"\"\n",
                "    \n",
                "    def __init__(self, model_name, target_lang, device=\"cuda\", output_dir=\".\", tier=\"INTERMEDIATE\", chunk_size=350, hf_token=None):\n",
                "        self.model_name = model_name\n",
                "        self.target_lang = target_lang\n",
                "        self.device = device\n",
                "        self.output_dir = Path(output_dir)\n",
                "        self.tier = tier\n",
                "        self.chunk_size = chunk_size\n",
                "        \n",
                "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
                "        self.engine = TranslationEngine(model_name, target_lang, device, hf_token)\n",
                "    \n",
                "    def translate_file(self, input_file):\n",
                "        \"\"\"Translate entire file.\"\"\"\n",
                "        print(f\"\\n{'=' * 70}\")\n",
                "        print(f\"üåê TRANSLATION GENERATOR\")\n",
                "        print(f\"{'=' * 70}\")\n",
                "        print(f\"üìÑ Input: {input_file}\")\n",
                "        print(f\"ü§ñ Model: {self.model_name}\")\n",
                "        print(f\"üåê Target: {self.target_lang}\")\n",
                "        print(f\"üéØ Quality: {self.tier}\")\n",
                "        print(f\"üñ•Ô∏è Device: {self.device}\")\n",
                "        print(f\"{'=' * 70}\\n\")\n",
                "        \n",
                "        # Read input\n",
                "        with open(input_file, 'r', encoding='utf-8') as f:\n",
                "            text = f.read()\n",
                "        \n",
                "        # Clean markers\n",
                "        lines = text.split('\\n')\n",
                "        cleaned = [l for l in lines if not (l.strip().startswith('===') and l.strip().endswith('==='))]\n",
                "        text = '\\n'.join(cleaned).strip()\n",
                "        \n",
                "        orig_words = len(text.split())\n",
                "        orig_chars = len(text)\n",
                "        print(f\"üìä Input: {orig_chars:,} chars, {orig_words:,} words\")\n",
                "        \n",
                "        # Chunk text\n",
                "        print(f\"\\nüì¶ Creating chunks ({self.chunk_size} words each)...\")\n",
                "        chunks = chunk_text(text, self.chunk_size)\n",
                "        print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
                "        \n",
                "        # Translate chunks\n",
                "        print(f\"\\nüéØ STARTING TRANSLATION\\n\")\n",
                "        \n",
                "        translations = []\n",
                "        start_time = time.time()\n",
                "        \n",
                "        for i, chunk in enumerate(chunks, 1):\n",
                "            chunk_start = time.time()\n",
                "            \n",
                "            print(f\"\\n{'=' * 50}\")\n",
                "            print(f\"üìÑ Chunk {i}/{len(chunks)}\")\n",
                "            print(f\"   Input: {len(chunk.split())} words, {len(chunk)} chars\")\n",
                "            \n",
                "            try:\n",
                "                translated = self.engine.translate(chunk, self.tier)\n",
                "                translated = clean_translation(translated)\n",
                "                translations.append(translated)\n",
                "                \n",
                "                chunk_time = time.time() - chunk_start\n",
                "                print(f\"   Output: {len(translated)} chars\")\n",
                "                print(f\"   ‚úÖ Completed in {chunk_time:.1f}s\")\n",
                "                \n",
                "                # Progress\n",
                "                elapsed = time.time() - start_time\n",
                "                avg = elapsed / i\n",
                "                remaining = len(chunks) - i\n",
                "                eta = remaining * avg\n",
                "                print(f\"   üìà Progress: {i/len(chunks)*100:.1f}% | ETA: {eta/60:.1f}m\")\n",
                "                \n",
                "            except Exception as e:\n",
                "                print(f\"   ‚ùå Error: {e}\")\n",
                "                translations.append(f\"[TRANSLATION ERROR: {e}]\")\n",
                "        \n",
                "        # Combine translations\n",
                "        final_translation = \"\\n\\n\".join(translations)\n",
                "        \n",
                "        # Save output\n",
                "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "        lang_code = self.target_lang.split('_')[0]\n",
                "        output_file = self.output_dir / f\"translation_{lang_code}_{timestamp}.txt\"\n",
                "        \n",
                "        with open(output_file, 'w', encoding='utf-8') as f:\n",
                "            f.write(final_translation)\n",
                "        \n",
                "        # Summary\n",
                "        total_time = time.time() - start_time\n",
                "        trans_chars = len(final_translation)\n",
                "        \n",
                "        print(f\"\\n{'=' * 70}\")\n",
                "        print(f\"üéâ TRANSLATION COMPLETE!\")\n",
                "        print(f\"{'=' * 70}\")\n",
                "        print(f\"‚è±Ô∏è Time: {total_time/60:.1f} minutes\")\n",
                "        print(f\"üì¶ Chunks: {len(chunks)}\")\n",
                "        print(f\"‚ö° Avg/chunk: {total_time/len(chunks):.1f}s\")\n",
                "        print(f\"üìù Input: {orig_chars:,} chars\")\n",
                "        print(f\"üìù Output: {trans_chars:,} chars\")\n",
                "        print(f\"üìä Ratio: {trans_chars/orig_chars:.2f}x\")\n",
                "        print(f\"üíæ Output: {output_file}\")\n",
                "        print(f\"{'=' * 70}\")\n",
                "        \n",
                "        return str(output_file)\n",
                "\n",
                "\n",
                "print(\"‚úÖ Translation Engine loaded and ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üåê Step 5: Generate Translation\n",
                "Run this cell to translate your uploaded file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create output directory\n",
                "OUTPUT_DIR = \"./translation_output\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# Initialize the generator\n",
                "print(\"üöÄ Initializing Translation Generator...\")\n",
                "generator = TranslationGenerator(\n",
                "    model_name=SELECTED_MODEL,\n",
                "    target_lang=TARGET_LANGUAGE,\n",
                "    device=DEVICE,\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    tier=TRANSLATION_TIER,\n",
                "    chunk_size=CHUNK_SIZE,\n",
                "    hf_token=HF_TOKEN\n",
                ")\n",
                "\n",
                "# Translate\n",
                "print(f\"\\nüåê Starting translation...\")\n",
                "OUTPUT_FILE = generator.translate_file(UPLOADED_FILE)\n",
                "\n",
                "print(f\"\\n‚úÖ Translation file generated: {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìñ Step 6: Preview & Download Translation\n",
                "View your translation and download it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import display, HTML\n",
                "import os\n",
                "\n",
                "if os.path.exists(OUTPUT_FILE):\n",
                "    # Read and display translation\n",
                "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
                "        translation = f.read()\n",
                "    \n",
                "    file_size = os.path.getsize(OUTPUT_FILE) / 1024  # KB\n",
                "    word_count = len(translation.split())\n",
                "    \n",
                "    print(f\"üìä Translation stats:\")\n",
                "    print(f\"   Words: {word_count:,}\")\n",
                "    print(f\"   Characters: {len(translation):,}\")\n",
                "    print(f\"   File size: {file_size:.2f} KB\")\n",
                "    \n",
                "    print(f\"\\nüìñ Preview (first 1000 chars):\")\n",
                "    print(f\"{'=' * 50}\")\n",
                "    print(translation[:1000])\n",
                "    print(f\"{'=' * 50}\")\n",
                "    if len(translation) > 1000:\n",
                "        print(f\"... [truncated, {len(translation) - 1000:,} more chars]\")\n",
                "else:\n",
                "    print(\"‚ùå Output file not found. Please run the translation step again.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download the translated file\n",
                "from google.colab import files\n",
                "\n",
                "print(\"üì• Downloading your translated file...\")\n",
                "files.download(OUTPUT_FILE)\n",
                "print(\"‚úÖ Download started! Check your browser's download folder.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üíæ (Optional) Save to Google Drive\n",
                "If you want to save the translation to your Google Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "import shutil\n",
                "\n",
                "print(\"üìÇ Mounting Google Drive...\")\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Create output folder in Drive\n",
                "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/Translation_Output\"\n",
                "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# Copy file to Drive\n",
                "drive_output_path = os.path.join(DRIVE_OUTPUT_DIR, os.path.basename(OUTPUT_FILE))\n",
                "shutil.copy(OUTPUT_FILE, drive_output_path)\n",
                "\n",
                "print(f\"\\n‚úÖ Translation saved to Google Drive:\")\n",
                "print(f\"   üìÅ {drive_output_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìö Quick Reference\n",
                "\n",
                "### Supported Languages (NLLB Codes):\n",
                "| Language | Code |\n",
                "|----------|------|\n",
                "| Hindi | `hin_Deva` |\n",
                "| Bengali | `ben_Beng` |\n",
                "| Tamil | `tam_Taml` |\n",
                "| Telugu | `tel_Telu` |\n",
                "| Marathi | `mar_Deva` |\n",
                "| Gujarati | `guj_Gujr` |\n",
                "| Spanish | `spa_Latn` |\n",
                "| French | `fra_Latn` |\n",
                "| German | `deu_Latn` |\n",
                "\n",
                "### Recommended Models:\n",
                "| Model | Best For | Speed |\n",
                "|-------|----------|-------|\n",
                "| `facebook/nllb-200-distilled-600M` | Fast multilingual | ‚ö° Fast |\n",
                "| `facebook/nllb-200-1.3B` | Better quality | üîÑ Medium |\n",
                "| `ai4bharat/indictrans2-en-indic-1B` | Best EN‚ÜíHindi | üîÑ Medium |\n",
                "| `google/madlad400-3b-mt` | Highest quality | üê¢ Slow |\n",
                "\n",
                "### Quality Tiers:\n",
                "- **BASIC**: Fast, good for simple texts\n",
                "- **INTERMEDIATE**: Balanced quality and speed (recommended)\n",
                "- **ADVANCED**: Best quality, preserves all nuances\n",
                "\n",
                "### Tips:\n",
                "- Use Colab GPU for faster translation\n",
                "- For long texts, use smaller chunk sizes (200-300 words)\n",
                "- NLLB models are best for multilingual translation"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}